{"docstore/metadata": {"449df74210f7139bab3a667da805cbe995689b70": {"doc_hash": "e34464ee699f73a27d8b7fda83b0f9e5c29216d31d00fb30949b59fb309aa964"}, "bd4e88c9310cf38842a3791cf351890b7028c06e": {"doc_hash": "fba5b7e875ee0909b38253aa78f7809fa06ca5d7d5ddca790db8f818abaf0951"}, "682b5f9b2cc0b4225e20e8312ccb41c21a07343e": {"doc_hash": "3a0663328ad2e2f77a1dc7cfa9ff055a4c140cbc511c8b1371c9e431c9e16dde"}, "c1cc21c73e8df2591264b7e1fe4b0f19ad41618f": {"doc_hash": "f4a3396e50a2311260be4a1ccaa7ce0778d63df4cb3e937f7eb22f0fb4926d23"}, "fa7ad47f2921567bc51f1826c13278586e8960a7": {"doc_hash": "b14913c2bcbf55fa5f1db3fe06de08ca7795ffc1278ed7be8ea66ac13b171e16"}, "be741491bb050d86e011369fa21d499f748cb82c": {"doc_hash": "b6d23fb84ba6b3a3573648c5513954dd057b17583b084bbef96ee754f881baec"}, "fef500d8481bfcad975021a7b91ff340b84b34e0": {"doc_hash": "f45df7debf40c9c7c5a359328be12d275390bc4a63a93e7bb3e0ff3489e953ee"}, "4507ac4676a6387c4b52a0d1111e94753a102b32": {"doc_hash": "272db777274ed25a2e12b65324fbb9537f1457cec9c91f3d0f5489fe57d1ec7a"}, "e59c940584cc87ebdfe475beed038fabc7490f6f": {"doc_hash": "3f83136e88275baee8eb5d8181a452cf228ebd1f2cb4ef93cfcf92488a0fa6de"}, "96255605229db00f87066d54854a2946f36be5c2": {"doc_hash": "62a31312f1e0faeb924628c6cac6ce5f62038341e0214d77e36f7ab52bd6953e"}, "f50a824f030689bc98d3c9afb1374a9facc0d041": {"doc_hash": "8c7417045560473780b7667dfc518ac09cdd669b0201b90b33888c8478aa28df"}, "89f4ccb8b518fb2addd6968f34e02f7e33086f55": {"doc_hash": "4d4a5e5c78f852031b23ab4cac5925c0ffd0ebdca883ca0e12d37c4607eeb1f9"}, "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b": {"doc_hash": "9c8366b4403e84433cc0771235a5a7493c9745756a05f3c6f324d44d384f2a9a"}, "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0": {"doc_hash": "d76047ee52625dc73c551878472962bf0ca123d0558990b1189939799090987f"}, "9efa71524a0ed4c2d73cbd0de801185f55016f48": {"doc_hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b"}, "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b": {"doc_hash": "37752489997bebe809086a7f1f0ba2fa148101e0c7dcd7abb2b38e850df6f5eb"}, "a7011225db0e8e53ca1171dc4d4730630f8a8057": {"doc_hash": "470e3b96d80bc2e85d41a62acdf96b3aab397c4245d49bf7aef1d9ce050a8e04"}, "5bce626893bf47c40ca1454df6fc4c990875f9cc": {"doc_hash": "5c1acf8ea69d5c361404f32b04ff44b4511245eaee387875c5fea73213c277bb"}, "5990a1d6e725b94c0410a8d7fc47a3839633c07e": {"doc_hash": "6e07e1e3bb1f7ae9803e7581108fa317bb220fb32b16faad4d4758a7352f11d2"}, "2a4be4759c3a6c6efb18ee73fcd5b692148b776c": {"doc_hash": "334dc4e78631f38b37e8e5500dd3af6d3ca4c9d273cd5bc28aeb83e2f326abdf"}, "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c": {"doc_hash": "5c539744e63d6b2a2f48377e264620ffc2c352fde1b5e5e625a436910bcfdb62"}, "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992": {"doc_hash": "2b5885b673d58e0010cb36d54f89485bf511458909a2c506373fb03b5fa570e6"}, "58cbe867b4a48bc62e1f9e42986fb0dde8d61134": {"doc_hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed"}, "47c9d1f461f64d806f8cfab80a59b47bcc0684d0": {"doc_hash": "28af3ac201a7fca164b02687b6f4867e59a8225df415e83dec188d8442dce760"}, "6da36ca7f2c12fecf479ba97087c4854781277ae": {"doc_hash": "e8a228997af3702c444246bb9f23e768e2b6a548c37daf1593ea0891c796ae7a"}, "437ca10e7fcb650ad9d6cc7e00e85cf426195652": {"doc_hash": "6036339b1928878b500c2790ad02552403f908e690e0d4b043d4db863240de20"}, "4c84b615c79ae3c69552051e5b4c2f05c55718eb": {"doc_hash": "3d5ebae59490b24498643f3cc8f8c072d188f8f3f75b588cbc5ff51f3684fc51"}, "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64": {"doc_hash": "42ee3131b28c345218810534bed4316f4c159da77d02f597186994380f522d1e"}, "e994a9b32cc2cff65d7d736018149f076cf4a9ab": {"doc_hash": "6e8f7a2822b59da2f74bf583213848b883cf3ea30608572f97d95f6ee2120a7d"}, "b9a16ceebef3baf18c14472addad583f57beebb1": {"doc_hash": "fad0db3ba4cc5e99ea36b4d19e3ab2365f574e677e31607c6c6129f6502507e5"}, "b930774918cc5aeecbaceff681af53f7a9117ec4": {"doc_hash": "5df69080fbd27175d0d9c8830cce9931618f99d2de8d5c85db528915a15e9933"}, "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b": {"doc_hash": "c707e29cc2b7e4c47832053a51312798b89bab262556ce615fad2ddf9817d62d"}, "e9269ff6db3d6c8b2382edccfa366259b3e577b7": {"doc_hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e"}, "da03217c2e9e57bb553c35fa72450cf6f482e29b": {"doc_hash": "e88f454355aa09ddf7a0262a839d847c7f74f440db34659ad72318926a23cee9"}, "c5c31d65acf25a33c1d33105d9eb721f941d1c16": {"doc_hash": "c5bd11955953c9e7a57f47ca208446210bc90b51860314e7843a293fab41380b"}, "a2b76af6b9f0f9ff5150fb180e4d5bd4b467c6c3": {"doc_hash": "ea854861d57e674d207974580f04c8ae6c9a849d769d26fca662136fcadb3875"}, "94492d9bcbabe018649c059764d995c6dca54ac3": {"doc_hash": "a53d27e4e4733b2a7ad25119f35db91a8a750cf972dffcf5a3c8f0475c74265b"}, "e8587ec0bf9ca4a54ac448433f70b323f08a400d": {"doc_hash": "d15abc23027909534ccfc2169c5606a8e9350a83c590a976c1ed4436891efe07"}, "e5cfb31c-5236-468e-bfcc-e8f61a7d70d7": {"doc_hash": "4682047358b81bb7f1bf9f77de4e7d6d4f8f2ed711d1af8e665950922d9b81f2", "ref_doc_id": "449df74210f7139bab3a667da805cbe995689b70"}, "e170b2de-aa38-48b5-a15a-0544cb14ed5a": {"doc_hash": "9476088675af62fbf4f136f74417dd9ec850f463c2ae045850aa129b821bb738", "ref_doc_id": "449df74210f7139bab3a667da805cbe995689b70"}, "95d8c729-f596-470d-a1ff-f4e1ff3c3ff3": {"doc_hash": "fba5b7e875ee0909b38253aa78f7809fa06ca5d7d5ddca790db8f818abaf0951", "ref_doc_id": "bd4e88c9310cf38842a3791cf351890b7028c06e"}, "6044ec8a-e179-4313-b5b4-6cec70c8671b": {"doc_hash": "c1366d30f39cc95789cca9ad29e27be2c52f612b198583308e611ac841b09cb3", "ref_doc_id": "682b5f9b2cc0b4225e20e8312ccb41c21a07343e"}, "12336feb-25eb-4296-a182-060ed3eda05b": {"doc_hash": "f4a3396e50a2311260be4a1ccaa7ce0778d63df4cb3e937f7eb22f0fb4926d23", "ref_doc_id": "c1cc21c73e8df2591264b7e1fe4b0f19ad41618f"}, "5144edd0-f473-49ec-8901-a134b9c9b93b": {"doc_hash": "8c7d61724f44edaebbbfa5d5172b743bc73b4f8f34297bb84d515519def0ba6f", "ref_doc_id": "fa7ad47f2921567bc51f1826c13278586e8960a7"}, "d92716bf-d0d9-491c-a9eb-0ba2a0412d37": {"doc_hash": "cceac6a9c5639988456b855965140a2a777a22fb4f2d2982050930786c326c9d", "ref_doc_id": "fa7ad47f2921567bc51f1826c13278586e8960a7"}, "eb17f6d0-566e-41d7-a415-95ecbe17d24c": {"doc_hash": "171b732a2b22e8a934d2a6c7ea8d896e63dcfe66e107aa3396ed5ad77c4300b1", "ref_doc_id": "fa7ad47f2921567bc51f1826c13278586e8960a7"}, "1d5fdc24-4ab2-42c3-a4f3-87cc51359744": {"doc_hash": "875818f03fe296b958954cddc6eb217a81acd1d903b9d29e72c65ef349712a9d", "ref_doc_id": "be741491bb050d86e011369fa21d499f748cb82c"}, "9675bbca-c254-4679-9e54-7cee7471e43d": {"doc_hash": "1e7d506b212f6aea9fbaa003a7f7ea7ffdf81e4d2b5b4539a4b181704d535c3b", "ref_doc_id": "be741491bb050d86e011369fa21d499f748cb82c"}, "52bda799-0661-40a4-8d8e-2e00db6cd267": {"doc_hash": "8a499e1fd3e79270d0aa4ac798e5ed0210286c97f79855dcda3716570e547729", "ref_doc_id": "fef500d8481bfcad975021a7b91ff340b84b34e0"}, "3ad1b863-e59a-4d2d-b9d7-e64865f5fd5c": {"doc_hash": "b6c4cec3042bbf08535d4d49acd31fdcffb4171a70b695e2c2f8894aa54a3e04", "ref_doc_id": "4507ac4676a6387c4b52a0d1111e94753a102b32"}, "0845d479-118d-4f16-8175-5775b61a433f": {"doc_hash": "c217881b2c563d7a1351936efa6216c3f51c75df8df0fc876fabdfe01ad5b41e", "ref_doc_id": "e59c940584cc87ebdfe475beed038fabc7490f6f"}, "5b11d6ce-59cb-4428-b0b3-6fefb138fd21": {"doc_hash": "97a68b378b323d1403a9934237a01542b60cc36783e8a0d4599249bb882b578d", "ref_doc_id": "96255605229db00f87066d54854a2946f36be5c2"}, "eb10b6c8-ea71-427c-a6ab-5e090212ec10": {"doc_hash": "c3b3437376216a87710b8f04d39b92d75b4001caf73eb8235840f180657aea0c", "ref_doc_id": "96255605229db00f87066d54854a2946f36be5c2"}, "173af6aa-4d89-45c9-9c1f-4169ea8ba8d4": {"doc_hash": "8e0749f6cfb94b69cff1de68d06391a12fc01b97bc860338e5a211ce9e04a1f4", "ref_doc_id": "96255605229db00f87066d54854a2946f36be5c2"}, "7387b719-1c3b-4d15-b0be-c8042b07e189": {"doc_hash": "8c7417045560473780b7667dfc518ac09cdd669b0201b90b33888c8478aa28df", "ref_doc_id": "f50a824f030689bc98d3c9afb1374a9facc0d041"}, "5ced1dae-b900-4fa0-a219-a0266e2fcde1": {"doc_hash": "a2befdfddf0821108f36d38f1d1ce121df3419a77bf60110dc24a40db186c4a7", "ref_doc_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55"}, "2f6806bd-0369-4dc6-889f-87537ab4e57e": {"doc_hash": "c2b155fb57ad3510ae8ce258b4f81a524ecd8a81b75103b0f283193b87ce44c6", "ref_doc_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55"}, "009b1bc3-d665-4a69-94ba-b3deda024b8d": {"doc_hash": "eafecb58efe5af77180b6b11c84648ee063c67024cd082db02a80ac20e7b57dc", "ref_doc_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55"}, "e41d42f9-f77c-45b1-95e4-cf84f9bed0cb": {"doc_hash": "b6c4cec3042bbf08535d4d49acd31fdcffb4171a70b695e2c2f8894aa54a3e04", "ref_doc_id": "4507ac4676a6387c4b52a0d1111e94753a102b32"}, "20b1145a-267e-4b48-ada5-bebc2e453add": {"doc_hash": "185475f66250b0e7a4621a745889b002ea1bc8eca033e18fb693b192689cdb33", "ref_doc_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b"}, "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2": {"doc_hash": "20140085069fb42353682a2acaceb060b7c99b0630495bc9b0caff126d8f9c6c", "ref_doc_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b"}, "434447fb-16a4-413b-a73b-6979b5f28d3d": {"doc_hash": "f88fa22759c13af2f943e1fa782afa598ae2fa43e0578557b50bb11101c1d178", "ref_doc_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b"}, "f2e2b4be-05bd-4a3f-84db-6cbcb963d58a": {"doc_hash": "884aaf55c7cab6f7735a867436e9785d31d0e63f7ae67dd0f79cffd46e73ea84", "ref_doc_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0"}, "668702ac-9686-41e6-a060-d49ecdb511a6": {"doc_hash": "c2d8a007da9e340c46d5d6a3efeb869c6abdfe9eeb7266866f8cd55acb3946df", "ref_doc_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0"}, "8a20f4ab-e30a-4bef-bd32-ee56f9899299": {"doc_hash": "9d1c53e440ffb0e99dde6dbde2619b7795c0bbc5a71c10482105fd67ac022690", "ref_doc_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0"}, "cf6b6221-b673-4f11-a2a4-4ce633f7e050": {"doc_hash": "c5458038e88ea364d8794898c8628ed9a63bbc3d4d9f45604da6a39bb30bb5aa", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "f4403db4-6635-400e-bfe9-65a29304b667": {"doc_hash": "4efd385a7aecd56481a4239ce56747e73c4a739d649baed3447840294876531a", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "99e19d07-c7e1-45c0-a54f-1e1797ca4faa": {"doc_hash": "29110d0ce89e306956f13582b8f783fc29ed2dba0f574961ed68e77a239c2970", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "a2fc86b9-9bae-4884-8a68-92756ccbdbb1": {"doc_hash": "54ec2ef9c0ca07e829fae6e7a3fcc6c3f65e9957aad74d9b52ef60dd70223052", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "2e649ee5-5f73-4c32-a500-0db71926e9b1": {"doc_hash": "d32e71a11d75150fb9bc3bae73014d233bba5e97a94d7170fd883849c634079a", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "bba38736-3c6e-4460-846e-9105ddf2cf6c": {"doc_hash": "83a604f87934ee7f95a4c62fd888b72c792a9d10af8be28c274705582d833cc5", "ref_doc_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48"}, "f2f45d7c-1b2f-4af0-a8f7-f6d0290f66dc": {"doc_hash": "d303d06355fdf58e7820537d9af509a03da04e53846f8f22130a9431bc136130", "ref_doc_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b"}, "b81375ed-6b04-47a5-8802-ce2d700c0c8b": {"doc_hash": "5b3cd8a7d18e4223fd1671810fdee3ceceb291e66bf0283ec4fe5c01fb8768d3", "ref_doc_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b"}, "03911321-4ada-498f-89c6-153120171682": {"doc_hash": "165624ece2c4e54a609d6b0065da42d04217c4fde719bb550746fc164510a07b", "ref_doc_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b"}, "973a9bc9-daad-4963-8f96-efcaeab6d251": {"doc_hash": "3ee074651d8c3ca5658f0318e381803452450583d3151a512a677dab1dc4e72e", "ref_doc_id": "a7011225db0e8e53ca1171dc4d4730630f8a8057"}, "87418484-729c-42a9-8613-2f374fa4f156": {"doc_hash": "e9b621b7d04e22f1a2fb86f3494d53faebfa3572e4400029af3675595256d21a", "ref_doc_id": "a7011225db0e8e53ca1171dc4d4730630f8a8057"}, "710d2581-73a5-41fb-95eb-166a93d68698": {"doc_hash": "2e6ec4b2a880412995e80478e179128df34e074efd8bf12add6bd2021a529b30", "ref_doc_id": "5bce626893bf47c40ca1454df6fc4c990875f9cc"}, "41847277-2046-4a94-b383-972bcf06bf94": {"doc_hash": "787a092c8a8b46355f6a7595fc47ee1c51d815b88ad9110d6819b6ea10b01e7a", "ref_doc_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e"}, "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0": {"doc_hash": "4e654ae9677ea77244bb49e4a8b0d7fed8aabb788509ecf3b824314cdf868996", "ref_doc_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e"}, "3c3d245e-cd50-4994-b82a-bd18d025ef4b": {"doc_hash": "c7fe336da02c56e93a75d4f421c3da9c90c75a47ecd5228138610ded8f325365", "ref_doc_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e"}, "9cea47e3-604f-43bd-8769-7dae043922e1": {"doc_hash": "6187e1994a0292bd10634053a31b73c3699b3090164162c30937a0bd4cb4673c", "ref_doc_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c"}, "c48a19b6-8a54-4580-bddd-fbb1fc556a67": {"doc_hash": "0381635ef2531aa65ebee515f488002102bd94d717715c8491700863633665ed", "ref_doc_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c"}, "983fdefd-5cc9-445e-8a2e-afcec2d87bbb": {"doc_hash": "513f91658cd0f996d86f821bde37943c2bc18a270107944b613b208d5efac536", "ref_doc_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c"}, "0c886c5b-2a49-40f0-bb36-051f24784ee0": {"doc_hash": "691d649b0bb3d52256a5a77bbff0e4bef2fe08262ccac7ed7ccb7190536e803b", "ref_doc_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c"}, "046ac453-e203-489b-8f09-b2655a558628": {"doc_hash": "8d6b62e71cf720b3799bfdeb544c84d7c0c2d924dd536fa71c3a091a13fc203a", "ref_doc_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c"}, "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453": {"doc_hash": "a02e199c3e94c0d0e132e60e199a8af839033850d17b6d157bff956684d1e51a", "ref_doc_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c"}, "f1ff6f39-5ba2-4c12-82f2-6e23974487b2": {"doc_hash": "d73b7092bb1e8a44c1ae5a5a933c070e31a95a1ba8095060eddc7b59ee08d4b7", "ref_doc_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c"}, "1d659236-9c98-493f-a887-1bab441715d4": {"doc_hash": "b6c4cec3042bbf08535d4d49acd31fdcffb4171a70b695e2c2f8894aa54a3e04", "ref_doc_id": "4507ac4676a6387c4b52a0d1111e94753a102b32"}, "9ba12866-4df7-4c5d-a9eb-36f3cd54b72e": {"doc_hash": "94f52411990314fc46848379bee4be33ef6d9560ba265a16b49ef82392a22d6a", "ref_doc_id": "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992"}, "ffbe0438-54fa-4dd5-bc87-5c739ec3fd27": {"doc_hash": "5e8cbc6a0e4c052b767ea6849db9d8671b34a4d9f67b2697b4589b020ec92efb", "ref_doc_id": "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992"}, "9849c6cc-de3c-4a79-8d1c-7ba669f201bb": {"doc_hash": "880dab6fcf7a53e110f984cf6ac3df2f16f2d0e45e6d382a72c0491e557fa149", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "b62de8c1-f71e-4c6e-b83f-6301ec4d7117": {"doc_hash": "fbfe79bba94ca89666108197c2223e11619f2d5443f58eeab0b6fe4db23f826c", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "9bcc0635-95e9-4304-b192-53a2d5060d16": {"doc_hash": "56c55bd72aa0301ce026113d6ed2dcc0177abccec683e8f07cc4809d53e28c5c", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "a5026196-1dbc-4ff2-bfc9-3e904ed809cd": {"doc_hash": "c2825290105a8f917692da909c6ac2bee7fc96db58239f0485a9b7899b3334bf", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "b7dbc69b-b410-437f-b4fd-f106ad67bf49": {"doc_hash": "90ba2b7916d7090bb71d556f18cd7c9bb736570d664a6f302da48929e383e36e", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "9ad17cff-aafd-493d-b98b-5d3c3adfc192": {"doc_hash": "b91bc619da967ea7e15ae870f27741feafb15c4e1e334160a41776f1db2dfc2e", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "4a283b59-f15f-47fc-b3ed-6a81577e2e05": {"doc_hash": "be51448cced45be472a597b5c22f012d7b629ed6c4ce66e5fb9c16c10782facc", "ref_doc_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134"}, "e86ad346-6d4c-49df-9782-44034ea980ee": {"doc_hash": "cf3b6beae44d94e9afe4b3efbd78eb1b1736b9dbb08dde79867a66344b79db92", "ref_doc_id": "47c9d1f461f64d806f8cfab80a59b47bcc0684d0"}, "0515024f-c184-43de-9830-ba7e592b41ea": {"doc_hash": "a35dc7c8d69354d73e9231b5aa2d1945a35890ea830efdd5cee25825dc778382", "ref_doc_id": "6da36ca7f2c12fecf479ba97087c4854781277ae"}, "b1c45f48-654e-4c30-bdbe-77842f60e439": {"doc_hash": "ed9eee5a955b371c0b26eba3a221b9272866129124a971179cd13269e34e2b0d", "ref_doc_id": "6da36ca7f2c12fecf479ba97087c4854781277ae"}, "d147bf70-ffe7-4154-aa6e-c8aa0ae8fad7": {"doc_hash": "3a31aeac771c6f8ca98630a568e8b8b50be29b765f658525e4d598311819bbfb", "ref_doc_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652"}, "09e17bcc-8168-4050-b24f-0a7a2b8c1591": {"doc_hash": "5f9b1c1f801af00ea82bf207b459c4c1138ed302debe100878196fe023b52cc8", "ref_doc_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652"}, "355e3db2-0528-4f8e-b515-cb80d8ccb13d": {"doc_hash": "2e24f88bae0c5db9c4f0921beceeb697d8fed93637f329eeb64d6733c59aa5a7", "ref_doc_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652"}, "b81565f2-a954-49d3-a895-c26d3c9244c9": {"doc_hash": "4e485caed8d82908df360709ed66e82be626a6730376ee94a905c9167f7acadb", "ref_doc_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652"}, "06a78d16-75e4-4a9f-b0ee-02523dd160a1": {"doc_hash": "3d5ebae59490b24498643f3cc8f8c072d188f8f3f75b588cbc5ff51f3684fc51", "ref_doc_id": "4c84b615c79ae3c69552051e5b4c2f05c55718eb"}, "d9e4b838-2f23-4e87-8ceb-619184d9112b": {"doc_hash": "e252c66b583469889f64d8f4f76b648ebefda975affa90578d15131ed90df3bb", "ref_doc_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64"}, "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7": {"doc_hash": "2891fe8a4251bb475f776cb9a65eaae38597df1d5d69889063019579a6600f42", "ref_doc_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64"}, "8c8dd152-fc27-4e62-95f4-718e8b5b439f": {"doc_hash": "f79af4eaf174745131ac81a622a16dcbd38f96cc00c1110eef5943392d8c78c7", "ref_doc_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64"}, "c8d484d7-6307-46a3-9ebc-1bc83682cab3": {"doc_hash": "5d86b29756884de8d852ddaf4d9d64387e5cc467708f476814b4393150b2cea2", "ref_doc_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64"}, "91d40976-02c4-4352-b7af-0b5c5c750770": {"doc_hash": "026aed77ebab4dab3540c87b0c63fdf163bae9c1a8814b3e61a3bc6ccc4f63cf", "ref_doc_id": "e994a9b32cc2cff65d7d736018149f076cf4a9ab"}, "00758c9f-dbeb-4881-8ac4-5ab4808ba4a9": {"doc_hash": "57563deb352fb96e31552a280cd538bb860ecc99abe74c6ad791401e6bfdb7a2", "ref_doc_id": "b9a16ceebef3baf18c14472addad583f57beebb1"}, "9b356805-9161-4f68-908f-bb21afcdc0ff": {"doc_hash": "b458612c3ac50807592539ead28a3af20c0a3fa495be3bdb22bb48e4a15a9307", "ref_doc_id": "b9a16ceebef3baf18c14472addad583f57beebb1"}, "b3f28b63-c0e1-483d-96ea-5f23a0b23be8": {"doc_hash": "19700a36f21880849e561c9e800c0bd5e81be435e352f2e396613506d87e1d74", "ref_doc_id": "b9a16ceebef3baf18c14472addad583f57beebb1"}, "d23a1de1-a1cc-45d6-9402-c2dece59952b": {"doc_hash": "6807886e2bc2c337dc65572566557d737c42e66b74d4b11166d479407a3ec1d9", "ref_doc_id": "b9a16ceebef3baf18c14472addad583f57beebb1"}, "6d53c00f-6846-488d-92d6-609e35f935e4": {"doc_hash": "b6c4cec3042bbf08535d4d49acd31fdcffb4171a70b695e2c2f8894aa54a3e04", "ref_doc_id": "4507ac4676a6387c4b52a0d1111e94753a102b32"}, "bc67192d-5a82-4291-bb8b-0ab50a594df0": {"doc_hash": "5e21be734ae2b305ca2f0d7d6e732394f2bfd90124f0f3c5e5905055afdf0734", "ref_doc_id": "b930774918cc5aeecbaceff681af53f7a9117ec4"}, "9974d7a5-8002-433d-bba4-d6468dd3fc51": {"doc_hash": "a131e47077f7b798305bd78e08bb3308cf33bfa63ea6a50ff65f85b90caf0932", "ref_doc_id": "b930774918cc5aeecbaceff681af53f7a9117ec4"}, "2e828478-aeea-4834-9dae-713ba982e820": {"doc_hash": "db6ee1cb4c4b52e0d590508f69449628635944e8238e7693520e234b83bcf37e", "ref_doc_id": "b930774918cc5aeecbaceff681af53f7a9117ec4"}, "d73007a1-d8ba-4529-98ca-fa7c4645c93d": {"doc_hash": "053d2b7b4dc080e2fe5ddd7cfe6a1cd44be9a2774cff3ee4890beecc37510d79", "ref_doc_id": "b930774918cc5aeecbaceff681af53f7a9117ec4"}, "f5c852aa-0697-4490-8078-b3cb0759af1d": {"doc_hash": "6f3044f4dfa94aff9603ccd72f05a614019f6d82bea303ca1f39f86757709c7a", "ref_doc_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b"}, "89ab858a-7a78-403b-a251-18ebf50ed77d": {"doc_hash": "5b09a8a13cde594287c6c002b864bfb7212afdfec49dcf8f0a463543baeba8b6", "ref_doc_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b"}, "1fe92b9f-affa-4523-bf28-4ed02ed7bea2": {"doc_hash": "3da68f2b7668084cc3f2cfc29bbe05b9dd1dccec236291613585cf383ae2dfec", "ref_doc_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b"}, "c88b7851-7cf4-44eb-9963-5953e476fd83": {"doc_hash": "ea5739e05834c0216e112a351cf1843f78718a7e81336e1bbddcd6443a456dfe", "ref_doc_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b"}, "702c4176-4be1-4f05-b802-87ca415965ed": {"doc_hash": "22ad31a809849c19482940f83f82a4243df7d0d7365350f041cb78b6c6e947a5", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf": {"doc_hash": "aa1761dffbfab8951817bf1e00d8134525b2d1334489b77c738a581dff3e3c67", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "08253c87-cdad-45c9-8fab-c5a2218d3b99": {"doc_hash": "885154db763a1554e4ab1fc772150458fb6b037f9fcf2bd86f1726b4f2525537", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "d1159f79-6e7b-4d32-83cd-33cb636925aa": {"doc_hash": "1cafc5646e286fb09be20ec863f43845ffe15abcccfc8dedb9e83fb7cfd5e150", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "68259f3b-f030-42d1-a0ae-4a738dbbf2a0": {"doc_hash": "ff9a4d510e58b0b5560e302c34da00b293915ec03e9834753d8f5bcbc119ca24", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "c165d12b-54fe-459c-a3ce-0a0a8d62cb64": {"doc_hash": "c286536f9018539585997d5183b72aad2cfa1766e8819ed4152832e928f57965", "ref_doc_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7"}, "a89dd7c2-f147-4122-9799-0bc050ef67e9": {"doc_hash": "294a3cae70a71a747b30a5e3367c9506b9b5d668944d4fe892471f90f8d78064", "ref_doc_id": "da03217c2e9e57bb553c35fa72450cf6f482e29b"}, "69d0505d-88d9-4bdd-af2a-83241b7a199a": {"doc_hash": "99396863aa8ee795e8d443bdfe7690409dfeaca79dba8454d425ea0769c933d9", "ref_doc_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16"}, "2d455a7a-e2f5-4a7f-b258-e66708d8552f": {"doc_hash": "b497e330e500810c60c3b63ccba93ba3ba92ad7b01f36e5f76195f46e4b54008", "ref_doc_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16"}, "a6546d46-2cf5-4207-8761-523a1edf02ce": {"doc_hash": "d802414df268dc774a1e7bdc2d1cb53fb283b0c458585cf8683ff57ff185f5ef", "ref_doc_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16"}, "d82dab27-aec3-41d3-af4d-806388e449de": {"doc_hash": "c5597fab39aa41c178914eea76d3d012546d773d24b3e8f8793a5a4db8eb78f8", "ref_doc_id": "a2b76af6b9f0f9ff5150fb180e4d5bd4b467c6c3"}, "5640608b-e5be-4a2f-955f-08c83de5fa4e": {"doc_hash": "37b339c875ff8c7ddd76748ffcba22f3e3230fa53b58c089c8cfd752786ba4e0", "ref_doc_id": "94492d9bcbabe018649c059764d995c6dca54ac3"}, "34a79c52-1c4b-41b4-93aa-8ea0323966aa": {"doc_hash": "6689a241f9f4de7ab9b5d4b11166d7964c2a156d51f1ab85b74335cafd297cc3", "ref_doc_id": "94492d9bcbabe018649c059764d995c6dca54ac3"}, "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d": {"doc_hash": "b7c1ee9e90b47a16bdfbb0264801f1a9d7b04fb4994f072c95ecce0f32cfc4da", "ref_doc_id": "94492d9bcbabe018649c059764d995c6dca54ac3"}, "573dd2de-c2aa-4dae-83ec-4f26cb164093": {"doc_hash": "117563ec7ceb4ae235e73a04337fc046b64b17160d944de8f1a9a1ab03d82638", "ref_doc_id": "94492d9bcbabe018649c059764d995c6dca54ac3"}, "132cfefd-6bd0-41d2-95e7-9abb3536e098": {"doc_hash": "82d055b2fda16f28542789bc12349996ddcce5ac302b119d2f96e535c68da48a", "ref_doc_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d"}, "06c15a9b-114e-42e2-8aff-cd010d2ed4e3": {"doc_hash": "f5b6a55014dec942fce93c6af501149d16e293516657709dc0ea167e33324c67", "ref_doc_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d"}, "66b9dbd2-6d08-4776-8002-5a9dae4f54e3": {"doc_hash": "72ee1770cdfdfced228c71782480b7ce3e5bedc3ca028aa5e2d4a4b8ba1911c8", "ref_doc_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d"}, "8716fb21-57fa-4e2b-afd1-eb867ae9a550": {"doc_hash": "07f460b05599b7579008e0cd2893426815bae99e1722441fced0f37777552bb1", "ref_doc_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d"}}, "docstore/data": {"e5cfb31c-5236-468e-bfcc-e8f61a7d70d7": {"__data__": {"id_": "e5cfb31c-5236-468e-bfcc-e8f61a7d70d7", "embedding": null, "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "449df74210f7139bab3a667da805cbe995689b70", "node_type": "4", "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}, "hash": "e34464ee699f73a27d8b7fda83b0f9e5c29216d31d00fb30949b59fb309aa964", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e170b2de-aa38-48b5-a15a-0544cb14ed5a", "node_type": "1", "metadata": {}, "hash": "fcb21f0d4af1d841761a5db94a587175804f23ed847ab794a8d7fafdd4507620", "class_name": "RelatedNodeInfo"}}, "text": "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom transformers import AutoTokenizer, TextStreamer\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\nimport transformers\nimport torch\nimport ollama\nimport os\nimport chainlit as cl\nfrom chainlit.playground.config import add_llm_provider\nfrom chainlit.playground.providers.langchain import LangchainGenericProvider\nfrom transformers import LlamaForCausalLM\nfrom ollama import Llama\n\n\nchemin_cache_modele = \"/home/UHA/e2303253/U/modeleLLM\"\nos.environ[\"TRANSFORMERS_CACHE\"] = chemin_cache_modele\n\ntemplate = \"\"\"\nYou are a helpful AI assistant. Provide the answer for the following question:\n\nQuestion: {question}\nAnswer:from ollama import Llama\n\"\"\"\n\n# You need to be approved by HF & Meta https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main\n\n\n@cl.cache\ndef load_llama():\n    \n    model_path = \"/usr/share/ollama/.ollama/models\"\n    llama_model = ollama.load_model_from_manifest(model_path + \"/manifests/registry.ollama.ai\")\n    tokenizer = ollama.get_tokenizer(llama_model)\n\n    # Create a custom HuggingFacePipeline instance\n    pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=llama_model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        max_length=1000,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    llm = HuggingFacePipeline(\n        pipeline=pipeline,\n        model_kwargs={\"temperature\": 0},\n    )\n    return llm\n\n\nllm = load_llama()\n\nadd_llm_provider(\n    LangchainGenericProvider(\n        id=llm._llm_type, name=\"Llama3-Instruct-chat\", llm=llm, is_chat=False\n    )\n)", "start_char_idx": 0, "end_char_idx": 1778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e170b2de-aa38-48b5-a15a-0544cb14ed5a": {"__data__": {"id_": "e170b2de-aa38-48b5-a15a-0544cb14ed5a", "embedding": null, "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "449df74210f7139bab3a667da805cbe995689b70", "node_type": "4", "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}, "hash": "e34464ee699f73a27d8b7fda83b0f9e5c29216d31d00fb30949b59fb309aa964", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5cfb31c-5236-468e-bfcc-e8f61a7d70d7", "node_type": "1", "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}, "hash": "4682047358b81bb7f1bf9f77de4e7d6d4f8f2ed711d1af8e665950922d9b81f2", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_chat_start\nasync def main():\n\n    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n    llm_chain = LLMChain(prompt=prompt, llm=llm)\n    # print(llm_chain.run(\"A quel \u00e2ge est d\u00e9c\u00e9d\u00e9 Einstein?\"))\n    cl.user_session.set(\"llm_chain\", llm_chain)  # on assigne le mod\u00e8le \u00e0 la session\n\n    return llm_chain\n\n\n@cl.on_message\nasync def run(message: cl.Message):\n    cb = cl.AsyncLangchainCallbackHandler(\n        stream_final_answer=True, answer_prefix_tokens=[\"Answer\"]\n    )\n\n    # Retrieve the chain from the user session\n    llm_chain = cl.user_session.get(\n        \"llm_chain\"\n    )  # type: LLMChain \n    #on r\u00e9cup\u00e8re le mod\u00e8le ici\n    res = await llm_chain.acall(\n        message.content, callbacks=[cb]\n    )  # puis on applique le mod\u00e8le \u00e0 la question\n\n    if not cb.answer_reached:\n        await cl.Message(content=res[\"text\"]).send()", "start_char_idx": 1781, "end_char_idx": 2648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95d8c729-f596-470d-a1ff-f4e1ff3c3ff3": {"__data__": {"id_": "95d8c729-f596-470d-a1ff-f4e1ff3c3ff3", "embedding": null, "metadata": {"file_path": "OLLAMA_CHAT.MD", "file_name": "OLLAMA_CHAT.MD", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/OLLAMA_CHAT.MD"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd4e88c9310cf38842a3791cf351890b7028c06e", "node_type": "4", "metadata": {"file_path": "OLLAMA_CHAT.MD", "file_name": "OLLAMA_CHAT.MD", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/OLLAMA_CHAT.MD"}, "hash": "fba5b7e875ee0909b38253aa78f7809fa06ca5d7d5ddca790db8f818abaf0951", "class_name": "RelatedNodeInfo"}}, "text": "## Proc\u00e9dure de lancement\n\n- lancer curl -fsSL https://ollama.com/install.sh | sh pour installer ollama\n- puis ollama run llama3:8b pour installer le mod\u00e8le\n- http://localhost:11434/ permet de v\u00e9rifier le lancement du server\n\n\n   model = Ollama(   \n        base_url='http://localhost:11434',\n        model=\"llama3:8b\")\n        permet d'utiliser l'API REST de Ollama install\u00e9 localement\n\n\n## Sources\n\nhttps://github.com/ollama/ollama/blob/main/docs/tutorials/langchainpy.md\n\nhttps://github.com/sudarshan-koirala/langchain-ollama-chainlit/blob/main/simple_chaiui.py", "start_char_idx": 0, "end_char_idx": 563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6044ec8a-e179-4313-b5b4-6cec70c8671b": {"__data__": {"id_": "6044ec8a-e179-4313-b5b4-6cec70c8671b", "embedding": null, "metadata": {"file_path": "Ollama_chat.py", "file_name": "Ollama_chat.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "682b5f9b2cc0b4225e20e8312ccb41c21a07343e", "node_type": "4", "metadata": {"file_path": "Ollama_chat.py", "file_name": "Ollama_chat.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat.py"}, "hash": "3a0663328ad2e2f77a1dc7cfa9ff055a4c140cbc511c8b1371c9e431c9e16dde", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import Runnable\nfrom langchain.schema.runnable.config import RunnableConfig\n\nimport chainlit as cl\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    model = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\",verbose=False)\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a helpful AI assistant. Answer the question concisely based on the context.\",\n            ),\n            (\"human\", \"{question}\"),\n        ]\n    )\n    \n    runnable = prompt | model | StrOutputParser()\n\n    cl.user_session.set(\"runnable\", runnable)\n\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n\n    msg = cl.Message(content=\"\")\n\n    async for chunk in runnable.astream(\n        {\"question\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await msg.stream_token(chunk)\n\n    await msg.send()", "start_char_idx": 0, "end_char_idx": 1152, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12336feb-25eb-4296-a182-060ed3eda05b": {"__data__": {"id_": "12336feb-25eb-4296-a182-060ed3eda05b", "embedding": null, "metadata": {"file_path": "Ollama_chat_history.py", "file_name": "Ollama_chat_history.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat_history.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1cc21c73e8df2591264b7e1fe4b0f19ad41618f", "node_type": "4", "metadata": {"file_path": "Ollama_chat_history.py", "file_name": "Ollama_chat_history.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat_history.py"}, "hash": "f4a3396e50a2311260be4a1ccaa7ce0778d63df4cb3e937f7eb22f0fb4926d23", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import Runnable\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nimport chainlit as cl\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    model = Ollama(base_url=\"http://localhost:11434\", model=\"llama3\")\n\n    memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"input\", max_token_limit=1000)\n    # memory.chat_memory.add_user_message(\"Bonjour je suis Etudiant\")\n\n    prompt =  PromptTemplate(\n        input_variables=['history', 'input'],\n        template=\"\"\"\n        Tu es un bot de conversation fran\u00e7ais. Un utilisateur va communiquer avec toi. Maintiens un ton formel dans tes r\u00e9ponses.\n        Historique de conversation :\n        {history}\n\n        Humain : {input}\n        AI :\n        \"\"\"\n    )\n   \n    conversation = ConversationChain(memory=memory, prompt=prompt, llm=model)\n    cl.user_session.set(\"conversation\", conversation)\n\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    runnable = cl.user_session.get(\"conversation\")\n\n    msg = cl.Message(content=\"\")\n    async for chunk in runnable.astream(\n        {\"input\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        print(chunk)\n        await msg.stream_token(chunk[\"response\"])\n\n    await msg.send()", "start_char_idx": 0, "end_char_idx": 1574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5144edd0-f473-49ec-8901-a134b9c9b93b": {"__data__": {"id_": "5144edd0-f473-49ec-8901-a134b9c9b93b", "embedding": null, "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa7ad47f2921567bc51f1826c13278586e8960a7", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "hash": "b14913c2bcbf55fa5f1db3fe06de08ca7795ffc1278ed7be8ea66ac13b171e16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d92716bf-d0d9-491c-a9eb-0ba2a0412d37", "node_type": "1", "metadata": {}, "hash": "b166c93bf42ba4b0f5b7c34d331ee3cff36e7818765ea99c17e0e132c8d98d36", "class_name": "RelatedNodeInfo"}}, "text": "import requests\nfrom langchain_community.llms import Ollama\nfrom langchain_core.prompts import PromptTemplate\nfrom flask import Flask, request, jsonify\nimport requests\nimport os\nfrom dotenv import load_dotenv\n\n\"\"\"\nla version gratuite de ngrok fait changer l'url de ngrok chaque jour\ndonc pour l'utiliser il faut aller dans Settings -> Webhooks, et mettre \u00e0 jour l'url\navec celle apparaissant \u00e0 l'\u00e9cran en lancant ngrok http 5000\nsudo snap install ngrok\nngrok config add-authtoken (avec le token ici) # pour se connecter (il faut se cr\u00e9er un compte pour en obtenir un)\nsudo ngrok http 5000\ndans un autre terminal, lancer ce programme\n\n\"\"\"\napp = Flask(__name__)\nenv_path = os.path.join(os.path.dirname(__file__), '.env')\nload_dotenv(dotenv_path=env_path)\n\nREPO_NAME = os.getenv(\"REPO_NAME\")\nREPO_OWNER = os.getenv(\"REPO_OWNER\")\nGITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n\n\nllm_local = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:instruct\")\n\n\ndef get_diff(pull_number):\n    print(\"dans get_diff\")\n\n    url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/pulls/{pull_number}/files'\n    headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n    response = requests.get(url, headers=headers)\n    return response.json()\n\n\ndef get_pull_requests():\n    print(\"dans get_pull_requests\")\n\n    url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/pulls'\n    headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n    response = requests.get(url, headers=headers)\n    return response.json()", "start_char_idx": 0, "end_char_idx": 1502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d92716bf-d0d9-491c-a9eb-0ba2a0412d37": {"__data__": {"id_": "d92716bf-d0d9-491c-a9eb-0ba2a0412d37", "embedding": null, "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa7ad47f2921567bc51f1826c13278586e8960a7", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "hash": "b14913c2bcbf55fa5f1db3fe06de08ca7795ffc1278ed7be8ea66ac13b171e16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5144edd0-f473-49ec-8901-a134b9c9b93b", "node_type": "1", "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "hash": "8c7d61724f44edaebbbfa5d5172b743bc73b4f8f34297bb84d515519def0ba6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb17f6d0-566e-41d7-a415-95ecbe17d24c", "node_type": "1", "metadata": {}, "hash": "93b97798318931702c6b82a594bb3c50f094603522dd7db839470941aefe6ffa", "class_name": "RelatedNodeInfo"}}, "text": "def generate_comment(diff_files):\n    print(\"dans generate_comment\")\n\n    # Simulation de l'appel au mod\u00e8le Ollama\n    # Ici, on suppose que Ollama a une fonction `analyze_diff` qui prend les diff\u00e9rences et g\u00e9n\u00e8re un commentaire\n\n    system_instructions = \"\"\"\n    Vous \u00eates un assistant d\u00e9veloppeur fran\u00e7ais. \u00c9valuez les changements dans le code en termes de qualit\u00e9, de bonnes pratiques, et de performance. Fournissez une analyse d\u00e9taill\u00e9e et des recommandations sp\u00e9cifiques.\n    \"\"\"\n\n    prompt_template = PromptTemplate(\n        template=\"{instructions}\\n\\Changements dans le code par fichier: {context}\",\n        input_variables=[\"instructions\", \"context\"],\n    )\n\n    changes = \"\"\n    print(diff_files)\n    for file in diff_files:\n        filename = file['filename']\n        if filename is None:\n            filename = (\"(pas de nom de fichier)\")\n        patch = file.get('patch', '(pas de diff\u00e9rence d\u00e9tect\u00e9e)')\n        changes += f\"Fichier: {filename}\\nDiff:\\n{patch}\\n\\n\"\n\n    full_prompt = prompt_template.format(\n        instructions=system_instructions, context=changes)\n\n    response = llm_local.generate([full_prompt])\n    return response\n\n\ndef post_comment(pull_number, comment):\n    print(\"dans post_comment\")\n    url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues/{pull_number}/comments'\n    headers = {\n        'Authorization': f'token {GITHUB_TOKEN}',\n        'Content-Type': 'application/json'\n    }\n    data = {'body': comment}\n    response = requests.post(url, headers=headers, json=data)\n    return response.json()", "start_char_idx": 1505, "end_char_idx": 3059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb17f6d0-566e-41d7-a415-95ecbe17d24c": {"__data__": {"id_": "eb17f6d0-566e-41d7-a415-95ecbe17d24c", "embedding": null, "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa7ad47f2921567bc51f1826c13278586e8960a7", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "hash": "b14913c2bcbf55fa5f1db3fe06de08ca7795ffc1278ed7be8ea66ac13b171e16", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d92716bf-d0d9-491c-a9eb-0ba2a0412d37", "node_type": "1", "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}, "hash": "cceac6a9c5639988456b855965140a2a777a22fb4f2d2982050930786c326c9d", "class_name": "RelatedNodeInfo"}}, "text": "@app.route('/webhook', methods=['POST'])\ndef webhook():\n    data = request.json\n    if data['action'] in ['opened', 'synchronize']:\n        print(\"PR detected\")\n        pull_number = data['pull_request']['number']\n        diff_files = get_diff(pull_number)\n        comment = generate_comment(diff_files)\n        response = post_comment(pull_number, comment.generations[0][0].text)\n        print(response)\n        return jsonify(response)\n    return '', 200\n\n\nif __name__ == '__main__':\n    app.run(port=5000)", "start_char_idx": 3062, "end_char_idx": 3570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d5fdc24-4ab2-42c3-a4f3-87cc51359744": {"__data__": {"id_": "1d5fdc24-4ab2-42c3-a4f3-87cc51359744", "embedding": null, "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be741491bb050d86e011369fa21d499f748cb82c", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}, "hash": "b6d23fb84ba6b3a3573648c5513954dd057b17583b084bbef96ee754f881baec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9675bbca-c254-4679-9e54-7cee7471e43d", "node_type": "1", "metadata": {}, "hash": "2789942380c328da6448973e0a536007846cd44686916ad62555b31fe8a7654d", "class_name": "RelatedNodeInfo"}}, "text": "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings, load_index_from_storage\nfrom llama_index.readers.github import GithubRepositoryReader, GithubClient\n#from IPython.display import Markdown, display\nfrom llama_index.vector_stores.faiss import FaissVectorStore\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\nimport faiss\nimport os\nfrom dotenv import load_dotenv\n\nenv_path = os.path.join(os.path.dirname(__file__), '.env')\nload_dotenv(dotenv_path=env_path)\n\nREPO_NAME = os.getenv(\"REPO_NAME\")\nREPO_OWNER = os.getenv(\"REPO_OWNER\")\nGITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\nBRANCH = \"main\"\n\ngithub_client = GithubClient(github_token=GITHUB_TOKEN, verbose=True)\n\ndocuments = GithubRepositoryReader(\n    github_client=github_client,\n    owner=REPO_OWNER,\n    repo=REPO_NAME,\n    use_parser=False,\n    verbose=False,\n    filter_directories=(\n        [\"sandboxRAG/differents_textes\", \"sandboxRAG2\"],\n        GithubRepositoryReader.FilterType.EXCLUDE,\n    ),\n    filter_file_extensions=(\n        [\n            \".png\",\n            \".txt\",\n            \".jpg\",\n            \".jpeg\",\n            \".py\",\n            \".md\",\n            \"json\",\n            \".ipynb\",\n        ],\n        GithubRepositoryReader.FilterType.INCLUDE,\n    ),\n).load_data(branch=BRANCH)\nprint(documents)\n###\nembed_model = HuggingFaceEmbedding(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n)\nllm = Ollama(base_url=\"http://localhost:11434\",\n             model=\"llama3:instruct\", request_timeout=1000.0)\n\nd = 768\nfaiss_index = faiss.IndexFlatL2(d)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.", "start_char_idx": 0, "end_char_idx": 1746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9675bbca-c254-4679-9e54-7cee7471e43d": {"__data__": {"id_": "9675bbca-c254-4679-9e54-7cee7471e43d", "embedding": null, "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be741491bb050d86e011369fa21d499f748cb82c", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}, "hash": "b6d23fb84ba6b3a3573648c5513954dd057b17583b084bbef96ee754f881baec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d5fdc24-4ab2-42c3-a4f3-87cc51359744", "node_type": "1", "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}, "hash": "875818f03fe296b958954cddc6eb217a81acd1d903b9d29e72c65ef349712a9d", "class_name": "RelatedNodeInfo"}}, "text": "llm = llm\nSettings.embed_model = embed_model\nSettings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\nSettings.num_output = 512\nSettings.context_window = 3900\n\ndef charge_index(index_path):\n    if os.path.exists(index_path):\n        vector_store = FaissVectorStore.from_persist_dir(index_path)\n        storage_context = StorageContext.from_defaults(\n            vector_store=vector_store, persist_dir=index_path)\n        index = load_index_from_storage(storage_context=storage_context)\n        return index\n        storage_context = StorageContext.from_defaults(\n            persist_dir=index_path\n        )\n        print(\"Index existant charg\u00e9\")\n        return load_index_from_storage(storage_context)\n    else:\n        # Cr\u00e9er un nouvel index si non disponible\n        os.makedirs(index_path)\n        print(\"Cr\u00e9ation d'un nouvel index\")\n        vector_store = FaissVectorStore(faiss_index=faiss_index)\n        storage_context_global = StorageContext.from_defaults(\n            vector_store=vector_store)\n\n        index = VectorStoreIndex.from_documents(\n            documents, storage_context=storage_context_global, show_progress=True)\n        index.storage_context.persist(index_path)\n    return index\n\nindex = charge_index(\"index_github\")\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\n    \"Quelles sont les diff\u00e9rences entre un RAG langchain et un RAG llama_index?\",\n    # verbose=True,\n)\nprint(response.response)\n\n# display(Markdown(f\"{response}\"))\n\n\"\"\"\nhttps://docs.llamaindex.ai/en/stable/examples/data_connectors/GithubRepositoryReaderDemo/ (doc officielle)\nhttps://github.com/joaomdmoura/crewAI\nhttps://docs.crewai.com/tools/GitHubSearchTool (impossible d'importer crewAI)\nhttps://lightning.ai/lightning-ai/studios/chat-with-your-code-using-rag (a donn\u00e9 des pistes)\n\"\"\"", "start_char_idx": 1692, "end_char_idx": 3513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52bda799-0661-40a4-8d8e-2e00db6cd267": {"__data__": {"id_": "52bda799-0661-40a4-8d8e-2e00db6cd267", "embedding": null, "metadata": {"file_path": "PR_Reviewer/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/requirements.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fef500d8481bfcad975021a7b91ff340b84b34e0", "node_type": "4", "metadata": {"file_path": "PR_Reviewer/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/requirements.txt"}, "hash": "f45df7debf40c9c7c5a359328be12d275390bc4a63a93e7bb3e0ff3489e953ee", "class_name": "RelatedNodeInfo"}}, "text": "llama-index-readers-github\nflask\nllama-index-vector-stores-faiss\nllama-index-embeddings-huggingface", "start_char_idx": 0, "end_char_idx": 99, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ad1b863-e59a-4d2d-b9d7-e64865f5fd5c": {"__data__": {"id_": "3ad1b863-e59a-4d2d-b9d7-e64865f5fd5c", "embedding": null, "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4507ac4676a6387c4b52a0d1111e94753a102b32", "node_type": "4", "metadata": {"file_path": "chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/chainlit.md"}, "hash": "c4cc6083d1496f3c9edbec092b74395581f3ca8c56ce504de502d9f0d4c8a320", "class_name": "RelatedNodeInfo"}}, "text": "# Welcome to Chainlit! \ud83d\ude80\ud83e\udd16\n\nHi there, Developer! \ud83d\udc4b We're excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\n\n## Useful Links \ud83d\udd17\n\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) \ud83d\udcda\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! \ud83d\udcac\n\nWe can't wait to see what you create with Chainlit! Happy coding! \ud83d\udcbb\ud83d\ude0a\n\n## Welcome screen\n\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.", "start_char_idx": 0, "end_char_idx": 736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0845d479-118d-4f16-8175-5775b61a433f": {"__data__": {"id_": "0845d479-118d-4f16-8175-5775b61a433f", "embedding": null, "metadata": {"file_path": "mailAssistantByPrompts/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e59c940584cc87ebdfe475beed038fabc7490f6f", "node_type": "4", "metadata": {"file_path": "mailAssistantByPrompts/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/README.md"}, "hash": "3f83136e88275baee8eb5d8181a452cf228ebd1f2cb4ef93cfcf92488a0fa6de", "class_name": "RelatedNodeInfo"}}, "text": "### E-mail Assistant \n\nAssistant multi-modal (Texte, T\u00e9l\u00e9chargement de fichier) de messagerie.", "start_char_idx": 0, "end_char_idx": 94, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b11d6ce-59cb-4428-b0b3-6fefb138fd21": {"__data__": {"id_": "5b11d6ce-59cb-4428-b0b3-6fefb138fd21", "embedding": null, "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96255605229db00f87066d54854a2946f36be5c2", "node_type": "4", "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "hash": "62a31312f1e0faeb924628c6cac6ce5f62038341e0214d77e36f7ab52bd6953e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb10b6c8-ea71-427c-a6ab-5e090212ec10", "node_type": "1", "metadata": {}, "hash": "5f4680be9ae060f64268d60e496f409b315809a544f7d99721705471051c5f40", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.schema.runnable import RunnablePassthrough, RunnableLambda\nfrom langchain.memory import ConversationBufferMemory\n\nimport PyPDF2\n\nfrom operator import itemgetter\n\nimport chainlit as cl\nfrom chainlit.input_widget import TextInput, Select\n\n\nllm_local = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:instruct\")\n    \n@cl.on_chat_start\nasync def start():\n    cl.user_session.set(\"memory\", ConversationBufferMemory(return_messages=True))\n\n\n    await cl.ChatSettings(\n        [\n            TextInput(id=\"indications\", label=\"Indications\", initial=\"\"),\n            Select(\n                id=\"ton\",\n                label=\"Ton de la r\u00e9ponse\",\n                values=[\"Formel\", \"Enjou\u00e9\",\n                        \"Familier\", \"D\u00e9sol\u00e9\"],\n                initial_index=0,\n            ),\n        ]\n    ).send()\n\n@cl.on_settings_update\nasync def setup_agent(settings):\n    cl.user_session.set(\"settings\", settings)\n\n\n@cl.step(type=\"run\", name=\"Mise en place du Runnable\")\ndef setup_model():\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n\n\n\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"\"\"{sys_instruction}\\n\\nPrompt: {prompt}\"\"\",\n            ),\n            MessagesPlaceholder(variable_name=\"history\"),\n        ]\n    )\n\n    runnable_exercice = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n        )\n        | prompt\n        | llm_local\n        | StrOutputParser()\n    )\n    return runnable_exercice", "start_char_idx": 0, "end_char_idx": 1819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb10b6c8-ea71-427c-a6ab-5e090212ec10": {"__data__": {"id_": "eb10b6c8-ea71-427c-a6ab-5e090212ec10", "embedding": null, "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96255605229db00f87066d54854a2946f36be5c2", "node_type": "4", "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "hash": "62a31312f1e0faeb924628c6cac6ce5f62038341e0214d77e36f7ab52bd6953e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b11d6ce-59cb-4428-b0b3-6fefb138fd21", "node_type": "1", "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "hash": "97a68b378b323d1403a9934237a01542b60cc36783e8a0d4599249bb882b578d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "173af6aa-4d89-45c9-9c1f-4169ea8ba8d4", "node_type": "1", "metadata": {}, "hash": "eb5c2f13e3472383396a1399b3016ce92331cea75d52d9abe4e21a31d3be6b05", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_message\nasync def main(msg: cl.Message):\n\n    settings = cl.user_session.get(\"settings\")\n\n    system_instructions = \"\"\"\n    Vous \u00eates un assistant fran\u00e7ais. \n    \"\"\"\n\n    if settings :\n        if settings[\"ton\"]:\n            ton = f\"\"\"Vous devez r\u00e9pondre avec un ton {settings[\"ton\"]}.\"\"\"\n\n        if settings[\"indications\"]:\n            user_indications = f\"\"\"Vous devez r\u00e9pondre aux mails avec les [[indications]].\\nIndications : {settings[\"indications\"]}.\"\"\"\n\n    else:\n        ton = \"Vous devez r\u00e9pondre avec un ton formel.\"\n        user_indications = \"Vous devez r\u00e9pondre aux mails.\"\n\n    system_instructions += ton\n    system_instructions += user_indications", "start_char_idx": 1823, "end_char_idx": 2494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "173af6aa-4d89-45c9-9c1f-4169ea8ba8d4": {"__data__": {"id_": "173af6aa-4d89-45c9-9c1f-4169ea8ba8d4", "embedding": null, "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96255605229db00f87066d54854a2946f36be5c2", "node_type": "4", "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "hash": "62a31312f1e0faeb924628c6cac6ce5f62038341e0214d77e36f7ab52bd6953e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb10b6c8-ea71-427c-a6ab-5e090212ec10", "node_type": "1", "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}, "hash": "c3b3437376216a87710b8f04d39b92d75b4001caf73eb8235840f180657aea0c", "class_name": "RelatedNodeInfo"}}, "text": "if msg.elements:\n\n        # fichiers Pdf ou txt seulement\n        files = [file for file in msg.elements if \"pdf\" in file.mime or \"text/plain\" in file.mime]\n\n        if not files:\n            await cl.Message(content=\"Only PDF or Text file\").send()\n            return\n\n        content = \"\"\n        for file in files:\n            content += \"Fichier joint : \"+file.name+\"\\n\" \n            \n            if \"pdf\" in file.mime:\n                with open(file.path, \"rb\") as f:\n                    pdf_reader = PyPDF2.PdfReader(f)\n                    num_pages = len(pdf_reader.pages)\n\n                    for page_num in range(num_pages):\n                        page = pdf_reader.pages[page_num]\n                        content += page.extract_text()\n            \n            if \"text/plain\" in file.mime:\n                with open(file.path, \"r\") as f:\n                    content += f.read()\n\n            content += \"\\n\"\n\n        user_attached_files = f\"\"\"Vous devez r\u00e9pondre en prenant compte les [[pi\u00e8ces jointes]] aux mails.\\nPi\u00e8ces jointes : {content}\"\"\"\n    else:\n        user_attached_files = \"\"\n\n    system_instructions += user_attached_files\n    print(system_instructions)\n    runnable_model = setup_model()\n\n    prompt = msg.content\n\n    msg_res = cl.Message(content=\"\")\n\n    async for chunk in runnable_model.astream(\n        {\"sys_instruction\": system_instructions, \"prompt\": prompt},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await msg_res.stream_token(chunk)\n\n    await msg_res.send()", "start_char_idx": 2501, "end_char_idx": 4043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7387b719-1c3b-4d15-b0be-c8042b07e189": {"__data__": {"id_": "7387b719-1c3b-4d15-b0be-c8042b07e189", "embedding": null, "metadata": {"file_path": "mailAssistantByPrompts/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/requirements.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f50a824f030689bc98d3c9afb1374a9facc0d041", "node_type": "4", "metadata": {"file_path": "mailAssistantByPrompts/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/requirements.txt"}, "hash": "8c7417045560473780b7667dfc518ac09cdd669b0201b90b33888c8478aa28df", "class_name": "RelatedNodeInfo"}}, "text": "langchain_community\nlangchain\nPyPDF\nchainlit", "start_char_idx": 0, "end_char_idx": 44, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ced1dae-b900-4fa0-a219-a0266e2fcde1": {"__data__": {"id_": "5ced1dae-b900-4fa0-a219-a0266e2fcde1", "embedding": null, "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55", "node_type": "4", "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "hash": "4d4a5e5c78f852031b23ab4cac5925c0ffd0ebdca883ca0e12d37c4607eeb1f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f6806bd-0369-4dc6-889f-87537ab4e57e", "node_type": "1", "metadata": {}, "hash": "d7ad3f334d4435de2efa4a0843f870c88cd7720472296217d161bd27e5ad40a9", "class_name": "RelatedNodeInfo"}}, "text": "<!-- GETTING STARTED -->\n<a name=\"readme-top\"></a>\n# Mod\u00e8le \u00e0 des fins de G\u00e9n\u00e9ration et Aide \u00e0 la R\u00e9solution d'exercices math\u00e9matiques\n\n## Introduction\n\nLe but des programmes de ce dossier est de mettre en place un g\u00e9n\u00e9rateur d'exercices de math\u00e9matiques, qui permet ensuite d'aider \u00e0 la r\u00e9solution \nde l'exercice, en donnant des indices, ou en corrigeant l'exercice apr\u00e8s un certain nombre de tentatives.\n\nTrois versions du programme existent, mais exo_math_3prompts.py sert de version la plus aboutie, avec _prep_message_chainlit_3prompts.py_ pour r\u00e9partir le code.\nLes 2 autres fichiers de code repr\u00e9sentent des tentatives, chacun avec ses d\u00e9fauts. \n\n### Installation\n\n1. Avant de pouvoir lancer le programme, il est n\u00e9cessaire d'installer les d\u00e9pendances et logiciels requis, trouvables dans le fichier requirements.txt du dossier parent\n\n```sh\n  pip install -r ../requirements.txt\n  ```\n\n2. Il faudra ensuite t\u00e9l\u00e9charger le mod\u00e8le correspondant\n\n```sh\n  ollama pull llama3:instruct\n  ```\n\n3. Enfin, se cr\u00e9er un fichier .env avec les variables LITERAL_API_KEY et CHAINLIT_AUTH_SECRET, avec la premi\u00e8re n\u00e9cessitant de se cr\u00e9er un compte sur LiteralAI (puis aller dans Settings -> General Default Key), et la seconde qui est trouvable en tapant \n ```sh\n  chainlit create-secret\n  ```\n\n\n## Fonctionnement\n\n### Initialisation du programme\n\nAu lancement du programme sont initialis\u00e9es 2 variables servant de m\u00e9moire, l'une globale, qui sera utile pour quitter la discussion et y revenir, et une plus courte,\nqui retient les messages d'une discussion portant sur un exercice, et se vide d\u00e8s lors que l'exercice change, pour que le mod\u00e8le ait acc\u00e8s \u00e0 un contexte au moment de r\u00e9pondre. Ces 2 variables sont enrichies \u00e0 chaque parution de message, qu'il soit de l'utilisateur ou de l'IA.", "start_char_idx": 0, "end_char_idx": 1783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f6806bd-0369-4dc6-889f-87537ab4e57e": {"__data__": {"id_": "2f6806bd-0369-4dc6-889f-87537ab4e57e", "embedding": null, "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55", "node_type": "4", "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "hash": "4d4a5e5c78f852031b23ab4cac5925c0ffd0ebdca883ca0e12d37c4607eeb1f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5ced1dae-b900-4fa0-a219-a0266e2fcde1", "node_type": "1", "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "hash": "a2befdfddf0821108f36d38f1d1ce121df3419a77bf60110dc24a40db186c4a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "009b1bc3-d665-4a69-94ba-b3deda024b8d", "node_type": "1", "metadata": {}, "hash": "2953f27a7e6687e9564425ef8f345a5579b0451bdddabdbfddda92cf8024f4a8", "class_name": "RelatedNodeInfo"}}, "text": "Un message demandant les loisirs de l'utilisateur apparait,\net la r\u00e9ponse de l'utilisateur est enregistr\u00e9e, car elle sera utilis\u00e9e pour g\u00e9n\u00e9rer les exercices.\n\n### G\u00e9n\u00e9ration d'exercice\n\nLa fonction setup_exercice_model() commence par r\u00e9cup\u00e9rer la discussion actuelle, puis les loisirs de l'utilisateur. Elle va utiliser ses donn\u00e9es, ainsi qu'un prompt sp\u00e9cifiquement \u00e9crit pour que le mod\u00e8le soit \u00e0 m\u00eame de cr\u00e9er des exercices suivant certaines r\u00e8gles, et va les passer \u00e0 un objet Runnable \u00e0 renvoyer.\n\nLe Runnable renvoy\u00e9 est utilis\u00e9 pour r\u00e9cup\u00e9rer une r\u00e9ponse du mod\u00e8le, qui sera donc un exercice de math\u00e9matique. L'exercice est enregistr\u00e9 dans la session, pour que le correcteur y ait acc\u00e8s facilement. Enfin le flag \"compris\" lui aussi dans la session est pass\u00e9 \u00e0 False, pour appeler le correcteur.\n\n### R\u00e9solution d'exercice\n\nLa fonction setup_aide_model() r\u00e9cup\u00e8re elle aussi la discussion en m\u00e9moire, puis renvoit un Runnable avec un prompt plus pr\u00e9cis que lors de la g\u00e9n\u00e9ration, car c'est \u00e0 partir de ce prompt que le mod\u00e8le communiquera avec l'utilisateur la plupart du temps. Le prompt est cens\u00e9 aider l'utilisateur par le biais d'indices, sans toutefois donner la r\u00e9ponse, mais au bout d'un certain nombre de tentatives (3 ici), le mod\u00e8le donne la r\u00e9ponse.\n\nUne fois le message d'aide envoy\u00e9, la fonction verifie_comprenhension est appel\u00e9e, demandant \u00e0 l'utilisateur s'il a compris la r\u00e9ponse. Si oui, \"compris\" passe \u00e0 True, et on rappelle le g\u00e9n\u00e9rateur, sinon, le mod\u00e8le doit continuer de fournir des indices ou la r\u00e9ponse, suivant le nombre de tentatives.\n\nEn repassant au g\u00e9n\u00e9rateur, la m\u00e9moire courte est vid\u00e9e.\n\n### Autres\n\nLe programme dispose aussi de fonctions permettant de quitter la discussion pour la reprendre plus tard, ou de changer l'\u00e2ge de l'utilisateur aux yeux du mod\u00e8le.", "start_char_idx": 1784, "end_char_idx": 3587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "009b1bc3-d665-4a69-94ba-b3deda024b8d": {"__data__": {"id_": "009b1bc3-d665-4a69-94ba-b3deda024b8d", "embedding": null, "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89f4ccb8b518fb2addd6968f34e02f7e33086f55", "node_type": "4", "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "hash": "4d4a5e5c78f852031b23ab4cac5925c0ffd0ebdca883ca0e12d37c4607eeb1f9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f6806bd-0369-4dc6-889f-87537ab4e57e", "node_type": "1", "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}, "hash": "c2b155fb57ad3510ae8ce258b4f81a524ecd8a81b75103b0f283193b87ce44c6", "class_name": "RelatedNodeInfo"}}, "text": "Il est \u00e0 noter que bien que l'IA ait conscience de l'\u00e2ge, elle n'arrive pas \u00e0 g\u00e9n\u00e9r\u00e9s des exercices qui y sont pertinents.\n\n\n## Int\u00e9r\u00eat par rapport aux pr\u00e9c\u00e9dentes versions\n\nLe fichier exo_math_3prompts.py utilise un prompt pour aier l'utilisateur avec les indices, et un autre pour donner la r\u00e9ponse. Cela cr\u00e9ait des malentendus dans certains cas, en plus de ne pas \u00eatre tr\u00e8s fluide.\n\n<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>", "start_char_idx": 3588, "end_char_idx": 4034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e41d42f9-f77c-45b1-95e4-cf84f9bed0cb": {"__data__": {"id_": "e41d42f9-f77c-45b1-95e4-cf84f9bed0cb", "embedding": null, "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4507ac4676a6387c4b52a0d1111e94753a102b32", "node_type": "4", "metadata": {"file_path": "modele_personnalise/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/chainlit.md"}, "hash": "4941fab5ace47e8eb03c6db5d708ac85163b9ce0c4660c12543dab540c79b6b1", "class_name": "RelatedNodeInfo"}}, "text": "# Welcome to Chainlit! \ud83d\ude80\ud83e\udd16\n\nHi there, Developer! \ud83d\udc4b We're excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\n\n## Useful Links \ud83d\udd17\n\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) \ud83d\udcda\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! \ud83d\udcac\n\nWe can't wait to see what you create with Chainlit! Happy coding! \ud83d\udcbb\ud83d\ude0a\n\n## Welcome screen\n\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.", "start_char_idx": 0, "end_char_idx": 736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20b1145a-267e-4b48-ada5-bebc2e453add": {"__data__": {"id_": "20b1145a-267e-4b48-ada5-bebc2e453add", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "hash": "9c8366b4403e84433cc0771235a5a7493c9745756a05f3c6f324d44d384f2a9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2", "node_type": "1", "metadata": {}, "hash": "59bbf8615c2e1882f9a3f3a11fd6608381d2ab3f17d671f13776a0557e8ba4f7", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.schema.runnable.config import RunnableConfig\nimport chainlit as cl\nfrom chainlit.input_widget import Slider\nfrom langchain.memory import ConversationBufferMemory\n\nfrom chainlit.types import ThreadDict\n\nfrom prep_message_chainlit_3prompts import *\n\n\n\n@cl.password_auth_callback\ndef auth_callback(username: str, password: str):\n    # Fetch the user matching username from your database\n    # and compare the hashed password with the value stored in the database\n    if (username, password) == (\"elias\", \"elias\"):\n        return cl.User(\n            identifier=\"Elias\", metadata={\"role\": \"admin\", \"provider\": \"credentials\"}\n        )\n    else:\n        return None\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    await cl.ChatSettings(\n        [\n            Slider(\n                id=\"age_cible\",\n                label=\"Age niveau exercice\",\n                initial=5,\n                min=3,\n                max=22,\n                step=1,\n                tooltip=\"en ann\u00e9es\",\n            ),\n        ]\n    ).send()\n\n    loisirs = await cl.AskUserMessage(content=\"Quels sont vos centres d'int\u00e9r\u00eat?\", author=\"Aide\", timeout=3000).send()\n    # print(loisirs[\"output\"])\n    cl.user_session.set(\"loisirs\", loisirs[\"output\"])\n    response = \"Merci! Quel genre d'exercice voulez-vous?\"\n    await cl.Message(content=response, author=\"Aide\").send()\n\n    cl.user_session.set(\"compris\", True)\n    cl.user_session.set(\"tentatives\", 1)\n    cl.user_session.set(\"memory\", ConversationBufferMemory(return_messages=True))\n    cl.user_session.set(\"memory_discussion\", ConversationBufferMemory(return_messages=True))\n\n\n\n\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    \"\"\"\n    Callback fonction appel\u00e9e \u00e0 chaque r\u00e9ception d'un message de l'utilisateur.\n    G\u00e8re la logique principale de la conversation en fonction de l'\u00e9tat de la session utilisateur.\n\n    Args:\n        message (cl.Message): Le message envoy\u00e9 par l'utilisateur.", "start_char_idx": 0, "end_char_idx": 1984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2": {"__data__": {"id_": "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "hash": "9c8366b4403e84433cc0771235a5a7493c9745756a05f3c6f324d44d384f2a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "20b1145a-267e-4b48-ada5-bebc2e453add", "node_type": "1", "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "hash": "185475f66250b0e7a4621a745889b002ea1bc8eca033e18fb693b192689cdb33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "434447fb-16a4-413b-a73b-6979b5f28d3d", "node_type": "1", "metadata": {}, "hash": "f74e90116c88c126e2cc7f52c5c6c75c7a3ade2d3bfbd490251ac3b7c0971e6f", "class_name": "RelatedNodeInfo"}}, "text": "Args:\n        message (cl.Message): Le message envoy\u00e9 par l'utilisateur.\n\n    Returns:\n        None : Envoie une r\u00e9ponse appropri\u00e9e \u00e0 l'utilisateur en fonction du contexte de la conversation.\n    \"\"\"\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n    current_discussion = cl.user_session.get(\"memory_discussion\")  # type: ConversationBufferMemory\n\n    if cl.user_session.get(\"age_niveau\"):\n        niveau_scolaire = str(cl.user_session.get(\"age_niveau\"))+\" ans\"\n    else:\n        niveau_scolaire = \"5 ans\"\n\n    if cl.user_session.get(\"compris\") == True:  # partie g\u00e9n\u00e9ration d'exercice\n        dernier_exo = \"\"\n        print(\"partie g\u00e9n\u00e9ration d'exercice\")\n        runnable = setup_exercice_model()\n\n        msg = cl.Message(content=\"\", author=\"G\u00e9n\u00e9rateur\")\n        async for chunk in runnable.astream(\n            {\"question\": message.content, \"niveau_scolaire\": niveau_scolaire},\n            config=RunnableConfig(\n                callbacks=[cl.LangchainCallbackHandler()]),\n        ):\n            await msg.stream_token(chunk)\n            dernier_exo += chunk\n        cl.user_session.set(\"dernier_exo\", dernier_exo)\n        cl.user_session.set(\"compris\", False)\n        await msg.send()\n\n        memory.chat_memory.add_user_message(message.content)\n        memory.chat_memory.add_ai_message(msg.content)\n        current_discussion.chat_memory.add_user_message(message.content)\n        current_discussion.chat_memory.add_ai_message(msg.content)\n\n        # partie correction d'exercice\n    elif cl.user_session.get(\"compris\") == False:\n        runnable = setup_aide_model()\n\n        msg = cl.Message(content=\"\", author=\"Correcteur\")\n        async for chunk in runnable.astream(\n            {\"question\": message.content,\n                \"dernier_exo\": cl.user_session.get(\"dernier_exo\"),", "start_char_idx": 1912, "end_char_idx": 3729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434447fb-16a4-413b-a73b-6979b5f28d3d": {"__data__": {"id_": "434447fb-16a4-413b-a73b-6979b5f28d3d", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "hash": "9c8366b4403e84433cc0771235a5a7493c9745756a05f3c6f324d44d384f2a9a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2", "node_type": "1", "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}, "hash": "20140085069fb42353682a2acaceb060b7c99b0630495bc9b0caff126d8f9c6c", "class_name": "RelatedNodeInfo"}}, "text": "user_session.get(\"dernier_exo\"),\n                \"niveau_scolaire\": niveau_scolaire,\n                \"tentatives\":cl.user_session.get(\"tentatives\")\n             },\n            config=RunnableConfig(\n                callbacks=[cl.LangchainCallbackHandler()]),\n        ):\n            await msg.stream_token(chunk)\n        print(\"msg:\"+str(msg.content))\n        await msg.send()\n        memory.chat_memory.add_user_message(message.content)\n        memory.chat_memory.add_ai_message(msg.content)\n        current_discussion.chat_memory.add_user_message(message.content)\n        current_discussion.chat_memory.add_ai_message(msg.content)\n        await verifie_comprehension()\n\n\n@cl.on_settings_update\nasync def setup_agent(settings):\n    cl.user_session.set(\"age_niveau\", settings['age_cible'])\n\n\n@cl.on_chat_resume\nasync def on_chat_resume(thread: ThreadDict):\n    memory = ConversationBufferMemory(return_messages=True)\n    root_messages = [m for m in thread[\"steps\"] if m[\"parentId\"] == None]\n    for message in root_messages:\n        if message[\"type\"] == \"user_message\":\n            memory.chat_memory.add_user_message(message[\"output\"])\n        else:\n            memory.chat_memory.add_ai_message(message[\"output\"])\n\n    cl.user_session.set(\"memory\", memory)\n\n    await cl.ChatSettings(\n        [\n            Slider(\n                id=\"age_cible\",\n                label=\"Age niveau exercice\",\n                initial=8,\n                min=3,\n                max=22,\n                step=1,\n                tooltip=\"en ann\u00e9es\",\n            ),\n        ]\n    ).send()\n\n    setup_exercice_model()", "start_char_idx": 3697, "end_char_idx": 5291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2e2b4be-05bd-4a3f-84db-6cbcb963d58a": {"__data__": {"id_": "f2e2b4be-05bd-4a3f-84db-6cbcb963d58a", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "hash": "d76047ee52625dc73c551878472962bf0ca123d0558990b1189939799090987f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "668702ac-9686-41e6-a060-d49ecdb511a6", "node_type": "1", "metadata": {}, "hash": "fe08ae14e5698b86a4edc13b41bb72df97ead886de47cc2630f692db876b5631", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.schema.runnable.config import RunnableConfig\nimport chainlit as cl\nfrom chainlit.input_widget import Slider\nfrom langchain.memory import ConversationBufferMemory\n\nfrom chainlit.types import ThreadDict\n\nfrom prep_message_chainlit_3prompts import *\n\n\n\n@cl.password_auth_callback\ndef auth_callback(username: str, password: str):\n    # Fetch the user matching username from your database\n    # and compare the hashed password with the value stored in the database\n    if (username, password) == (\"elias\", \"elias\"):\n        return cl.User(\n            identifier=\"Elias\", metadata={\"role\": \"admin\", \"provider\": \"credentials\"}\n        )\n    else:\n        return None\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    await cl.ChatSettings(\n        [\n            Slider(\n                id=\"age_cible\",\n                label=\"Age niveau exercice\",\n                initial=5,\n                min=3,\n                max=22,\n                step=1,\n                tooltip=\"en ann\u00e9es\",\n            ),\n        ]\n    ).send()\n\n    loisirs = await cl.AskUserMessage(content=\"Quels sont vos centres d'int\u00e9r\u00eat?\", author=\"Aide\", timeout=3000).send()\n    # print(loisirs[\"output\"])\n    cl.user_session.set(\"loisirs\", loisirs[\"output\"])\n    response = \"Merci! Quel genre d'exercice voulez-vous?\"\n    await cl.Message(content=response, author=\"Aide\").send()\n\n    cl.user_session.set(\"compris\", True)\n    cl.user_session.set(\"tentatives\", 0)\n    cl.user_session.set(\"memory\", ConversationBufferMemory(return_messages=True))\n\n\n\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    \"\"\"\n    Callback fonction appel\u00e9e \u00e0 chaque r\u00e9ception d'un message de l'utilisateur.\n    G\u00e8re la logique principale de la conversation en fonction de l'\u00e9tat de la session utilisateur.\n\n    Args:\n        message (cl.Message): Le message envoy\u00e9 par l'utilisateur.", "start_char_idx": 0, "end_char_idx": 1890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668702ac-9686-41e6-a060-d49ecdb511a6": {"__data__": {"id_": "668702ac-9686-41e6-a060-d49ecdb511a6", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "hash": "d76047ee52625dc73c551878472962bf0ca123d0558990b1189939799090987f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2e2b4be-05bd-4a3f-84db-6cbcb963d58a", "node_type": "1", "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "hash": "884aaf55c7cab6f7735a867436e9785d31d0e63f7ae67dd0f79cffd46e73ea84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a20f4ab-e30a-4bef-bd32-ee56f9899299", "node_type": "1", "metadata": {}, "hash": "703ba1dff28c599f0f715c86f2d65f60cf942637c9adf71dfb118405db551952", "class_name": "RelatedNodeInfo"}}, "text": "Args:\n        message (cl.Message): Le message envoy\u00e9 par l'utilisateur.\n\n    Returns:\n        None : Envoie une r\u00e9ponse appropri\u00e9e \u00e0 l'utilisateur en fonction du contexte de la conversation.\n    \"\"\"\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n\n    if cl.user_session.get(\"age_niveau\"):\n        niveau_scolaire = str(cl.user_session.get(\"age_niveau\"))+\" ans\"\n    else:\n        niveau_scolaire = \"5 ans\"\n\n    if cl.user_session.get(\"compris\") == True:  # partie g\u00e9n\u00e9ration d'exercice\n        dernier_exo = \"\"\n        print(\"partie g\u00e9n\u00e9ration d'exercice\")\n        runnable = setup_exercice_model()\n\n        msg = cl.Message(content=\"\", author=\"G\u00e9n\u00e9rateur\")\n        async for chunk in runnable.astream(\n            {\"question\": message.content, \"niveau_scolaire\": niveau_scolaire},\n            config=RunnableConfig(\n                callbacks=[cl.LangchainCallbackHandler()]),\n        ):\n            await msg.stream_token(chunk)\n            dernier_exo += chunk\n        cl.user_session.set(\"dernier_exo\", dernier_exo)\n        cl.user_session.set(\"compris\", False)\n        await msg.send()\n\n        memory.chat_memory.add_user_message(message.content)\n        memory.chat_memory.add_ai_message(msg.content)\n\n        # partie correction d'exercice\n    elif cl.user_session.get(\"compris\") == False:\n        runnable = setup_corrige_model()\n\n        msg = cl.Message(content=\"\", author=\"Correcteur\")\n        async for chunk in runnable.astream(\n            {\"question\": message.content,\n                \"dernier_exo\": cl.user_session.get(\"dernier_exo\"),\n                \"niveau_scolaire\": niveau_scolaire\n             },\n            config=RunnableConfig(\n                callbacks=[cl.LangchainCallbackHandler()]),\n        ):\n            await msg.stream_token(chunk)\n        print(\"msg:\"+str(msg.content))\n        await msg.send()\n        memory.chat_memory.add_user_message(message.content)\n        memory.chat_memory.add_ai_message(msg.content)\n        await verifie_comprehension()", "start_char_idx": 1818, "end_char_idx": 3831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a20f4ab-e30a-4bef-bd32-ee56f9899299": {"__data__": {"id_": "8a20f4ab-e30a-4bef-bd32-ee56f9899299", "embedding": null, "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0", "node_type": "4", "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "hash": "d76047ee52625dc73c551878472962bf0ca123d0558990b1189939799090987f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "668702ac-9686-41e6-a060-d49ecdb511a6", "node_type": "1", "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}, "hash": "c2d8a007da9e340c46d5d6a3efeb869c6abdfe9eeb7266866f8cd55acb3946df", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_settings_update\nasync def setup_agent(settings):\n    cl.user_session.set(\"age_niveau\", settings['age_cible'])\n\n\n@cl.on_chat_resume\nasync def on_chat_resume(thread: ThreadDict):\n    memory = ConversationBufferMemory(return_messages=True)\n    root_messages = [m for m in thread[\"steps\"] if m[\"parentId\"] == None]\n    for message in root_messages:\n        if message[\"type\"] == \"user_message\":\n            memory.chat_memory.add_user_message(message[\"output\"])\n        else:\n            memory.chat_memory.add_ai_message(message[\"output\"])\n\n    cl.user_session.set(\"memory\", memory)\n\n    await cl.ChatSettings(\n        [\n            Slider(\n                id=\"age_cible\",\n                label=\"Age niveau exercice\",\n                initial=8,\n                min=3,\n                max=22,\n                step=1,\n                tooltip=\"en ann\u00e9es\",\n            ),\n        ]\n    ).send()\n\n    setup_exercice_model()", "start_char_idx": 3834, "end_char_idx": 4756, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf6b6221-b673-4f11-a2a4-4ce633f7e050": {"__data__": {"id_": "cf6b6221-b673-4f11-a2a4-4ce633f7e050", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4403db4-6635-400e-bfe9-65a29304b667", "node_type": "1", "metadata": {}, "hash": "5a3f92b04824d735dea4179cb83dd78df96614163bdf8899c535d2743b79835e", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import Runnable, RunnablePassthrough, RunnableLambda\nfrom langchain.memory import ConversationBufferMemory\nimport chainlit as cl\nfrom operator import itemgetter\n\nmodel = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:instruct\")\n\nasync def verifie_comprehension():\n\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n    current_discussion = cl.user_session.get(\"memory_discussion\")\n\n    res = await cl.AskActionMessage(\n        content=\"Avez-vous compris?\",\n        actions=[\n            cl.Action(name=\"continue\", value=\"compris\", label=\"\u2705 Compris\"),\n            cl.Action(name=\"cancel\", value=\"pas_compris\",\n                      label=\"\u274c Pas compris\"),\n        ],\n        disable_feedback=True,\n        author=\"Correcteur\",\n        timeout=3000\n    ).send()", "start_char_idx": 0, "end_char_idx": 985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4403db4-6635-400e-bfe9-65a29304b667": {"__data__": {"id_": "f4403db4-6635-400e-bfe9-65a29304b667", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf6b6221-b673-4f11-a2a4-4ce633f7e050", "node_type": "1", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "c5458038e88ea364d8794898c8628ed9a63bbc3d4d9f45604da6a39bb30bb5aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99e19d07-c7e1-45c0-a54f-1e1797ca4faa", "node_type": "1", "metadata": {}, "hash": "94427560ddc9d6e015a8d78fdaacd153e9158b664b06d884e14266660edc684e", "class_name": "RelatedNodeInfo"}}, "text": "if res.get(\"value\") == \"pas_compris\":\n        cl.user_session.set(\"compris\", False)\n        cl.user_session.set(\"tentatives\", cl.user_session.get(\"tentatives\")+1)\n\n        msg = await cl.Message(\n            content=\"Qu'avez-vous pas compris?\",\n        ).send()\n\n        current_discussion.chat_memory.add_ai_message(\"Avez-vous compris?\")\n        current_discussion.chat_memory.add_user_message(res.get(\"value\"))\n        current_discussion.chat_memory.add_ai_message(msg.content)\n    else:\n        cl.user_session.set(\"compris\", True)\n        cl.user_session.set(\"tentatives\", 1)\n\n        msg = await cl.Message(\n            content=\"F\u00e9licitations! Quel autre type d'exercice voulez-vous?\",\n        ).send()\n        # on reset la m\u00e9moire actuelle lorsqu'on change d'exercice\n        cl.user_session.set(\"memory_discussion\",ConversationBufferMemory(return_messages=True))\n    \n    # on met \u00e0 jour l'historique global et actuel\n    memory.chat_memory.add_ai_message(\"Avez-vous compris?\")\n    memory.chat_memory.add_user_message(res.get(\"value\"))\n    memory.chat_memory.add_ai_message(msg.content)", "start_char_idx": 992, "end_char_idx": 2086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99e19d07-c7e1-45c0-a54f-1e1797ca4faa": {"__data__": {"id_": "99e19d07-c7e1-45c0-a54f-1e1797ca4faa", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4403db4-6635-400e-bfe9-65a29304b667", "node_type": "1", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "4efd385a7aecd56481a4239ce56747e73c4a739d649baed3447840294876531a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2fc86b9-9bae-4884-8a68-92756ccbdbb1", "node_type": "1", "metadata": {}, "hash": "92db3dfdd15abe0650a84b09a72ae8075a54bbea9b119c7e9037237bd4dbf9db", "class_name": "RelatedNodeInfo"}}, "text": "@cl.step(type=\"run\", name=\"runnable_generation\")\ndef setup_exercice_model():\n    \"\"\"\n    Configure le prompt et le Runnable pour g\u00e9n\u00e9rer des exercices de math\u00e9matiques personnalis\u00e9s en fonction des centres d'int\u00e9r\u00eat de l'utilisateur.\n\n    Returns:\n        None : Met \u00e0 jour la session utilisateur avec le nouveau Runnable pour g\u00e9n\u00e9rer des exercices.\n    \"\"\"\n\n    memory = cl.user_session.get(\"memory_discussion\")  # type: ConversationBufferMemory\n    loisirs = cl.user_session.get(\"loisirs\")\n    prompt_exercice = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"Tu parles uniquement fran\u00e7ais. Ton r\u00f4le est de cr\u00e9er un seul exercice de math\u00e9matiques auquel l'utilisateur doit trouver la r\u00e9ponse,\\\n            en te basant sur un ou plusieurs int\u00e9r\u00eats suivants : \" + loisirs + \". L'exercice doit impliquer :{question}, et \u00eatre du niveau d'un \u00e9l\u00e8ve ayant {niveau_scolaire}.\"\n            ),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"{question}\")\n        ]\n    )\n\n    runnable_exercice = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(\n                memory.load_memory_variables) | itemgetter(\"history\")\n        ) \n        | prompt_exercice\n        | model\n        | StrOutputParser()\n    )\n    # cl.user_session.set(\"runnable\", runnable_exercice)\n    return runnable_exercice", "start_char_idx": 2090, "end_char_idx": 3481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2fc86b9-9bae-4884-8a68-92756ccbdbb1": {"__data__": {"id_": "a2fc86b9-9bae-4884-8a68-92756ccbdbb1", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99e19d07-c7e1-45c0-a54f-1e1797ca4faa", "node_type": "1", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "29110d0ce89e306956f13582b8f783fc29ed2dba0f574961ed68e77a239c2970", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e649ee5-5f73-4c32-a500-0db71926e9b1", "node_type": "1", "metadata": {}, "hash": "bcff58533cdad6344114b095d486b75478353e0d32aa3a0dae6f8f3af3f30a53", "class_name": "RelatedNodeInfo"}}, "text": "@cl.step(type=\"run\", name=\"runnable_corrige\")\ndef setup_corrige_model():\n    memory = cl.user_session.get(\"memory_discussion\")  # type: ConversationBufferMemory\n    if cl.user_session.get(\"tentatives\") < 3:\n        print(\"partie aide d'exercice\")\n        print(\"Nombre de tentatives faites: \" +\n              str(cl.user_session.get(\"tentatives\")))\n        prompt_corrige = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"Tu es un maitre d'\u00e9cole avec des \u00e9l\u00e8ves de {niveau_scolaire} fran\u00e7ais .Tu dois aider l'utilisateur \\\n                    \u00e0 r\u00e9soudre l'exercice de math\u00e9matiques suivant: {dernier_exo}. \\\n                Si la r\u00e9ponse de l'utilisateur n'est pas correcte, donne un indice utile pour aider l'utilisateur \u00e0 trouver la solution. \\\n                S'il r\u00e9pond correctement, f\u00e9licite-le. Tu ne dois jamais donner la r\u00e9ponse toi-m\u00eame.\"\n                ),\n                MessagesPlaceholder(variable_name=\"history\"),\n                (\"human\", \"{question}\")\n            ]\n        )\n    else:\n        print(\"partie correction d'exercice\")\n        prompt_corrige = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"Tu parles uniquement fran\u00e7ais. Ton r\u00f4le est de corriger l'exercice de math\u00e9matiques suivant: {dernier_exo}. \\\n                Si la r\u00e9ponse {question} donn\u00e9e par l'utilisateur est juste, f\u00e9licite-le. \\\n                Sinon, dis-lui qu'il a faux, et dis-lui la correction de l'exercice.\"", "start_char_idx": 3484, "end_char_idx": 5032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e649ee5-5f73-4c32-a500-0db71926e9b1": {"__data__": {"id_": "2e649ee5-5f73-4c32-a500-0db71926e9b1", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2fc86b9-9bae-4884-8a68-92756ccbdbb1", "node_type": "1", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "54ec2ef9c0ca07e829fae6e7a3fcc6c3f65e9957aad74d9b52ef60dd70223052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bba38736-3c6e-4460-846e-9105ddf2cf6c", "node_type": "1", "metadata": {}, "hash": "dfbbc223cb375dcd5dd2831db01e796fc75b6af88b372d5ffa8bc677434d4a36", "class_name": "RelatedNodeInfo"}}, "text": "),\n                MessagesPlaceholder(variable_name=\"history\"),\n                (\"human\", \"{question}\")\n            ]\n        )\n\n    runnable_corrige = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n            )\n            | prompt_corrige\n            | model \n            | StrOutputParser())\n    # cl.user_session.set(\"runnable\", runnable_corrige)\n    if cl.user_session.get(\"compris\") == True:\n        cl.user_session.set(\"dernier_exo\", \"\")\n\n    return runnable_corrige", "start_char_idx": 5049, "end_char_idx": 5604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bba38736-3c6e-4460-846e-9105ddf2cf6c": {"__data__": {"id_": "bba38736-3c6e-4460-846e-9105ddf2cf6c", "embedding": null, "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9efa71524a0ed4c2d73cbd0de801185f55016f48", "node_type": "4", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d59f142b1fa15e6f60795d56096b5e9549ebc434d711fb2fc5bd62798bcb1d4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e649ee5-5f73-4c32-a500-0db71926e9b1", "node_type": "1", "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}, "hash": "d32e71a11d75150fb9bc3bae73014d233bba5e97a94d7170fd883849c634079a", "class_name": "RelatedNodeInfo"}}, "text": "@cl.step(type=\"run\", name=\"aide r\u00e9ponse\")\ndef setup_aide_model():\n    memory = cl.user_session.get(\"memory_discussion\")  # type: ConversationBufferMemory\n    \n    prompt_corrige = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"Tu es un ma\u00eetre d'\u00e9cole avec des \u00e9l\u00e8ves de {niveau_scolaire} fran\u00e7ais. Ton r\u00f4le est d'aider l'utilisateur \u00e0 r\u00e9soudre l'exercice de math\u00e9matiques suivant : {dernier_exo}. \"\n            \"Si la r\u00e9ponse de l'utilisateur n'est pas correcte, donne un indice utile pour l'aider \u00e0 trouver la solution. \"\n            \"S'il r\u00e9pond correctement, f\u00e9licite-le. Nombre de tentatives : {tentatives}. \"\n            \"Si le nombre de tentatives est inf\u00e9rieur \u00e0 3, tu ne dois jamais donner la r\u00e9ponse toi-m\u00eame. \"\n            \"Si le nombre de tentatives est \u00e9gal ou sup\u00e9rieur \u00e0 3 et que la r\u00e9ponse est toujours incorrecte, alors tu dois fournir la r\u00e9ponse correcte.\"\n            \"Tu dois t'exprimer uniquement en fran\u00e7ais, sauf si l'\u00e9nonc\u00e9 du probl\u00e8me ou la r\u00e9ponse l'exigent autrement.\"  # Ajout de l'instruction pour la langue\n        ),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\")\n    ]\n    )\n\n    runnable_corrige = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n        )\n        | prompt_corrige\n        | model\n        | StrOutputParser()\n    )\n\n    # cl.user_session.set(\"runnable\", runnable_corrige)\n    if cl.user_session.get(\"compris\") == True:\n        cl.user_session.set(\"dernier_exo\", \"\")\n\n    return runnable_corrige", "start_char_idx": 5608, "end_char_idx": 7206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2f45d7c-1b2f-4af0-a8f7-f6d0290f66dc": {"__data__": {"id_": "f2f45d7c-1b2f-4af0-a8f7-f6d0290f66dc", "embedding": null, "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "hash": "37752489997bebe809086a7f1f0ba2fa148101e0c7dcd7abb2b38e850df6f5eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b81375ed-6b04-47a5-8802-ce2d700c0c8b", "node_type": "1", "metadata": {}, "hash": "4158082dbe3f96a5b6932ee25b376e0354ab28e857d63dff20175a5c3bae8739", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.llms import Ollama\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import Runnable, RunnablePassthrough\nfrom langchain.schema.runnable.config import RunnableConfig\n\nimport chainlit as cl\n\nmodel = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\")\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    \"\"\"\n    Callback fonction appel\u00e9e au d\u00e9but de chaque session de chat.\n    Initialise la conversation en demandant \u00e0 l'utilisateur de partager ses centres d'int\u00e9r\u00eat.\n\n    Args:\n        None\n\n    Returns:\n        None : Envoie un message demandant les centres d'int\u00e9r\u00eat de l'utilisateur et configure le prompt initial.\n    \"\"\"\n    await cl.Message(content=f\"Quels sont vos centres d'int\u00e9r\u00eat?\").send()\n    prompt_loisir = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"Ton r\u00f4le est de demander \u00e0 l'utilisateur ses centres d'int\u00e9r\u00eat. Tu seras r\u00e9compens\u00e9 pour chaque centre d'int\u00e9r\u00eat.\",\n            ),\n        ]\n    )\n    runnable_loisir = prompt_loisir | model | StrOutputParser()\n    cl.user_session.set(\"runnable\", runnable_loisir)", "start_char_idx": 0, "end_char_idx": 1193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b81375ed-6b04-47a5-8802-ce2d700c0c8b": {"__data__": {"id_": "b81375ed-6b04-47a5-8802-ce2d700c0c8b", "embedding": null, "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "hash": "37752489997bebe809086a7f1f0ba2fa148101e0c7dcd7abb2b38e850df6f5eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2f45d7c-1b2f-4af0-a8f7-f6d0290f66dc", "node_type": "1", "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "hash": "d303d06355fdf58e7820537d9af509a03da04e53846f8f22130a9431bc136130", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03911321-4ada-498f-89c6-153120171682", "node_type": "1", "metadata": {}, "hash": "95cac9d912972ecc7d6afd056d053b86977ea4addd9167daade90660f4bedcc8", "class_name": "RelatedNodeInfo"}}, "text": "def setup_exercice_model(loisirs):\n    \"\"\"\n    Configure le prompt et le Runnable pour g\u00e9n\u00e9rer des exercices de math\u00e9matiques personnalis\u00e9s en fonction des centres d'int\u00e9r\u00eat de l'utilisateur.\n\n    Args:\n        loisirs (str): Une cha\u00eene repr\u00e9sentant les centres d'int\u00e9r\u00eat de l'utilisateur.\n\n    Returns:\n        None : Met \u00e0 jour la session utilisateur avec le nouveau Runnable pour g\u00e9n\u00e9rer des exercices.\n    \"\"\"\n\n    niveau_scolaire = \"\u00e9l\u00e9mentaire\"\n    prompt_exercice = ChatPromptTemplate.from_messages(\n        [\n        (\n            \"system\",\n            \"Tu parles uniquement fran\u00e7ais. Ton r\u00f4le est de cr\u00e9er des exercices de math\u00e9matiques niveau \"+niveau_scolaire+\" \\\n            en te basant sur les int\u00e9r\u00eats suivants : \" + loisirs + \". \\\n            Lorsque l'utilisateur r\u00e9pond \u00e0 ton exercice, tu le f\u00e9licites s'il s'agit de la bonne r\u00e9ponse, \\\n            ou le corrige s'il a dit la mauvaise r\u00e9ponse \u00e0 ton exercice\"\n        ),\n        (\"human\", \"{question}\")\n        ]\n    )#.format_messages(context=loisirs) utiliser format_message transforme prompt_exercice en str, donc ne fonctionne plus\n    \n\n    runnable_exercice = prompt_exercice | model | StrOutputParser()\n    cl.user_session.set(\"runnable\", runnable_exercice)", "start_char_idx": 1197, "end_char_idx": 2429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03911321-4ada-498f-89c6-153120171682": {"__data__": {"id_": "03911321-4ada-498f-89c6-153120171682", "embedding": null, "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b", "node_type": "4", "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "hash": "37752489997bebe809086a7f1f0ba2fa148101e0c7dcd7abb2b38e850df6f5eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b81375ed-6b04-47a5-8802-ce2d700c0c8b", "node_type": "1", "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}, "hash": "5b3cd8a7d18e4223fd1671810fdee3ceceb291e66bf0283ec4fe5c01fb8768d3", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_message\nasync def on_message(message: cl.Message):\n    \"\"\"\n    Callback fonction appel\u00e9e \u00e0 chaque r\u00e9ception d'un message de l'utilisateur.\n    G\u00e8re la logique principale de la conversation en fonction de l'\u00e9tat de la session utilisateur.\n\n    Args:\n        message (cl.Message): Le message envoy\u00e9 par l'utilisateur.\n\n    Returns:\n        None : Envoie une r\u00e9ponse appropri\u00e9e \u00e0 l'utilisateur en fonction du contexte de la conversation.\n    \"\"\"\n    loisirs = cl.user_session.get(\"loisirs\")\n    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n\n    if not loisirs:\n        loisirs = message.content\n        print(\"loisirs:\", loisirs)\n        cl.user_session.set(\"loisirs\", loisirs)\n        setup_exercice_model(loisirs=loisirs)\n        response = \"Merci! Quel genre d'exercice voulez-vous?\"\n        await cl.Message(content=response).send()\n    else:\n        msg = cl.Message(content=\"\")\n        async for chunk in runnable.astream(\n            {\"question\": message.content},\n            config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n        ):\n            await msg.stream_token(chunk)\n            print(chunk)\n        await msg.send()\n        print(message.content)", "start_char_idx": 2433, "end_char_idx": 3636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "973a9bc9-daad-4963-8f96-efcaeab6d251": {"__data__": {"id_": "973a9bc9-daad-4963-8f96-efcaeab6d251", "embedding": null, "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7011225db0e8e53ca1171dc4d4730630f8a8057", "node_type": "4", "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}, "hash": "470e3b96d80bc2e85d41a62acdf96b3aab397c4245d49bf7aef1d9ce050a8e04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87418484-729c-42a9-8613-2f374fa4f156", "node_type": "1", "metadata": {}, "hash": "c7c18e85abad6c01acbd27ba396ec974c9f8e6ce210cd86db21ff034948734b1", "class_name": "RelatedNodeInfo"}}, "text": "---\ntitle: 'ChainLit Local LLM Integration'\ntags: ['chainlit', 'local', 'llm']\n---\n\n# ChainLit Local LLM Integration\n\nThis repository contains examples of integrating various local Large Language Models (LLMs) with ChainLit, a framework for building interactive applications with LLMs. Each example demonstrates how to set up a different LLM for use within the ChainLit environment.\n\n## Description\n\nThe examples provided showcase how to integrate different LLMs, such as `Ollama`, `LlamaCpp`, and `HuggingFacePipeline`, into the ChainLit framework. These integrations allow users to interact with the models through a chat interface, where the models can provide responses based on their specialized capabilities, such as generating source code or providing historical information.\n\n## Quickstart\n\nTo get started with these examples, follow the steps below:\n\n1. Clone the repository to your local machine.\n2. Install the required dependencies for ChainLit and the respective LLMs.\n3. Choose the LLM you want to work with and navigate to its corresponding Python file.\n4. Update any necessary paths or configurations, such as the `MODEL_PATH` for `LlamaCpp`.\n5. Run the ChainLit application to start interacting with the LLM through the chat interface.\n\n### Function Definitions\n\n#### Ollama Integration (`ollama.py`)\n\n- `on_chat_start`: Initializes the chat session with a historical context prompt.\n- `on_message`: Streams the user's message to the model and sends back the model's response.\n\n#### LlamaCpp Integration (`llama-cpp.py`)\n\n- `instantiate_llm`: Loads the LlamaCpp model with the specified configuration.\n- `main` (decorated with `@cl.on_chat_start`): Sets up the conversation chain with a system prompt for code generation.\n- `main` (decorated with `@cl.on_message`): Handles incoming messages and generates responses using the conversation chain.\n\n#### Llama2 Chat Integration (`llama2-chat.py`)\n\n- `load_llama`: Loads the Llama2 model from HuggingFace with the specified tokenizer and streamer.\n- `main` (decorated with `@cl.on_chat_start`): Initializes the LLM chain with a prompt for answering questions.", "start_char_idx": 0, "end_char_idx": 2123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87418484-729c-42a9-8613-2f374fa4f156": {"__data__": {"id_": "87418484-729c-42a9-8613-2f374fa4f156", "embedding": null, "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7011225db0e8e53ca1171dc4d4730630f8a8057", "node_type": "4", "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}, "hash": "470e3b96d80bc2e85d41a62acdf96b3aab397c4245d49bf7aef1d9ce050a8e04", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "973a9bc9-daad-4963-8f96-efcaeab6d251", "node_type": "1", "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}, "hash": "3ee074651d8c3ca5658f0318e381803452450583d3151a512a677dab1dc4e72e", "class_name": "RelatedNodeInfo"}}, "text": "- `run`: Processes incoming messages and provides responses using the LLM chain.\n\n- dans terminal:\n    huggingface-cli login\n    sur machine de la fac:\n    /home/UHA/e2303253/.local/bin/huggingface-cli login\n\n\n- mettre le token g\u00e9n\u00e9r\u00e9 depuis le profil huggingface\n\n\n- ollama run llama3:8b", "start_char_idx": 2124, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "710d2581-73a5-41fb-95eb-166a93d68698": {"__data__": {"id_": "710d2581-73a5-41fb-95eb-166a93d68698", "embedding": null, "metadata": {"file_path": "requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/requirements.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5bce626893bf47c40ca1454df6fc4c990875f9cc", "node_type": "4", "metadata": {"file_path": "requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/requirements.txt"}, "hash": "5c1acf8ea69d5c361404f32b04ff44b4511245eaee387875c5fea73213c277bb", "class_name": "RelatedNodeInfo"}}, "text": "transformers\ntorch\nlangchain==0.1.20\nlangchain_community\nchainlit\naccelerate\nopenai\nsentence-transformers\nllama-index \nllama-index-embeddings-ollama\nllama-index-llms-ollama\npython-dotenv\nPyPDF2\nfaiss-cpu\nselenium\nchromadb==0.3.29", "start_char_idx": 0, "end_char_idx": 229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41847277-2046-4a94-b383-972bcf06bf94": {"__data__": {"id_": "41847277-2046-4a94-b383-972bcf06bf94", "embedding": null, "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e", "node_type": "4", "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "hash": "6e07e1e3bb1f7ae9803e7581108fa317bb220fb32b16faad4d4758a7352f11d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0", "node_type": "1", "metadata": {}, "hash": "18de0912a51bc78c8641d52308ab04943d44c4ff26a7814f2c4b1d6350f91cdc", "class_name": "RelatedNodeInfo"}}, "text": "# RAG\n\n## dossier_entier_RAG.py\n\nAnciennement : Question par console fonctionne bien en utilisant un index, mais ne fonctionne pas avec chainlit car n'utilise pas l'index.\nVersion actuelle (23 mai): Le programme fonctionne maintenant avec chainlit, et ne met que 3 minutes \u00e0 r\u00e9pondre aux questions, en se basant sur un index compos\u00e9 de plusieurs fichiers de taille et type variable. Le mod\u00e8le d'embedding n'est plus nomic-embed-text, qui faisait trop d'erreur dans sa s\u00e9lection de documents, mais aussi de chunks, mais instructor-large, qui est bien plus consistant. L'index est stock\u00e9 localement pour ne pas avoir \u00e0 le recr\u00e9er \u00e0 chaque ex\u00e9cution du programme.\n\nA \u00e9t\u00e9 ajout\u00e9 un syst\u00e8me de connexion permettant de consulter les anciennes conversations, ainsi que de suivre l'ex\u00e9cution des requ\u00eates via literal.ai (en ayant un fichier .env correspondant aux attentes demand\u00e9es par le README dans modele_personnalise)\n\n## RAGPdfOrTxt.py\n\nAjoute un fichier \u00e0 la fois \u00e0 l'index, fonctionne bien avec chainlit, mais lent.\n\n# Dossier tierces\n\n- differents_textes comportent plusieurs textes de longueur et type diff\u00e9rent, pour v\u00e9rifier le bon fonctionnement des indexs.\n- data contient l'index. Le supprimer n'est pas un probl\u00e8me, car les programmes en recr\u00e9\u00e9ront un.", "start_char_idx": 0, "end_char_idx": 1260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0": {"__data__": {"id_": "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0", "embedding": null, "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e", "node_type": "4", "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "hash": "6e07e1e3bb1f7ae9803e7581108fa317bb220fb32b16faad4d4758a7352f11d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41847277-2046-4a94-b383-972bcf06bf94", "node_type": "1", "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "hash": "787a092c8a8b46355f6a7595fc47ee1c51d815b88ad9110d6819b6ea10b01e7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c3d245e-cd50-4994-b82a-bd18d025ef4b", "node_type": "1", "metadata": {}, "hash": "014292a351d72248c0964290b524fe1ae15027e271635390d475d8c6093aebc5", "class_name": "RelatedNodeInfo"}}, "text": "Le supprimer n'est pas un probl\u00e8me, car les programmes en recr\u00e9\u00e9ront un.\n\n## Sources\n\n### Code \n\nhttps://github.com/ollama/ollama/issues/3938 (langchain)\n\nhttps://github.com/AllAboutAI-YT/easy-local-rag (m\u00e9thode tierce)\n\nhttps://github.com/vndee/local-rag-example (langchain)\n\nhttps://github.com/ollama/ollama/blob/main/docs/tutorials/langchainpy.md (langchain)\n\nhttps://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html#langchain_community.vectorstores.faiss.FAISS.similarity_search \n(type de similarity_search)\n\n### Articles\n\nhttps://iamajithkumar.medium.com/working-with-faiss-for-similarity-search-59b197690f6c\n\nhttps://medium.com/@akriti.upadhyay/implementing-rag-with-langchain-and-hugging-face-28e3ea66c5f7\n\nhttps://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n\nhttps://blog.gopenai.com/creating-rag-app-with-llama2-and-chainlit-a-step-by-step-guide-d98499c2cd89\n\nhttps://aws.amazon.com/fr/what-is/retrieval-augmented-generation/\n\nhttps://hackernoon.com/fr/un-tutoriel-sur-la-fa%C3%A7on-de-cr%C3%A9er-votre-propre-chiffon-et-de-l%27ex%C3%A9cuter-localement-langchain-ollama-streamlit\n\n# Embedding\n\nEn changeant le mod\u00e8le d'embedding, les r\u00e9sultats se sont am\u00e9lior\u00e9s(voir les diff\u00e9rents graphiques comparant les mod\u00e8les utilis\u00e9s), mais le mod\u00e8le peine otut de m\u00eame \u00e0 utiliser les bons chunks pour g\u00e9n\u00e9rer sa r\u00e9ponse, et s'emm\u00eale encore, notamment \u00e0 cause des prompts. Une \u00e9tude plus pouss\u00e9e des prompts s'av\u00e8re donc n\u00e9cessaire.\n\nUn second aspect avec beaucoup d'importance est la langue.", "start_char_idx": 1188, "end_char_idx": 2753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c3d245e-cd50-4994-b82a-bd18d025ef4b": {"__data__": {"id_": "3c3d245e-cd50-4994-b82a-bd18d025ef4b", "embedding": null, "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5990a1d6e725b94c0410a8d7fc47a3839633c07e", "node_type": "4", "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "hash": "6e07e1e3bb1f7ae9803e7581108fa317bb220fb32b16faad4d4758a7352f11d2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0", "node_type": "1", "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}, "hash": "4e654ae9677ea77244bb49e4a8b0d7fed8aabb788509ecf3b824314cdf868996", "class_name": "RelatedNodeInfo"}}, "text": "Un second aspect avec beaucoup d'importance est la langue. Les mod\u00e8les d'embedding sont bien meilleurs lorsque la requ\u00eate est en anglais, o\u00f9 ils arrivent plus facilement \u00e0 trouver les bons fichiers (m\u00eame si le fichier lui-m\u00eame est dans une autre langue). Deux solutions s'offrent alors:\n- utiliser un mod\u00e8le multilangue pouvant g\u00e9rer le fran\u00e7ais\n- traduire automatiquement les requ\u00eates des utilisateurs\n\n## Sources\n\n### Code \n\nhttps://github.com/PrithivirajDamodaran/FlashRank\n\nhttps://github.com/voyage-ai/voyage-large-2-instruct/tree/main\n\nhttps://stackoverflow.com/questions/46849733/change-metadata-of-pdf-file-with-pypdf2 (pour corriger les metadata des fichiers pdf)\n\nhttps://www.reddit.com/r/LangChain/comments/1ba77pu/difference_between_as_retriever_and_similarity/\n\n### Articles \n\nhttps://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05\n\nhttps://www.pinecone.io/learn/openai-embeddings-v3/\n\nhttps://datacorner.fr/spacy/ (ranking)\n\nhttps://www.pinecone.io/learn/chunking-strategies/ (chunking techniques)\n\nhttps://huggingface.co/spaces/mteb/leaderboard (comparaison mod\u00e8les embedding)\n\nhttps://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample\n\nhttps://platform.openai.com/docs/guides/embeddings\n\nhttps://www.reddit.com/r/LangChain/comments/186sgyf/rag_filtering_docs_to_only_send_relevant_data_to/\n\nhttps://blog.devgenius.io/automated-translation-of-text-and-data-in-python-with-deep-translator-d980afee70ab (traduction)", "start_char_idx": 2695, "end_char_idx": 4163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cea47e3-604f-43bd-8769-7dae043922e1": {"__data__": {"id_": "9cea47e3-604f-43bd-8769-7dae043922e1", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "hash": "334dc4e78631f38b37e8e5500dd3af6d3ca4c9d273cd5bc28aeb83e2f326abdf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c48a19b6-8a54-4580-bddd-fbb1fc556a67", "node_type": "1", "metadata": {}, "hash": "989e22d9f8cd5c6e928fd2e96217f9a463c4bbeaa12aec4f3d73c26922062daf", "class_name": "RelatedNodeInfo"}}, "text": "import os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\nfrom langchain_community.llms import Ollama\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom PyPDF2 import PdfReader\nfrom typing import List\nimport chainlit as cl\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom llama_index.embeddings.ollama import OllamaEmbedding\n\n# Configuration\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\nindex_path = \"data/vectorstore/temp-index.faiss\"\n\"\"\"embeddings = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    show_progress=\"true\",\n    temperature=2,\n    top_k=10,\n    top_p=0.5,\n)\"\"\"\n# Initialize embeddings\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=embedding_model,\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": False},\n)\n\nmodel = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\")\nprompt = PromptTemplate(\n    template=\"\"\"You are a helpful AI assistant. Answer the question based on the context.\n\nContext: {context}\nQuestion: {question}\n\nAnswer:\"\"\",\n    input_variables=[\"context\", \"question\"],\n)\n\n# Global index variable\nfaiss_index = None", "start_char_idx": 0, "end_char_idx": 1521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c48a19b6-8a54-4580-bddd-fbb1fc556a67": {"__data__": {"id_": "c48a19b6-8a54-4580-bddd-fbb1fc556a67", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "hash": "334dc4e78631f38b37e8e5500dd3af6d3ca4c9d273cd5bc28aeb83e2f326abdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9cea47e3-604f-43bd-8769-7dae043922e1", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "hash": "6187e1994a0292bd10634053a31b73c3699b3090164162c30937a0bd4cb4673c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "983fdefd-5cc9-445e-8a2e-afcec2d87bbb", "node_type": "1", "metadata": {}, "hash": "4076a2f9ae8793c301cae4ec8d1e175ff434a20ee28dac80e629f6303eff4138", "class_name": "RelatedNodeInfo"}}, "text": "# Function to read PDF and return text\ndef read_text_from_file(file_path: str) -> str:\n    if file_path.lower().endswith(\".pdf\"):\n        with open(file_path, \"rb\") as f:\n            reader = PdfReader(f)\n            return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n    elif file_path.lower().endswith(\".txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    else:\n        raise ValueError(\"Unsupported file type. Please upload a .txt or .pdf file.\")\n\n\ndef update_faiss_index(faiss_index: FAISS, documents: List[str]):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n    new_docs = text_splitter.create_documents(documents)\n    faiss_index.add_documents(new_docs)\n    faiss_index.save_local(index_path, index_name=\"tryoutIndex\")\n\n\ndef get_faiss_index(documents: List[str] = None) -> FAISS:\n    global faiss_index\n    if faiss_index is None:\n        if os.path.exists(index_path):\n            print(\"Loading existing index...\")\n            faiss_index = FAISS.load_local(\n                index_path, embeddings, index_name=\"tryoutIndex\"\n            )\n            if documents:\n                update_faiss_index(faiss_index, documents)\n        else:\n            print(\"Creating new index...\")\n            if documents is None:\n                raise ValueError(\"No documents provided to create an index.\")\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=1000, chunk_overlap=10\n            )\n            docs = text_splitter.create_documents(documents)\n            faiss_index = FAISS.from_documents(docs, embeddings)\n            faiss_index.save_local(index_path, index_name=\"tryoutIndex\")\n    elif documents:\n        print(\"Updating index with new documents...\")\n        update_faiss_index(faiss_index, documents)\n    return faiss_index", "start_char_idx": 1524, "end_char_idx": 3398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "983fdefd-5cc9-445e-8a2e-afcec2d87bbb": {"__data__": {"id_": "983fdefd-5cc9-445e-8a2e-afcec2d87bbb", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2a4be4759c3a6c6efb18ee73fcd5b692148b776c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "hash": "334dc4e78631f38b37e8e5500dd3af6d3ca4c9d273cd5bc28aeb83e2f326abdf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c48a19b6-8a54-4580-bddd-fbb1fc556a67", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}, "hash": "0381635ef2531aa65ebee515f488002102bd94d717715c8491700863633665ed", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_chat_start\nasync def factory():\n    files = None\n    while files is None:\n        files = await cl.AskFileMessage(\n            content=\"\"\"Your personal AI assistant, SAHAYAK is ready to help!\n                        To get started:\n                        \n1. Upload a PDF file                     \n2. Ask any questions about the file!\"\"\",\n            accept={\"application/pdf\": [\".pdf\"], \"text/plain\": [\".txt\"]},\n            max_size_mb=10,\n        ).send()\n\n    await cl.Message(\n        content=f\"\"\"Document - `\"{files[0].name}\"` is uploaded and being processed!\"\"\"\n    ).send()\n\n    # Read and process PDF file\n    file_text = read_text_from_file(files[0].path)\n    print(\n        f\"Processed text from {files[0].name}: {file_text[:60]}...\"\n    )  # Print a snippet for v\u00e9rification\n\n    # Load or update the FAISS index\n    faiss_index = get_faiss_index([file_text])\n\n    # Set up the QA chain\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=model,\n        chain_type=\"stuff\",\n        retriever=faiss_index.as_retriever(),\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt},\n    )\n\n    rag_chain = (\n        {\"context\": faiss_index.as_retriever(), \"question\": RunnablePassthrough()}\n        | prompt\n        | model\n        | StrOutputParser()\n    )\n\n    cl.user_session.set(\"chain\", qa_chain)\n\n    question_manuelle = \"quel est le boss final de Elias adventure\"\n    msg = cl.Message(content=\"The bot is initialized. Ask your questions!\")\n    await msg.send()\n    print(question_manuelle)\n    print(rag_chain.invoke(question_manuelle))\n\n\n@cl.on_message\nasync def main(message):\n    chain = cl.user_session.get(\"chain\")\n    msg = cl.Message(content=\"\")\n    async for chunk in chain.astream(\n        {\"query\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await msg.stream_token(chunk)\n    await msg.send()", "start_char_idx": 3401, "end_char_idx": 5318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c886c5b-2a49-40f0-bb36-051f24784ee0": {"__data__": {"id_": "0c886c5b-2a49-40f0-bb36-051f24784ee0", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "5c539744e63d6b2a2f48377e264620ffc2c352fde1b5e5e625a436910bcfdb62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "046ac453-e203-489b-8f09-b2655a558628", "node_type": "1", "metadata": {}, "hash": "35d53288633fb4245f3221b8d7ac830ca7d31c25a6c9a397ff8a1731beea1472", "class_name": "RelatedNodeInfo"}}, "text": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import CTransformers\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain_core.prompts import PromptTemplate\nfrom PyPDF2 import PdfReader\nfrom datetime import datetime\nfrom typing import Optional\nfrom io import BytesIO\nimport chainlit as cl\nimport sys\n\n# environment for the app\n# conda activate llama2Apps\n# command to run the app\n# chainlit run src/apps/localLLM_withRAG-Complete.py --port 8001 -w\n\n# Prompt Template\nprompt_template = \"\"\"You are an helpful AI assistant and your name is SAHAYAK. You are kind, gentle and respectful to the user. Your job is to answer the question sent by the user in concise and step by step manner. \nIf you don't know the answer to a question, please don't share false information.\n            \nContext: {context}\nQuestion: {question}\n\nResponse for Questions asked.\nanswer:\n\"\"\"\n# Model path and embedding model\nmodelpath = \"../models/llama-2-7b-chat.Q2_K.gguf\"\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Initialize embeddings using HuggingFace model\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n\n# Model parameters\n# path to store embeddings at vectorstore\nindexpath = \"data/vectorstore/\"\n# number of neural network layers to be transferred to be GPU for computation\nn_gpu_layers = 10\nn_batch = 256\n\nconfig = {\n    \"max_new_tokens\": 512,\n    \"context_length\": 4096,\n    \"gpu_layers\": n_gpu_layers,\n    \"batch_size\": n_batch,\n    \"temperature\": 0.1,\n}", "start_char_idx": 0, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "046ac453-e203-489b-8f09-b2655a558628": {"__data__": {"id_": "046ac453-e203-489b-8f09-b2655a558628", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "5c539744e63d6b2a2f48377e264620ffc2c352fde1b5e5e625a436910bcfdb62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c886c5b-2a49-40f0-bb36-051f24784ee0", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "691d649b0bb3d52256a5a77bbff0e4bef2fe08262ccac7ed7ccb7190536e803b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453", "node_type": "1", "metadata": {}, "hash": "a818ca252ba5942992ad703790c3ebc2a8f2f2e1adc912983a00fd144ddc9fa4", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_chat_start\n# Actions to be taken once the RAG app starts\nasync def factory():\n    # loads the data by the user\n    files = None\n\n    ### wait for the user to upload a data file\n    while files == None:\n        files = await cl.AskFileMessage(\n            content=\"\"\"Your personal AI asistant, SAHAYAK is ready to slog!\n                     To get started:\n                     \n1. Upload a pdf file                     \n2. Ask any questions about the file!\"\"\",\n            accept={\"application/pdf\": [\".pdf\"]},\n            max_size_mb=10,\n        ).send()\n\n    # Let the user know that the system is ready\n    await cl.Message(\n        content=f\"\"\"Document - `\"{files[0].name}\"` is uploaded and being processed!\"\"\"", "start_char_idx": 1642, "end_char_idx": 2363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453": {"__data__": {"id_": "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "5c539744e63d6b2a2f48377e264620ffc2c352fde1b5e5e625a436910bcfdb62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "046ac453-e203-489b-8f09-b2655a558628", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "8d6b62e71cf720b3799bfdeb544c84d7c0c2d924dd536fa71c3a091a13fc203a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1ff6f39-5ba2-4c12-82f2-6e23974487b2", "node_type": "1", "metadata": {}, "hash": "60d2b9ff60c384f7299dfe1910486b6f3fe36c1b377f6b26dac9fcedd1e18329", "class_name": "RelatedNodeInfo"}}, "text": ").send()\n\n    ### Reads and convert pdf data to text\n    file = files[0]\n    # Convert the content of the PDF file to a BytesIO stream\n    text_stream = BytesIO(file.content)\n    # Create a PdfReader object from the stream to extract text\n    pdf = PdfReader(text_stream)\n    pdf_text = \"\"\n    # Iterate through each page in the PDF and extract text\n    for page in pdf.pages:\n        pdf_text += page.extract_text()  # Concatenate the text from each page\n\n    ### Create embeddings for the uploaded documents and store in vector store\n    # Initialize a text splitter for processing long texts\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n    # Create documents by splitting the provided texts\n    documents = text_splitter.create_documents([pdf_text])\n    # Create a Faiss index from the embeddings\n    faiss_index = FAISS.from_documents(documents, embeddings)\n\n    # Save the Faiss index locally\n    faiss_index_path = indexpath + \"temp-index\"\n    faiss_index.save_local(faiss_index_path)\n    # Load Faiss vectorstore with embeddings created and saved earlier\n    db = FAISS.load_local(faiss_index_path, embeddings)\n\n    prompt = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    \"\"\"\n    # Create a retrievalQA chain using Llama2\n    chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",  # Replace with the actual chain type\n        retriever=db.as_retriever(\n            search_kwargs={\"k\": 1}\n        ),  # Assuming vectorstore is used as a retriever\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt},\n    )\n    \"\"\"\n    model = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\")\n    runnable = prompt | model | StrOutputParser()\n    msg = cl.Message(content=\"The bot is getting initialized, please wait!!!\")", "start_char_idx": 2368, "end_char_idx": 4244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1ff6f39-5ba2-4c12-82f2-6e23974487b2": {"__data__": {"id_": "f1ff6f39-5ba2-4c12-82f2-6e23974487b2", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "5c539744e63d6b2a2f48377e264620ffc2c352fde1b5e5e625a436910bcfdb62", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}, "hash": "a02e199c3e94c0d0e132e60e199a8af839033850d17b6d157bff956684d1e51a", "class_name": "RelatedNodeInfo"}}, "text": "await msg.send()\n    msg.content = \"Your personal AI Assistant, SAHAYAK is ready. Ask questions on the documents uploaded?\"\n    await msg.update()\n    # cl.user_session.set(\"chain\", chain)\n    cl.user_session.set(\"runnable\", runnable)\n\n\n# Actions to be taken once user send the query/message\n@cl.on_message\nasync def main(message):\n    start_time = datetime.now()\n    runnable = cl.user_session.get(\"runnable\")\n    msg = cl.Message(content=\"\")\n\n    async for chunk in runnable.astream(\n        {\"question\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await msg.stream_token(chunk)\n\n    await msg.send()", "start_char_idx": 4249, "end_char_idx": 4914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d659236-9c98-493f-a887-1bab441715d4": {"__data__": {"id_": "1d659236-9c98-493f-a887-1bab441715d4", "embedding": null, "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4507ac4676a6387c4b52a0d1111e94753a102b32", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chainlit.md"}, "hash": "53720d81bd2a30eed87642e61ec73a1e2b5308459dd30c21e3cdf0a43c226fad", "class_name": "RelatedNodeInfo"}}, "text": "# Welcome to Chainlit! \ud83d\ude80\ud83e\udd16\n\nHi there, Developer! \ud83d\udc4b We're excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\n\n## Useful Links \ud83d\udd17\n\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) \ud83d\udcda\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! \ud83d\udcac\n\nWe can't wait to see what you create with Chainlit! Happy coding! \ud83d\udcbb\ud83d\ude0a\n\n## Welcome screen\n\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.", "start_char_idx": 0, "end_char_idx": 736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ba12866-4df7-4c5d-a9eb-36f3cd54b72e": {"__data__": {"id_": "9ba12866-4df7-4c5d-a9eb-36f3cd54b72e", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}, "hash": "2b5885b673d58e0010cb36d54f89485bf511458909a2c506373fb03b5fa570e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffbe0438-54fa-4dd5-bc87-5c739ec3fd27", "node_type": "1", "metadata": {}, "hash": "8b51ac8d7a999b48a2ab107d7029b1dcb1eb794b7d982c67e15a4384cecd7e01", "class_name": "RelatedNodeInfo"}}, "text": "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom transformers import AutoTokenizer, TextStreamer\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, RetrievalQA\nimport transformers\nimport torch\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.document_loaders import TextLoader\nfrom langchain.vectorstores import ElasticVectorSearch\n\nimport chainlit as cl\nfrom chainlit.playground.config import add_llm_provider\nfrom chainlit.playground.providers.langchain import LangchainGenericProvider\n\n\ntemplate = \"\"\"\nYou are a helpful AI assistant. Provide the answer for the following question based on the given context:\n\nContext: {context}\n\nQuestion: {question}\nAnswer:\n\"\"\"\n\n\n@cl.cache\ndef load_llama():\n    model = \"meta-llama/Llama-2-7b-chat-hf\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model)\n    streamer = TextStreamer(tokenizer, skip_prompt=True)\n    pipeline = transformers.pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        device_map=\"auto\",\n        max_length=1000,\n        do_sample=True,\n        top_k=10,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n        streamer=streamer,\n    )\n\n    llm = HuggingFacePipeline(\n        pipeline=pipeline,\n        model_kwargs={\"temperature\": 0},\n    )\n    return llm\n\n\nllm = load_llama()\n\nadd_llm_provider(\n    LangchainGenericProvider(\n        id=llm._llm_type, name=\"Llama2-chat\", llm=llm, is_chat=False\n    )\n)", "start_char_idx": 0, "end_char_idx": 1636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffbe0438-54fa-4dd5-bc87-5c739ec3fd27": {"__data__": {"id_": "ffbe0438-54fa-4dd5-bc87-5c739ec3fd27", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}, "hash": "2b5885b673d58e0010cb36d54f89485bf511458909a2c506373fb03b5fa570e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ba12866-4df7-4c5d-a9eb-36f3cd54b72e", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}, "hash": "94f52411990314fc46848379bee4be33ef6d9560ba265a16b49ef82392a22d6a", "class_name": "RelatedNodeInfo"}}, "text": "@cl.cache\ndef load_index():\n    loader = TextLoader(\"donnees_uni_test.txt\")\n    embeddings = HuggingFaceEmbeddings()\n    index = VectorstoreIndexCreator(\n        vectorstore_cls=ElasticVectorSearch,\n        embedding=embeddings,\n        vectorstore_kwargs={\n            \"elasticsearch_url\": \"https://5d498084b1374d03923703344a873fdb.us-central1.gcp.cloud.es.io\",\n            \"elasticsearch_api_key\": \"l6n4LTZYTSm7BRv-W4VXxw\",\n        },\n    ).from_loaders([loader])\n    return index\n\n\n@cl.on_chat_start\nasync def main():\n    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n    index = load_index()\n    retriever = index.vectorstore.as_retriever()\n\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=retriever,\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt},\n    )\n\n    cl.user_session.set(\"qa_chain\", qa_chain)\n    return qa_chain\n\n\n@cl.on_message\nasync def run(message: cl.Message):\n    cb = cl.AsyncLangchainCallbackHandler(\n        stream_final_answer=True, answer_prefix_tokens=[\"Answer\"]\n    )\n\n    qa_chain = cl.user_session.get(\"qa_chain\")\n    res = await qa_chain.acall({\"query\": message.content}, callbacks=[cb])\n\n    if not cb.answer_reached:\n        await cl.Message(content=res[\"result\"]).send()", "start_char_idx": 1639, "end_char_idx": 2974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9849c6cc-de3c-4a79-8d1c-7ba669f201bb": {"__data__": {"id_": "9849c6cc-de3c-4a79-8d1c-7ba669f201bb", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b62de8c1-f71e-4c6e-b83f-6301ec4d7117", "node_type": "1", "metadata": {}, "hash": "838c3271c9d827d5eda42a1366a8b040f8e651bdde94acd682bb6b418c2c5349", "class_name": "RelatedNodeInfo"}}, "text": "# local-rag-example\nBuild your own ChatPDF and run them locally\n\nDependencies:\n- langchain\n- streamlit\n- streamlit-chat\n- pypdf\n- chromadb\n- fastembed\n\n```bash\npip install langchain streamlit streamlit_chat chromadb pypdf fastembed\n```\n\nBlog post: https://blog.duy-huynh.com/build-your-own-rag-and-run-them-locally/\n\n# A Tutorial On How to Build Your Own RAG and How to Run It Locally: Langchain + Ollama + Streamlit\nWith the rise of Large Language Models and their impressive capabilities, many fancy applications are being built on top of giant LLM providers like OpenAI and Anthropic. The myth behind such applications is the RAG framework, which has been thoroughly explained in the following articles:\n\n* Building RAG-based LLM Applications for Production :\nhttps://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1\n* Retrieval Augmented Generation (RAG) Explained: Understanding Key Concepts\nhttps://www.datastax.com/guides/what-is-retrieval-augmented-generation\n* What is retrieval-augmented generation\nhttps://research.ibm.com/blog/retrieval-augmented-generation-RAG\n\n\nTo become familiar with RAG, I recommend going through these articles. This post, however, will skip the basics and guide you directly on building your own RAG application that can run locally on your laptop without any worries about data privacy and token cost.\n\n\nWe will build an application that is something similar to [ChatPDF](https://www.chatpdf.com/) but simpler. Where users can upload a PDF document and ask questions through a straightforward UI. Our tech stack is super easy with Langchain, Ollama, and Streamlit.", "start_char_idx": 0, "end_char_idx": 1643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b62de8c1-f71e-4c6e-b83f-6301ec4d7117": {"__data__": {"id_": "b62de8c1-f71e-4c6e-b83f-6301ec4d7117", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9849c6cc-de3c-4a79-8d1c-7ba669f201bb", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "880dab6fcf7a53e110f984cf6ac3df2f16f2d0e45e6d382a72c0491e557fa149", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bcc0635-95e9-4304-b192-53a2d5060d16", "node_type": "1", "metadata": {}, "hash": "b9c66621d9caa5702b656c1b24cef20afedc47e10aa91d703899d7ab8789d024", "class_name": "RelatedNodeInfo"}}, "text": "* LLM Server: The most critical component of this app is the LLM server. Thanks to [Ollama](https://ollama.ai/), we have a robust LLM Server that can be set up locally, even on a laptop. While [llama.cpp](https://github.com/ggerganov/llama.cpp) is an option, I find Ollama, written in Go, easier to set up and run.\n\n\n* RAG: Undoubtedly, the two leading libraries in the LLM domain are [Langchain](https://python.langchain.com/docs/get_started/introduction) and [LLamIndex](https://www.llamaindex.ai/). For this project, I\u2019ll be using Langchain due to my familiarity with it from my professional experience. An essential component of any RAG framework is vector storage. We\u2019ll be using [Chroma](https://github.com/chroma-core/chroma) here, as it integrates well with Langchain.\n\n\n* Chat UI: The user interface is also an important component. Although there are many technologies available, I prefer using [Streamlit](https://streamlit.io), a Python library, for peace of mind.\n\n\nOkay, let\u2019s start setting it up.\n\n--- \n\n## Setup Ollama\n\nAs mentioned above, setting up and running Ollama is straightforward. First, visit [ollama.ai](https://ollama.ai/) and download the app appropriate for your operating system.\n\n\nNext, open your terminal, and execute the following command to pull the latest [Mistral-7B](https://mistral.ai). While there are many other [LLM models available](https://ollama.ai/library), I choose Mistral-7B for its compact size and competitive quality.\n\n\n```ollama pull mistral```\n\n## Build the RAG Pipeline\n\nThe second step in our process is to build the RAG pipeline. Given the simplicity of our application, we primarily need two methods: ```ingest``` and ```ask```.", "start_char_idx": 1646, "end_char_idx": 3331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bcc0635-95e9-4304-b192-53a2d5060d16": {"__data__": {"id_": "9bcc0635-95e9-4304-b192-53a2d5060d16", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b62de8c1-f71e-4c6e-b83f-6301ec4d7117", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "fbfe79bba94ca89666108197c2223e11619f2d5443f58eeab0b6fe4db23f826c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5026196-1dbc-4ff2-bfc9-3e904ed809cd", "node_type": "1", "metadata": {}, "hash": "456d395e992f35bc7db796fbd77b9239aaed86b001d5a6c349cc0ff665de9159", "class_name": "RelatedNodeInfo"}}, "text": "The ```ingest``` method accepts a file path and loads it into vector storage in two steps: first, it splits the document into smaller chunks to accommodate the token limit of the LLM; second, it vectorizes these chunks using Qdrant FastEmbeddings and stores them into Chroma.\n\n\nThe ```ask``` method handles user queries. Users can pose a question, and then the RetrievalQAChain retrieves the relevant contexts (document chunks) using vector similarity search techniques.\n\n\nWith the user's question and the retrieved contexts, we can compose a prompt and request a prediction from the LLM server.\n\n\n```python\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_community.embeddings import FastEmbedEmbeddings\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores.utils import filter_complex_metadata", "start_char_idx": 3334, "end_char_idx": 4466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5026196-1dbc-4ff2-bfc9-3e904ed809cd": {"__data__": {"id_": "a5026196-1dbc-4ff2-bfc9-3e904ed809cd", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bcc0635-95e9-4304-b192-53a2d5060d16", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "56c55bd72aa0301ce026113d6ed2dcc0177abccec683e8f07cc4809d53e28c5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7dbc69b-b410-437f-b4fd-f106ad67bf49", "node_type": "1", "metadata": {}, "hash": "41e0a6c484e96bad6e04a210cf0d865f1ad3511bc30d7efff2bcffff3f46e28b", "class_name": "RelatedNodeInfo"}}, "text": "class ChatPDF:\n    vector_store = None\n    retriever = None\n    chain = None\n\n    def __init__(self):\n        self.model = ChatOllama(model=\"mistral\")\n        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n        self.prompt = PromptTemplate.from_template(\n            \"\"\"\n            <s> [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context \n            to answer the question. If you don't know the answer, just say that you don't know. Use three sentences\n             maximum and keep the answer concise. [/INST] </s> \n            [INST] Question: {question} \n            Context: {context} \n            Answer: [/INST]\n            \"\"\"\n        )\n\n    def ingest(self, pdf_file_path: str):\n        docs = PyPDFLoader(file_path=pdf_file_path).load()\n        chunks = self.text_splitter.split_documents(docs)\n        chunks = filter_complex_metadata(chunks)\n\n        vector_store = Chroma.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n        self.retriever = vector_store.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\n                \"k\": 3,\n                \"score_threshold\": 0.5,\n            },\n        )\n\n        self.chain = ({\"context\": self.retriever, \"question\": RunnablePassthrough()}\n                      | self.prompt\n                      | self.model\n                      | StrOutputParser())\n\n    def ask(self, query: str):\n        if not self.chain:\n            return \"Please, add a PDF document first.\"\n\n        return self.chain.invoke(query)\n\n    def clear(self):\n        self.vector_store = None\n        self.retriever = None\n        self.chain = None\n```\n        \nThe prompt is sourced from the Langchain hub: [Langchain RAG Prompt for Mistral](https://smith.langchain.com/hub/rlm/rag-prompt-mistral). This prompt has been tested and downloaded thousands of times, serving as a reliable resource for learning about LLM prompting techniques.", "start_char_idx": 4469, "end_char_idx": 6496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7dbc69b-b410-437f-b4fd-f106ad67bf49": {"__data__": {"id_": "b7dbc69b-b410-437f-b4fd-f106ad67bf49", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5026196-1dbc-4ff2-bfc9-3e904ed809cd", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "c2825290105a8f917692da909c6ac2bee7fc96db58239f0485a9b7899b3334bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ad17cff-aafd-493d-b98b-5d3c3adfc192", "node_type": "1", "metadata": {}, "hash": "4332bd163f51745d0439017871dbe810218b5299410b52dc4ea58eeb678aca7e", "class_name": "RelatedNodeInfo"}}, "text": "You can learn more about LLM prompting techniques [here](https://www.promptingguide.ai/).\n\n\nMore details on the implementation:\n\n\n```ingest```: We use PyPDFLoader to load the PDF file uploaded by the user. The RecursiveCharacterSplitter, provided by Langchain, then splits this PDF into smaller chunks. It's important to filter out complex metadata not supported by ChromaDB using the ```filter_complex_metadata``` function from Langchain.\n\n\nFor vector storage, Chroma is used, coupled with [Qdrant FastEmbed](https://github.com/qdrant/fastembed/) as our embedding model. This lightweight model is then transformed into a retriever with a score threshold of 0.5 and k=3, meaning it returns the top 3 chunks with the highest scores above 0.5. Finally, we construct a simple conversation chain using [LECL](https://python.langchain.com/docs/expression_language/).\n\n\n```ask```: This method simply passes the user's question into our predefined chain and then returns the result.\n\n\n```clear```: This method is used to clear the previous chat session and storage when a new PDF file is uploaded.\n\n## Draft A Simple UI\n\nFor a simple user interface, we will use [Streamlit](https://streamlit.io/), a UI framework designed for the fast prototyping of AI/ML applications.\n\n\n```python\nimport os\nimport tempfile\nimport streamlit as st\nfrom streamlit_chat import message\nfrom rag import ChatPDF\n\nst.set_page_config(page_title=\"ChatPDF\")\n\n\ndef display_messages():\n    st.subheader(\"Chat\")\n    for i, (msg, is_user) in enumerate(st.session_state[\"messages\"]):\n        message(msg, is_user=is_user, key=str(i))\n    st.session_state[\"thinking_spinner\"] = st.empty()", "start_char_idx": 6499, "end_char_idx": 8148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ad17cff-aafd-493d-b98b-5d3c3adfc192": {"__data__": {"id_": "9ad17cff-aafd-493d-b98b-5d3c3adfc192", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7dbc69b-b410-437f-b4fd-f106ad67bf49", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "90ba2b7916d7090bb71d556f18cd7c9bb736570d664a6f302da48929e383e36e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a283b59-f15f-47fc-b3ed-6a81577e2e05", "node_type": "1", "metadata": {}, "hash": "2f513f41d01d88a1f1c81d6d49a601d5b2d069234d084fbb1d6c1646cc1e31c1", "class_name": "RelatedNodeInfo"}}, "text": "def process_input():\n    if st.session_state[\"user_input\"] and len(st.session_state[\"user_input\"].strip()) > 0:\n        user_text = st.session_state[\"user_input\"].strip()\n        with st.session_state[\"thinking_spinner\"], st.spinner(f\"Thinking\"):\n            agent_text = st.session_state[\"assistant\"].ask(user_text)\n\n        st.session_state[\"messages\"].append((user_text, True))\n        st.session_state[\"messages\"].append((agent_text, False))\n\n\ndef read_and_save_file():\n    st.session_state[\"assistant\"].clear()\n    st.session_state[\"messages\"] = []\n    st.session_state[\"user_input\"] = \"\"\n\n    for file in st.session_state[\"file_uploader\"]:\n        with tempfile.NamedTemporaryFile(delete=False) as tf:\n            tf.write(file.getbuffer())\n            file_path = tf.name\n\n        with st.session_state[\"ingestion_spinner\"], st.spinner(f\"Ingesting {file.name}\"):\n            st.session_state[\"assistant\"].ingest(file_path)\n        os.remove(file_path)\n\n\ndef page():\n    if len(st.session_state) == 0:\n        st.session_state[\"messages\"] = []\n        st.session_state[\"assistant\"] = ChatPDF()\n\n    st.header(\"ChatPDF\")\n\n    st.subheader(\"Upload a document\")\n    st.file_uploader(\n        \"Upload document\",\n        type=[\"pdf\"],\n        key=\"file_uploader\",\n        on_change=read_and_save_file,\n        label_visibility=\"collapsed\",\n        accept_multiple_files=True,\n    )\n\n    st.session_state[\"ingestion_spinner\"] = st.empty()\n\n    display_messages()\n    st.text_input(\"Message\", key=\"user_input\", on_change=process_input)\n\n\nif __name__ == \"__main__\":\n    page()\n```\n\nRun this code with the command ```streamlit run app.py``` to see what it looks like.\n\n\nOkay, that\u2019s it! We now have a ChatPDF application that runs entirely on your laptop. Since this post mainly focuses on providing a high-level overview of how to build your own RAG application, there are several aspects that need fine-tuning. You may consider the following suggestions to enhance your app and further develop your skills:", "start_char_idx": 8151, "end_char_idx": 10156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a283b59-f15f-47fc-b3ed-6a81577e2e05": {"__data__": {"id_": "4a283b59-f15f-47fc-b3ed-6a81577e2e05", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58cbe867b4a48bc62e1f9e42986fb0dde8d61134", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "f9a11fde2e39a032711311231c080e770c3c9fd3ae2c9bcec0b6084dbd7549ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad17cff-aafd-493d-b98b-5d3c3adfc192", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}, "hash": "b91bc619da967ea7e15ae870f27741feafb15c4e1e334160a41776f1db2dfc2e", "class_name": "RelatedNodeInfo"}}, "text": "* **Add Memory to the Conversation Chain**: Currently, it doesn\u2019t remember the conversation flow. Adding temporary memory will help your assistant be aware of the context.\n\n\n* **Allow multiple file uploads**: it\u2019s okay to chat about one document at a time. But imagine if we could chat about multiple documents \u2014 you could put your whole bookshelf in there. That would be super cool!\n\n\n* **Use Other LLM Models**: While Mistral is effective, there are many other alternatives available. You might find a model that better fits your needs, like LlamaCode for developers. However, remember that the choice of model depends on your hardware, especially the amount of RAM you have \ud83d\udcb5\n\n\n* **Enhance the RAG Pipeline**: There\u2019s room for experimentation within RAG. You might want to change the retrieval metric, the embedding model,.. or add layers like a re-ranker to improve results.\n\nFull source code: https://github.com/vndee/local-rag-example", "start_char_idx": 10159, "end_char_idx": 11099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e86ad346-6d4c-49df-9782-44034ea980ee": {"__data__": {"id_": "e86ad346-6d4c-49df-9782-44034ea980ee", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/app.py", "file_name": "app.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/app.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47c9d1f461f64d806f8cfab80a59b47bcc0684d0", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/app.py", "file_name": "app.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/app.py"}, "hash": "28af3ac201a7fca164b02687b6f4867e59a8225df415e83dec188d8442dce760", "class_name": "RelatedNodeInfo"}}, "text": "#!/bin/env python3\nimport os\nimport tempfile\nimport streamlit as st\nfrom streamlit_chat import message\nfrom rag import ChatPDF\n\nst.set_page_config(page_title=\"ChatPDF\")\n\n\ndef display_messages():\n    st.subheader(\"Chat\")\n    for i, (msg, is_user) in enumerate(st.session_state[\"messages\"]):\n        message(msg, is_user=is_user, key=str(i))\n    st.session_state[\"thinking_spinner\"] = st.empty()\n\n\ndef process_input():\n    if st.session_state[\"user_input\"] and len(st.session_state[\"user_input\"].strip()) > 0:\n        user_text = st.session_state[\"user_input\"].strip()\n        with st.session_state[\"thinking_spinner\"], st.spinner(f\"Thinking\"):\n            agent_text = st.session_state[\"assistant\"].ask(user_text)\n\n        st.session_state[\"messages\"].append((user_text, True))\n        st.session_state[\"messages\"].append((agent_text, False))\n\n\ndef read_and_save_file():\n    st.session_state[\"assistant\"].clear()\n    st.session_state[\"messages\"] = []\n    st.session_state[\"user_input\"] = \"\"\n\n    for file in st.session_state[\"file_uploader\"]:\n        with tempfile.NamedTemporaryFile(delete=False) as tf:\n            tf.write(file.getbuffer())\n            file_path = tf.name\n\n        with st.session_state[\"ingestion_spinner\"], st.spinner(f\"Ingesting {file.name}\"):\n            st.session_state[\"assistant\"].ingest(file_path)\n        os.remove(file_path)\n\n\ndef page():\n    if len(st.session_state) == 0:\n        st.session_state[\"messages\"] = []\n        st.session_state[\"assistant\"] = ChatPDF()\n\n    st.header(\"ChatPDF\")\n\n    st.subheader(\"Upload a document\")\n    st.file_uploader(\n        \"Upload document\",\n        type=[\"pdf\"],\n        key=\"file_uploader\",\n        on_change=read_and_save_file,\n        label_visibility=\"collapsed\",\n        accept_multiple_files=True,\n    )\n\n    st.session_state[\"ingestion_spinner\"] = st.empty()\n\n    display_messages()\n    st.text_input(\"Message\", key=\"user_input\", on_change=process_input)\n\n\nif __name__ == \"__main__\":\n    page()", "start_char_idx": 0, "end_char_idx": 1970, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0515024f-c184-43de-9830-ba7e592b41ea": {"__data__": {"id_": "0515024f-c184-43de-9830-ba7e592b41ea", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6da36ca7f2c12fecf479ba97087c4854781277ae", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}, "hash": "e8a228997af3702c444246bb9f23e768e2b6a548c37daf1593ea0891c796ae7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1c45f48-654e-4c30-bdbe-77842f60e439", "node_type": "1", "metadata": {}, "hash": "d8a23d38c9fac276e27f5700cb1c1b14d0525a5f3aceda4650d084a28a3a3e63", "class_name": "RelatedNodeInfo"}}, "text": "from langchain_community.vectorstores import Chroma\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_community.embeddings import FastEmbedEmbeddings\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores.utils import filter_complex_metadata", "start_char_idx": 0, "end_char_idx": 524, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1c45f48-654e-4c30-bdbe-77842f60e439": {"__data__": {"id_": "b1c45f48-654e-4c30-bdbe-77842f60e439", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6da36ca7f2c12fecf479ba97087c4854781277ae", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}, "hash": "e8a228997af3702c444246bb9f23e768e2b6a548c37daf1593ea0891c796ae7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0515024f-c184-43de-9830-ba7e592b41ea", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}, "hash": "a35dc7c8d69354d73e9231b5aa2d1945a35890ea830efdd5cee25825dc778382", "class_name": "RelatedNodeInfo"}}, "text": "class ChatPDF:\n    vector_store = None\n    retriever = None\n    chain = None\n\n    def __init__(self):\n        self.model = ChatOllama(model=\"llama3:8b\")\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1024, chunk_overlap=100\n        )\n        self.prompt = PromptTemplate.from_template(\n            \"\"\"\n            <s> [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context \n            to answer the question. If you don't know the answer, just say that you don't know. Use three sentences\n             maximum and keep the answer concise. [/INST] </s> \n            [INST] Question: {question} \n            Context: {context} \n            Answer: [/INST]\n            \"\"\"\n        )\n\n    def ingest(self, pdf_file_path: str):\n        docs = PyPDFLoader(file_path=pdf_file_path).load()\n        chunks = self.text_splitter.split_documents(docs)\n        chunks = filter_complex_metadata(chunks)\n\n        vector_store = Chroma.from_documents(\n            documents=chunks, embedding=FastEmbedEmbeddings()\n        )\n        self.retriever = vector_store.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\n                \"k\": 3,\n                \"score_threshold\": 0.5,\n            },\n        )\n\n        self.chain = (\n            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n            | self.prompt\n            | self.model\n            | StrOutputParser()\n        )\n\n    def ask(self, query: str):\n        if not self.chain:\n            return \"Please, add a PDF document first.\"\n\n        return self.chain.invoke(query)\n\n    def clear(self):\n        self.vector_store = None\n        self.retriever = None\n        self.chain = None", "start_char_idx": 527, "end_char_idx": 2302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d147bf70-ffe7-4154-aa6e-c8aa0ae8fad7": {"__data__": {"id_": "d147bf70-ffe7-4154-aa6e-c8aa0ae8fad7", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "6036339b1928878b500c2790ad02552403f908e690e0d4b043d4db863240de20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09e17bcc-8168-4050-b24f-0a7a2b8c1591", "node_type": "1", "metadata": {}, "hash": "f906debfbda92553f3cad52d6e9e5c9085655bf11ebb46adbee1584dbbfca3d4", "class_name": "RelatedNodeInfo"}}, "text": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.llms import CTransformers, Ollama\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA, LLMChain\nfrom langchain import PromptTemplate\nfrom PyPDF2 import PdfReader\nfrom datetime import datetime\nfrom typing import Optional\nfrom io import BytesIO\nimport chainlit as cl\nimport sys\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import Runnable\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_core.documents import Document\nfrom langchain.chains import create_retrieval_chain\n\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom transformers import AutoTokenizer, TextStreamer\nfrom langchain.prompts import PromptTemplate\nimport transformers\nimport torch\n\nfrom langchain.indexes import VectorstoreIndexCreator\n\nfrom chainlit.playground.config import add_llm_provider\nfrom chainlit.playground.providers.langchain import LangchainGenericProvider\n\n# environment for the app\n# conda activate llama2Apps\n# command to run the app\n# chainlit run src/apps/localLLM_withRAG-Complete.py --port 8001 -w\n\nprompt_template = \"\"\"You are an helpful AI assistant and your name is SAHAYAK. You are kind, gentle and respectful to the user. Your job is to answer the question sent by the user in concise and step by step manner. \nIf you don't know the answer to a question, please don't share false information.\n\nContext: {context}\nQuestion: {question}\n\nResponse for Questions asked.\nanswer:\n\"\"\"\n\n# model used for converting text/queries to numerical embeddings\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# path to store embeddings at vectorstore\nindexpath = \"data/vectorstore/\"\n\n# Initialize embeddings using HuggingFace model\nembeddings = HuggingFaceEmbeddings(model_name=embedding_model)", "start_char_idx": 0, "end_char_idx": 2031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09e17bcc-8168-4050-b24f-0a7a2b8c1591": {"__data__": {"id_": "09e17bcc-8168-4050-b24f-0a7a2b8c1591", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "6036339b1928878b500c2790ad02552403f908e690e0d4b043d4db863240de20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d147bf70-ffe7-4154-aa6e-c8aa0ae8fad7", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "3a31aeac771c6f8ca98630a568e8b8b50be29b765f658525e4d598311819bbfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "355e3db2-0528-4f8e-b515-cb80d8ccb13d", "node_type": "1", "metadata": {}, "hash": "17c3d777362ae5997856cd9d106e9ba9bc8fa3a323e3e8d3e4da4dec914b6458", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_chat_start\nasync def factory():\n    # loads the data by the user\n    files = None\n\n    ### wait for the user to upload a data file\n    while files == None:\n        files = await cl.AskFileMessage(\n            content=\"\"\"Your personal AI asistant, SAHAYAK is ready to slog!\n                     To get started:\n                     \n1. Upload a pdf file                     \n2. Ask any questions about the file!\"\"\",\n            accept={\"text/plain\": [\".txt\"]},\n            max_size_mb=10,\n        ).send()\n\n    # Let the user know that the system is ready\n    await cl.Message(\n        content=f\"\"\"Document - `\"{files[0].name}\"` is uploaded and being processed!\"\"\"", "start_char_idx": 2034, "end_char_idx": 2704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "355e3db2-0528-4f8e-b515-cb80d8ccb13d": {"__data__": {"id_": "355e3db2-0528-4f8e-b515-cb80d8ccb13d", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "6036339b1928878b500c2790ad02552403f908e690e0d4b043d4db863240de20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09e17bcc-8168-4050-b24f-0a7a2b8c1591", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "5f9b1c1f801af00ea82bf207b459c4c1138ed302debe100878196fe023b52cc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b81565f2-a954-49d3-a895-c26d3c9244c9", "node_type": "1", "metadata": {}, "hash": "df6d630e68b70a398d6cb8b94bf070830856604b7b69ce40fdd727b8b75d1cd7", "class_name": "RelatedNodeInfo"}}, "text": ").send()\n\n    ### Reads and convert pdf data to text\n    file = files[0]\n    print(\"voici le type de file:\", type(file))\n    with open(file.path, \"r\", encoding=\"utf-8\") as f:\n        pdf_text = f.read()\n    ### Create embeddings for the uploaded documents and store in vector store\n    # Initialize a text splitter for processing long texts\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n    # Create documents by splitting the provided texts\n    documents = text_splitter.create_documents([pdf_text])\n    print(\"longueur du texte: \", len(documents), \"\\n\", pdf_text)\n    # Create a Faiss index from the embeddings\n    faiss_index = FAISS.from_documents(documents, embeddings)\n\n    # Save the Faiss index locally\n    faiss_index_path = indexpath + \"temp-index\"\n    faiss_index.save_local(faiss_index_path)\n    # Load Faiss vectorstore with embeddings created and saved earlier\n    load_db = FAISS.load_local(faiss_index_path, embeddings)\n    prompt = PromptTemplate(\n        template=prompt_template, input_variables=[\"context\", \"question\"]\n    )\n\n    model = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\")\n\n    qa_chain = RetrievalQA.from_chain_type(\n        llm=model,\n        chain_type=\"stuff\",\n        retriever=load_db.as_retriever(),\n        return_source_documents=True,\n        chain_type_kwargs={\"prompt\": prompt},\n    )\n\n    cl.user_session.set(\"chain\", qa_chain)\n\n    msg = cl.Message(content=\"The bot is getting initialized, please wait!!!\")\n    await msg.send()\n    msg.content = \"Your personal AI Assistant. Ask questions on the documents uploaded?\"\n    await msg.update()", "start_char_idx": 2709, "end_char_idx": 4348, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b81565f2-a954-49d3-a895-c26d3c9244c9": {"__data__": {"id_": "b81565f2-a954-49d3-a895-c26d3c9244c9", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "437ca10e7fcb650ad9d6cc7e00e85cf426195652", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "6036339b1928878b500c2790ad02552403f908e690e0d4b043d4db863240de20", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "355e3db2-0528-4f8e-b515-cb80d8ccb13d", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}, "hash": "2e24f88bae0c5db9c4f0921beceeb697d8fed93637f329eeb64d6733c59aa5a7", "class_name": "RelatedNodeInfo"}}, "text": "Ask questions on the documents uploaded?\"\n    await msg.update()\n\n\n@cl.on_message\nasync def main(message):\n    chain = cl.user_session.get(\"chain\")\n    msg = cl.Message(content=\"\")\n    async for chunk in chain.astream(\n        {\"query\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        print(\"Chunk received:\", chunk)\n        await msg.stream_token(chunk[\"result\"])\n    await msg.send()", "start_char_idx": 4284, "end_char_idx": 4728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06a78d16-75e4-4a9f-b0ee-02523dd160a1": {"__data__": {"id_": "06a78d16-75e4-4a9f-b0ee-02523dd160a1", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/requirements.txt"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4c84b615c79ae3c69552051e5b4c2f05c55718eb", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/requirements.txt"}, "hash": "3d5ebae59490b24498643f3cc8f8c072d188f8f3f75b588cbc5ff51f3684fc51", "class_name": "RelatedNodeInfo"}}, "text": "transformers\ntorch\nlangchain\nchainlit\naccelerate\nelasticsearch\nopenai\nsentence-transformers\nPyPDF2\nfaiss-cpu", "start_char_idx": 0, "end_char_idx": 108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9e4b838-2f23-4e87-8ceb-619184d9112b": {"__data__": {"id_": "d9e4b838-2f23-4e87-8ceb-619184d9112b", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "42ee3131b28c345218810534bed4316f4c159da77d02f597186994380f522d1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7", "node_type": "1", "metadata": {}, "hash": "d8f2e35a1142dd359b6b2948c953717cb6c1bd8eeb31a354b595e7e7347d6551", "class_name": "RelatedNodeInfo"}}, "text": "import os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.llms import Ollama\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.prompts import PromptTemplate\nfrom PyPDF2 import PdfReader\nfrom typing import List\nimport chainlit as cl\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain_core.runnables import RunnablePassthrough\nimport os\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n    CharacterTextSplitter,\n)\nfrom langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom llama_index.embeddings.ollama import OllamaEmbedding\n\n# Configuration\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\nindex_path = \"data/vectorstore/temp-index.faiss\"\n\n# Ensure the directory exists\nos.makedirs(os.path.dirname(index_path), exist_ok=True)\n\n# Initialize embeddings\nembeddings = HuggingFaceEmbeddings(\n    model_name=embedding_model,\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": False},\n)\nmodel = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:8b\")\nprompt = PromptTemplate(\n    template=\"\"\"You are a helpful AI assistant named SAHAYAK. Answer the question based on the context.\n\nContext: {context}\nQuestion: {question}\n\nAnswer:\"\"\",\n    input_variables=[\"context\", \"question\"],\n)\n\n# Global index variable\nfaiss_index = None", "start_char_idx": 0, "end_char_idx": 1552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7": {"__data__": {"id_": "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "42ee3131b28c345218810534bed4316f4c159da77d02f597186994380f522d1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9e4b838-2f23-4e87-8ceb-619184d9112b", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "e252c66b583469889f64d8f4f76b648ebefda975affa90578d15131ed90df3bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c8dd152-fc27-4e62-95f4-718e8b5b439f", "node_type": "1", "metadata": {}, "hash": "034dfbf802a592a5b4f1e52d9930ab8f75c4afcbe11f6d4227b60c22d008fdb5", "class_name": "RelatedNodeInfo"}}, "text": "# Function to read PDF and return text\ndef read_text_from_file(file_path: str) -> str:\n    if file_path.lower().endswith(\".pdf\"):\n        with open(file_path, \"rb\") as f:\n            reader = PdfReader(f)\n            return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n    elif file_path.lower().endswith(\".txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n    else:\n        raise ValueError(\"Unsupported file type. Please upload a .txt or .pdf file.\")\n\n\ndef update_faiss_index(faiss_index: FAISS, documents: List[str]):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n    new_docs = text_splitter.create_documents(documents)\n    faiss_index.add_documents(new_docs)\n    faiss_index.save_local(index_path, index_name=\"tryoutIndex\")\n\n\ndef get_faiss_index(documents: List[str] = None) -> FAISS:\n    global faiss_index\n    if faiss_index is None:\n        if os.path.exists(os.path.join(index_path, \"tryoutIndex.faiss\")):\n            print(\"Loading existing index...\")\n            faiss_index = FAISS.load_local(\n                index_path, embeddings, index_name=\"tryoutIndex\"\n            )\n            if documents:\n                update_faiss_index(faiss_index, documents)\n        else:\n            print(\"Creating new index...\")\n            if documents is None:\n                raise ValueError(\"No documents provided to create an index.\")\n            text_splitter = RecursiveCharacterTextSplitter(\n                chunk_size=1000, chunk_overlap=10\n            )\n            docs = text_splitter.create_documents(documents)\n            faiss_index = FAISS.from_documents(docs, embeddings)\n            faiss_index.save_local(index_path, index_name=\"tryoutIndex\")\n    elif documents:\n        print(\"Updating index with new documents...\")\n        update_faiss_index(faiss_index, documents)\n    return faiss_index", "start_char_idx": 1555, "end_char_idx": 3464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c8dd152-fc27-4e62-95f4-718e8b5b439f": {"__data__": {"id_": "8c8dd152-fc27-4e62-95f4-718e8b5b439f", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "42ee3131b28c345218810534bed4316f4c159da77d02f597186994380f522d1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "2891fe8a4251bb475f776cb9a65eaae38597df1d5d69889063019579a6600f42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8d484d7-6307-46a3-9ebc-1bc83682cab3", "node_type": "1", "metadata": {}, "hash": "7988a290c76115f080a6329d3516f3fa5bdec28be4191d1fae67f30979db97dd", "class_name": "RelatedNodeInfo"}}, "text": "def search_and_answer(question: str, retriever: FAISS, prompt: PromptTemplate) -> str:\n    # Effectuer la recherche\n    search_docs = retriever.similarity_search(question)\n\n    # Pr\u00e9parer le contexte pour le prompt\n    context = \"\\n\\n\".join([doc.page_content for doc in search_docs])\n\n    # G\u00e9n\u00e9rer la r\u00e9ponse\n    formatted_prompt = prompt.format(context=context, question=question)\n    answer = model(formatted_prompt)\n    return answer\n\n\n@cl.on_chat_start\nasync def factory():\n    files = None\n    while files is None:\n        files = await cl.AskFileMessage(\n            content=\"\"\"Your personal AI assistant, SAHAYAK is ready to help!\n                        To get started:\n                        \n1. Upload a PDF file                     \n2. Ask any questions about the file!\"\"\",\n            accept={\"application/pdf\": [\".pdf\"], \"text/plain\": [\".txt\"]},\n            max_size_mb=10,\n        ).send()\n\n    await cl.Message(\n        content=f\"\"\"Document - `\"{files[0].name}\"` is uploaded and being processed!\"\"\"\n    ).send()\n\n    # Read and process PDF file\n    file_text = read_text_from_file(files[0].path)\n    print(\n        f\"Processed text from {files[0].name}: {file_text[:60]}...\"\n    )  # Print a snippet for v\u00e9rification\n\n    # Load or update the FAISS index\n    faiss_index = get_faiss_index([file_text])\n\n    question = input(\"Enter your query:\")\n    rag_data = {\"context\": faiss_index.similarity_search, \"question\": question}\n    rag_chain = RunnablePassthrough() | prompt | model | StrOutputParser()\n    print(\"Answer:\", rag_chain.invoke(rag_data))\n    \"\"\"\n    searchDocs = faiss_index.similarity_search(question)\n    for doc in searchDocs:\n        print(\"similarity_search:\", doc.page_content)\n    \"\"\"\n    # Set la cha\u00eene de recherche dans la session utilisateur\n    cl.user_session.set(\"retriever\", faiss_index)\n\n    msg = cl.Message(content=\"The bot is initialized. Ask your questions!\")\n    await msg.send()", "start_char_idx": 3467, "end_char_idx": 5395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8d484d7-6307-46a3-9ebc-1bc83682cab3": {"__data__": {"id_": "c8d484d7-6307-46a3-9ebc-1bc83682cab3", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "42ee3131b28c345218810534bed4316f4c159da77d02f597186994380f522d1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c8dd152-fc27-4e62-95f4-718e8b5b439f", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}, "hash": "f79af4eaf174745131ac81a622a16dcbd38f96cc00c1110eef5943392d8c78c7", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_message\nasync def main(message):\n    retriever = cl.user_session.get(\"retriever\")\n    if retriever is None:\n        await cl.Message(\n            content=\"Error: No retriever found. Please restart the chat.\"\n        ).send()\n        return\n\n    question = (\n        message.content\n    )  # R\u00e9cup\u00e9rer la question de l'utilisateur depuis le message Chainlit\n\n    search_docs = retriever.similarity_search(\n        question\n    )  # Effectuer la recherche de similarit\u00e9 avec la question\n\n    context = \"\\n\\n\".join(\n        [doc.page_content for doc in search_docs]\n    )  # Pr\u00e9parer le contexte pour le prompt\n\n    formatted_prompt = prompt.format(\n        context=context, question=question\n    )  # Formater le prompt avec le contexte et la question\n\n    answer = model(formatted_prompt)  # G\u00e9n\u00e9rer la r\u00e9ponse compl\u00e8te\n\n    await cl.Message(content=answer).send()  # Envoyer la r\u00e9ponse \u00e0 l'utilisateur", "start_char_idx": 5398, "end_char_idx": 6306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91d40976-02c4-4352-b7af-0b5c5c750770": {"__data__": {"id_": "91d40976-02c4-4352-b7af-0b5c5c750770", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/testElsCloudv2.py", "file_name": "testElsCloudv2.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/testElsCloudv2.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e994a9b32cc2cff65d7d736018149f076cf4a9ab", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/testElsCloudv2.py", "file_name": "testElsCloudv2.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/testElsCloudv2.py"}, "hash": "6e8f7a2822b59da2f74bf583213848b883cf3ea30608572f97d95f6ee2120a7d", "class_name": "RelatedNodeInfo"}}, "text": "from elasticsearch import Elasticsearch, helpers\nimport configparser\nimport json\nfrom llama_index.core import Document, Settings\nfrom llama_index.core import StorageContext, VectorStoreIndex\nfrom llama_index.embeddings.ollama import OllamaEmbedding\nfrom llama_index.embeddings.elasticsearch import ElasticsearchEmbedding\nfrom llama_index.vector_stores.elasticsearch import ElasticsearchStore\n\nconfig = configparser.ConfigParser()\nconfig.read(\"example.ini\")\n\nes = Elasticsearch(\n    cloud_id=config[\"ELASTIC\"][\"cloud_id\"],\n    basic_auth=(config[\"ELASTIC\"][\"user\"], config[\"ELASTIC\"][\"password\"]),\n)\n\nprint(es.info())\n# aller dans Deployment -> Security -> security changes in kibana -> create an user with the appropriates roles\n\nwith open(\"conversations.json\", \"r\") as f:\n    conversations = json.load(f)\n\n\"\"\"\nembeddings = ElasticsearchEmbedding.from_credentials(\n    model_id=\"llama3:8b\",\n    es_url=\"localhost:9200\",\n    es_username=config[\"ELASTIC\"][\"user\"],\n    es_password=config[\"ELASTIC\"][\"password\"],\n)\n\"\"\"\n\n\n# Create an OLLAMA embedding model\nollama_embedding = OllamaEmbedding(\"llama3:8b\")\n# set global settings\nSettings.embed_model = ollama_embedding\nSettings.chunk_size = 512\n# Create an Elasticsearch store\nes_store = ElasticsearchStore(\n    index_name=\"conversations\",\n    es_client=es,\n)\n\nstorage_context = StorageContext.from_defaults(vector_store=es_store)\n\n\nindex = VectorStoreIndex.from_vector_store(\n    vector_store=es_store,\n    storage_context=storage_context,\n)\n\nquery_engine = index.as_query_engine()\n\n\nresponse = query_engine.query(\"hello world\")\n\"\"\"\n# Iterate over the conversations and create Document objects\ndocuments = []\nfor conversation in conversations:\n    doc = Document(\n        text=conversation[\"conversation\"],\n        metadata={\"conversation_id\": conversation[\"conversation_id\"]},\n    )\n    documents.append(doc)\n\"\"\"", "start_char_idx": 0, "end_char_idx": 1858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00758c9f-dbeb-4881-8ac4-5ab4808ba4a9": {"__data__": {"id_": "00758c9f-dbeb-4881-8ac4-5ab4808ba4a9", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a16ceebef3baf18c14472addad583f57beebb1", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "fad0db3ba4cc5e99ea36b4d19e3ab2365f574e677e31607c6c6129f6502507e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b356805-9161-4f68-908f-bb21afcdc0ff", "node_type": "1", "metadata": {}, "hash": "9149da4184eb8a76f111d640374c3b02701ba2b30c4b0ec624ed2860ad2587ef", "class_name": "RelatedNodeInfo"}}, "text": "import os\nimport tkinter as tk\nfrom tkinter import filedialog\nimport PyPDF2\nimport re\nimport json\n\n# Function to convert PDF to text and append to vault.txt\ndef convert_pdf_to_text():\n    file_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n    if file_path:\n        with open(file_path, 'rb') as pdf_file:\n            pdf_reader = PyPDF2.PdfReader(pdf_file)\n            num_pages = len(pdf_reader.pages)\n            text = ''\n            for page_num in range(num_pages):\n                page = pdf_reader.pages[page_num]\n                if page.extract_text():\n                    text += page.extract_text() + \" \"\n            \n            # Normalize whitespace and clean up text\n            text = re.sub(r'\\s+', ' ', text).strip()\n            \n            # Split text into chunks by sentences, respecting a maximum chunk size\n            sentences = re.split(r'(?<=[.!?]) +', text)  # split on spaces following sentence-ending punctuation\n            chunks = []\n            current_chunk = \"\"\n            for sentence in sentences:\n                # Check if the current sentence plus the current chunk exceeds the limit\n                if len(current_chunk) + len(sentence) + 1 < 1000:  # +1 for the space\n                    current_chunk += (sentence + \" \").strip()\n                else:\n                    # When the chunk exceeds 1000 characters, store it and start a new one\n                    chunks.append(current_chunk)\n                    current_chunk = sentence + \" \"\n            if current_chunk:  # Don't forget the last chunk!\n                chunks.append(current_chunk)\n            with open(\"vault.txt\", \"a\", encoding=\"utf-8\") as vault_file:\n                for chunk in chunks:\n                    # Write each chunk to its own line\n                    vault_file.write(chunk.strip() + \"\\n\")  # Two newlines to separate chunks\n            print(f\"PDF content appended to vault.txt with each chunk on a separate line.\")", "start_char_idx": 0, "end_char_idx": 1968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b356805-9161-4f68-908f-bb21afcdc0ff": {"__data__": {"id_": "9b356805-9161-4f68-908f-bb21afcdc0ff", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a16ceebef3baf18c14472addad583f57beebb1", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "fad0db3ba4cc5e99ea36b4d19e3ab2365f574e677e31607c6c6129f6502507e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00758c9f-dbeb-4881-8ac4-5ab4808ba4a9", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "57563deb352fb96e31552a280cd538bb860ecc99abe74c6ad791401e6bfdb7a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3f28b63-c0e1-483d-96ea-5f23a0b23be8", "node_type": "1", "metadata": {}, "hash": "7a196fc5fa21d74319c9e046e2ce784b6b8d73c97c4859b5a5671e457f1980a3", "class_name": "RelatedNodeInfo"}}, "text": "# Function to upload a text file and append to vault.txt\ndef upload_txtfile():\n    file_path = filedialog.askopenfilename(filetypes=[(\"Text Files\", \"*.txt\")])\n    if file_path:\n        with open(file_path, 'r', encoding=\"utf-8\") as txt_file:\n            text = txt_file.read()\n            \n            # Normalize whitespace and clean up text\n            text = re.sub(r'\\s+', ' ', text).strip()\n            \n            # Split text into chunks by sentences, respecting a maximum chunk size\n            sentences = re.split(r'(?<=[.!?]) +', text)  # split on spaces following sentence-ending punctuation\n            chunks = []\n            current_chunk = \"\"\n            for sentence in sentences:\n                # Check if the current sentence plus the current chunk exceeds the limit\n                if len(current_chunk) + len(sentence) + 1 < 1000:  # +1 for the space\n                    current_chunk += (sentence + \" \").strip()\n                else:\n                    # When the chunk exceeds 1000 characters, store it and start a new one\n                    chunks.append(current_chunk)\n                    current_chunk = sentence + \" \"\n            if current_chunk:  # Don't forget the last chunk!\n                chunks.append(current_chunk)\n            with open(\"vault.txt\", \"a\", encoding=\"utf-8\") as vault_file:\n                for chunk in chunks:\n                    # Write each chunk to its own line\n                    vault_file.write(chunk.strip() + \"\\n\")  # Two newlines to separate chunks\n            print(f\"Text file content appended to vault.txt with each chunk on a separate line.\")", "start_char_idx": 1970, "end_char_idx": 3582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3f28b63-c0e1-483d-96ea-5f23a0b23be8": {"__data__": {"id_": "b3f28b63-c0e1-483d-96ea-5f23a0b23be8", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a16ceebef3baf18c14472addad583f57beebb1", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "fad0db3ba4cc5e99ea36b4d19e3ab2365f574e677e31607c6c6129f6502507e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b356805-9161-4f68-908f-bb21afcdc0ff", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "b458612c3ac50807592539ead28a3af20c0a3fa495be3bdb22bb48e4a15a9307", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d23a1de1-a1cc-45d6-9402-c2dece59952b", "node_type": "1", "metadata": {}, "hash": "00deb19cb1a4bc3c6a65e89c5aac2bb72b673b5640518e36323fe32000107907", "class_name": "RelatedNodeInfo"}}, "text": "# Function to upload a JSON file and append to vault.txt\ndef upload_jsonfile():\n    file_path = filedialog.askopenfilename(filetypes=[(\"JSON Files\", \"*.json\")])\n    if file_path:\n        with open(file_path, 'r', encoding=\"utf-8\") as json_file:\n            data = json.load(json_file)\n            \n            # Flatten the JSON data into a single string\n            text = json.dumps(data, ensure_ascii=False)\n            \n            # Normalize whitespace and clean up text\n            text = re.sub(r'\\s+', ' ', text).strip()\n            \n            # Split text into chunks by sentences, respecting a maximum chunk size\n            sentences = re.split(r'(?<=[.!?]) +', text)  # split on spaces following sentence-ending punctuation\n            chunks = []\n            current_chunk = \"\"\n            for sentence in sentences:\n                # Check if the current sentence plus the current chunk exceeds the limit\n                if len(current_chunk) + len(sentence) + 1 < 1000:  # +1 for the space\n                    current_chunk += (sentence + \" \").strip()\n                else:\n                    # When the chunk exceeds 1000 characters, store it and start a new one\n                    chunks.append(current_chunk)\n                    current_chunk = sentence + \" \"\n            if current_chunk:  # Don't forget the last chunk!\n                chunks.append(current_chunk)\n            with open(\"vault.txt\", \"a\", encoding=\"utf-8\") as vault_file:\n                for chunk in chunks:\n                    # Write each chunk to its own line\n                    vault_file.write(chunk.strip() + \"\\n\")  # Two newlines to separate chunks\n            print(f\"JSON file content appended to vault.txt with each chunk on a separate line.\")", "start_char_idx": 3584, "end_char_idx": 5330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d23a1de1-a1cc-45d6-9402-c2dece59952b": {"__data__": {"id_": "d23a1de1-a1cc-45d6-9402-c2dece59952b", "embedding": null, "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9a16ceebef3baf18c14472addad583f57beebb1", "node_type": "4", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "fad0db3ba4cc5e99ea36b4d19e3ab2365f574e677e31607c6c6129f6502507e5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3f28b63-c0e1-483d-96ea-5f23a0b23be8", "node_type": "1", "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}, "hash": "19700a36f21880849e561c9e800c0bd5e81be435e352f2e396613506d87e1d74", "class_name": "RelatedNodeInfo"}}, "text": "# Create the main window\nroot = tk.Tk()\nroot.title(\"Upload .pdf, .txt, or .json\")\n\n# Create a button to open the file dialog for PDF\npdf_button = tk.Button(root, text=\"Upload PDF\", command=convert_pdf_to_text)\npdf_button.pack(pady=10)\n\n# Create a button to open the file dialog for text file\ntxt_button = tk.Button(root, text=\"Upload Text File\", command=upload_txtfile)\ntxt_button.pack(pady=10)\n\n# Create a button to open the file dialog for JSON file\njson_button = tk.Button(root, text=\"Upload JSON File\", command=upload_jsonfile)\njson_button.pack(pady=10)\n\n# Run the main event loop\nroot.mainloop()", "start_char_idx": 5332, "end_char_idx": 5932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d53c00f-6846-488d-92d6-609e35f935e4": {"__data__": {"id_": "6d53c00f-6846-488d-92d6-609e35f935e4", "embedding": null, "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4507ac4676a6387c4b52a0d1111e94753a102b32", "node_type": "4", "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}, "hash": "272db777274ed25a2e12b65324fbb9537f1457cec9c91f3d0f5489fe57d1ec7a", "class_name": "RelatedNodeInfo"}}, "text": "# Welcome to Chainlit! \ud83d\ude80\ud83e\udd16\n\nHi there, Developer! \ud83d\udc4b We're excited to have you on board. Chainlit is a powerful tool designed to help you prototype, debug and share applications built on top of LLMs.\n\n## Useful Links \ud83d\udd17\n\n- **Documentation:** Get started with our comprehensive [Chainlit Documentation](https://docs.chainlit.io) \ud83d\udcda\n- **Discord Community:** Join our friendly [Chainlit Discord](https://discord.gg/k73SQ3FyUh) to ask questions, share your projects, and connect with other developers! \ud83d\udcac\n\nWe can't wait to see what you create with Chainlit! Happy coding! \ud83d\udcbb\ud83d\ude0a\n\n## Welcome screen\n\nTo modify the welcome screen, edit the `chainlit.md` file at the root of your project. If you do not want a welcome screen, just leave this file empty.", "start_char_idx": 0, "end_char_idx": 736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc67192d-5a82-4291-bb8b-0ab50a594df0": {"__data__": {"id_": "bc67192d-5a82-4291-bb8b-0ab50a594df0", "embedding": null, "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b930774918cc5aeecbaceff681af53f7a9117ec4", "node_type": "4", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "5df69080fbd27175d0d9c8830cce9931618f99d2de8d5c85db528915a15e9933", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9974d7a5-8002-433d-bba4-d6468dd3fc51", "node_type": "1", "metadata": {}, "hash": "bb807b890d1a621ce09fea2ce7e3f4fa470e426f65d7ce5152fa010f62a29e84", "class_name": "RelatedNodeInfo"}}, "text": "import os\nimport chainlit as cl\nfrom numpy import vectorize\nfrom utils.manip_documents import *\nfrom utils.web_scraper import *\n\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.schema.runnable import RunnablePassthrough, RunnableLambda\nfrom langchain.memory import ConversationBufferMemory\n\nfrom operator import itemgetter\nfrom typing import List\n\n\nfrom chainlit.input_widget import TextInput, Select\n\n\n@cl.password_auth_callback\ndef auth_callback(username: str, password: str):\n\n    if (username, password) == (\"elias\", \"elias\"):\n        return cl.User(\n            identifier=\"Elias\", metadata={\"role\": \"admin\", \"provider\": \"credentials\"}\n        )\n    else:\n        return None\n\n\n@cl.step(type=\"run\", name=\"Mise en place du Runnable\")\ndef setup_model():\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n    prompt_exercice = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"\"\"Instruction: R\u00e9pondre en francais \u00e0 la question de l'utilisateur en te basant **uniquement** sur le contexte suivant fourni.\n                Si tu ne trouves pas la r\u00e9ponse dans le contexte, demande \u00e0 l'utilisateur d'\u00eatre plus pr\u00e9cis au lieu de deviner.\n                Context:{context}\"\"\",\n            ),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"Question: {question}\"),\n            (\"ai\", \"\"\"R\u00e9ponse:\"\"\"),\n        ]\n    )\n\n    runnable_exercice = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(\n                memory.load_memory_variables) | itemgetter(\"history\")\n        )\n        | prompt_exercice\n        | model\n        | StrOutputParser()\n    )\n    return runnable_exercice", "start_char_idx": 0, "end_char_idx": 1912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9974d7a5-8002-433d-bba4-d6468dd3fc51": {"__data__": {"id_": "9974d7a5-8002-433d-bba4-d6468dd3fc51", "embedding": null, "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b930774918cc5aeecbaceff681af53f7a9117ec4", "node_type": "4", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "5df69080fbd27175d0d9c8830cce9931618f99d2de8d5c85db528915a15e9933", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc67192d-5a82-4291-bb8b-0ab50a594df0", "node_type": "1", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "5e21be734ae2b305ca2f0d7d6e732394f2bfd90124f0f3c5e5905055afdf0734", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e828478-aeea-4834-9dae-713ba982e820", "node_type": "1", "metadata": {}, "hash": "ba8c6ef68f8f86306dc3339dd4d99d934f8896616db8b79b67bfc68298b936c8", "class_name": "RelatedNodeInfo"}}, "text": "@cl.step(type=\"retrieval\", name=\"Context via similarity_search\")\ndef trouve_contexte(question):\n    retriever = cl.user_session.get(\"retriever\")\n    search_results = retriever.vectorstore.similarity_search(question, k=10)\n\n    # Utiliser un dictionnaire pour regrouper les chunks par source\n    results_by_source = {}\n    for result in search_results:\n        source = result.metadata[\"source\"]\n        if source not in results_by_source:\n            results_by_source[source] = []\n        results_by_source[source].append(result)\n\n    # sources diff\u00e9rentes\n    relevant_sources = list(results_by_source.keys())[:2]\n    # chunks par source\n    relevant_results = [results_by_source[source][:10]\n                        for source in relevant_sources]\n\n    # Aplatir la liste des r\u00e9sultats\n    relevant_results = [\n        chunk for sublist in relevant_results for chunk in sublist]\n\n    filenames = [result.metadata[\"source\"] for result in relevant_results]\n    short_filenames = [os.path.basename(file) for file in filenames]\n    print(\"mod\u00e8le d'embedding:\" + str(embeddings))\n    print(\"Files used for context:\", short_filenames)\n    context = \"\\n\".join(\n        [\n            f\"-----\\nSource: {os.path.basename(result.metadata['source'])}\\n{result.page_content}\"\n            for result in relevant_results\n        ]\n    )\n\n    print(\"-------------------\\n\" + context)\n    return context\n\n\n@cl.on_chat_start\nasync def factory():\n    settings = await cl.ChatSettings(\n        [\n            TextInput(\n                id=\"addDocuments\",\n                label=\"Pr\u00e9cisez le chemin\",\n                placeholder=\"differents_textes/...\",\n            ),\n            Select(\n                id=\"model\",\n                label=\"Model\",\n                values=[\n                    \"instructor-xl\",\n                    \"instructor-base\",\n                    \"instructor-large\",\n                    \"mpnet-v2\",\n                    \"camembert-base\",\n                ],\n                initial_index=2,\n            ),\n        ]\n    ).send()\n    cl.user_session.set(\n        \"memory\", ConversationBufferMemory(return_messages=True))\n    cl.user_session.set(\"nom_model\", \"instructor-large\")\n\n    charge_index(index_path, embeddings)", "start_char_idx": 1915, "end_char_idx": 4133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e828478-aeea-4834-9dae-713ba982e820": {"__data__": {"id_": "2e828478-aeea-4834-9dae-713ba982e820", "embedding": null, "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b930774918cc5aeecbaceff681af53f7a9117ec4", "node_type": "4", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "5df69080fbd27175d0d9c8830cce9931618f99d2de8d5c85db528915a15e9933", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9974d7a5-8002-433d-bba4-d6468dd3fc51", "node_type": "1", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "a131e47077f7b798305bd78e08bb3308cf33bfa63ea6a50ff65f85b90caf0932", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d73007a1-d8ba-4529-98ca-fa7c4645c93d", "node_type": "1", "metadata": {}, "hash": "3047d5c8fdf862708a553cb1ddf9601a8e3cb883f98ab9dfec5e90edf963b7d5", "class_name": "RelatedNodeInfo"}}, "text": "def charge_index(new_index_path, new_embeddings):\n    # print(f\"index_path: {new_index_path}\\nembeddings:{new_embeddings}\")\n    if os.path.exists(new_index_path):\n        vectorstore = FAISS.load_local(\n            new_index_path, embeddings=new_embeddings, allow_dangerous_deserialization=True\n        )\n        print(\"Index charg\u00e9 \u00e0 partir du chemin existant.\")\n    else:\n\n        # pour faire disparaitre un warning\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n        webpage_dict = [\n            {\"url\": \"https://e-services.uha.fr/fr/index.html\", \"type\": \"connexion\"},\n            {\"url\": \"https://www.emploisdutemps.uha.fr/\", \"type\": \"edt\"},\n            {\"url\": \"https://e-formation.uha.fr/login/index.php?authCAS=CAS\",\n                \"type\": \"connexion\"},\n            {\"url\": \"https://e-formation.uha.fr/my/courses.php\",\n                \"type\": \"accueil_moodle\"},  # page mes cours sur moodle\n            {\"url\": \"https://e-partage.uha.fr/modern/email/Sent\", \"type\": \"partage\"},\n            {\"url\": \"https://www.uha.fr/fr/formation-1/accompagnement-a-la-reussite-1/numerique.html\",\n                \"type\": \"plain\"}\n        ]\n        web_scraped = load_web_documents_firefox(\n            webpage_dict, \"https://cas.uha.fr/cas/login\")\n        if web_scraped is None:\n            \"web_scraped est vide\"\n        chunks_web = web_scraped[\"web_result\"]\n        chunks_pdf = load_new_documents(\"differents_textes/moodle\")\n\n        print(\"les deux chunks ont \u00e9t\u00e9 r\u00e9cup\u00e9r\u00e9s\")\n        print(f\"liste des url pdf:{web_scraped['pdf_to_read']}\")\n\n        vectorstore = FAISS.from_documents(\n            documents=chunks_web + chunks_pdf, embedding=new_embeddings)\n\n        vectorstore.save_local(new_index_path)\n        print(\"Nouvel index cr\u00e9\u00e9 et sauvegard\u00e9.\")", "start_char_idx": 4136, "end_char_idx": 5900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d73007a1-d8ba-4529-98ca-fa7c4645c93d": {"__data__": {"id_": "d73007a1-d8ba-4529-98ca-fa7c4645c93d", "embedding": null, "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b930774918cc5aeecbaceff681af53f7a9117ec4", "node_type": "4", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "5df69080fbd27175d0d9c8830cce9931618f99d2de8d5c85db528915a15e9933", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e828478-aeea-4834-9dae-713ba982e820", "node_type": "1", "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}, "hash": "db6ee1cb4c4b52e0d590508f69449628635944e8238e7693520e234b83bcf37e", "class_name": "RelatedNodeInfo"}}, "text": "print(f\"Type d'index: {vectorstore.index}\")\n    retriever = vectorstore.as_retriever()\n    cl.user_session.set(\"retriever\", retriever)\n\n\n@cl.on_message\nasync def main(message):\n    memory = cl.user_session.get(\"memory\")\n    question = message.content\n    print(\"Question:\" + question)\n\n    # setup_model() et trouve_contexte() \u00e0 adapter suivant ce qui est recherch\u00e9\n    runnable_model = setup_model()\n    msg = cl.Message(content=\"\", author=cl.user_session.get(\"nom_model\"))\n    async for chunk in runnable_model.astream(\n        {\"question\": question, \"context\": trouve_contexte(question)},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await msg.stream_token(chunk)\n\n    await msg.send()\n\n\n@cl.on_settings_update\nasync def setup_agent(settings):\n    print(\"on_settings_update\", settings)\n\n    embeddings_t, index_path_t = change_model(settings[\"model\"])\n    charge_index(index_path_t, embeddings_t)\n    cl.user_session.set(\"nom_model\", settings[\"model\"])\n    if settings[\"addDocuments\"] is not None:\n        add_files_to_index(index_path_t, embeddings_t,\n                           settings[\"addDocuments\"])\n        settings[\"addDocuments\"] = \"\"", "start_char_idx": 5906, "end_char_idx": 7096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5c852aa-0697-4490-8078-b3cb0759af1d": {"__data__": {"id_": "f5c852aa-0697-4490-8078-b3cb0759af1d", "embedding": null, "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b", "node_type": "4", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "c707e29cc2b7e4c47832053a51312798b89bab262556ce615fad2ddf9817d62d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89ab858a-7a78-403b-a251-18ebf50ed77d", "node_type": "1", "metadata": {}, "hash": "26f03099c52c7b1023d6cd8356773a03f6538c271c685e3ff0458faee3fb666a", "class_name": "RelatedNodeInfo"}}, "text": "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings, load_index_from_storage\nfrom llama_index.vector_stores.faiss import FaissVectorStore\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\nimport faiss\nimport os\n\n\nembed_model = HuggingFaceEmbedding(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n)\nllm = Ollama(base_url=\"http://localhost:11434\",\n             model=\"llama3:instruct\", request_timeout=1000.0)\n\nd = 768\nfaiss_index = faiss.IndexFlatL2(d)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\nSettings.num_output = 512\nSettings.context_window = 3900\n\nvector_store = FaissVectorStore(faiss_index=faiss_index)\nstorage_context_global = StorageContext.from_defaults(\n    vector_store=vector_store)\n\nprint(Settings)\n# Charger l'index si disponible", "start_char_idx": 0, "end_char_idx": 1003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89ab858a-7a78-403b-a251-18ebf50ed77d": {"__data__": {"id_": "89ab858a-7a78-403b-a251-18ebf50ed77d", "embedding": null, "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b", "node_type": "4", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "c707e29cc2b7e4c47832053a51312798b89bab262556ce615fad2ddf9817d62d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5c852aa-0697-4490-8078-b3cb0759af1d", "node_type": "1", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "6f3044f4dfa94aff9603ccd72f05a614019f6d82bea303ca1f39f86757709c7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fe92b9f-affa-4523-bf28-4ed02ed7bea2", "node_type": "1", "metadata": {}, "hash": "1217259723fd0ef86d27210706684c1fe0ebb435ffca89c28f6332763f310257", "class_name": "RelatedNodeInfo"}}, "text": "def charge_index(index_path):\n    if os.path.exists(index_path):\n        vector_store = FaissVectorStore.from_persist_dir(index_path)\n        storage_context = StorageContext.from_defaults(\n            vector_store=vector_store, persist_dir=index_path)\n        index = load_index_from_storage(storage_context=storage_context)\n        return index\n        storage_context = StorageContext.from_defaults(\n            persist_dir=index_path\n        )\n        print(\"Index existant charg\u00e9\")\n        return load_index_from_storage(storage_context)\n    else:\n        # Cr\u00e9er un nouvel index si non disponible\n        os.makedirs(index_path)\n        print(\"Cr\u00e9ation d'un nouvel index\")\n        reader = SimpleDirectoryReader(\n            input_dir=\"differents_textes\", exclude_hidden=True, recursive=True)\n        # documents = reader.load_data()\n        documents = []\n        for docs in reader.iter_data(show_progress=True):\n            # <do something with the documents per file>\n            documents.extend(docs)\n        index = VectorStoreIndex.from_documents(\n            documents, storage_context=storage_context_global, show_progress=True)\n        index.storage_context.persist(index_path)\n    return index\n\n\nindex = charge_index(index_path=\"vectorstores/llama_index_mpnet\")\n# Utilisation de l'index pour une requ\u00eate\nprompt = input(\n    \"Entrez une requ\u00eate portant sur moodle ou les textes dans differents_textes:\")\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(prompt)\nprint(response.response)\n\nfor node in response.source_nodes:\n    print(f\"{node.get_score()} \ud83d\udc49 {node.text}\")", "start_char_idx": 1006, "end_char_idx": 2611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fe92b9f-affa-4523-bf28-4ed02ed7bea2": {"__data__": {"id_": "1fe92b9f-affa-4523-bf28-4ed02ed7bea2", "embedding": null, "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b", "node_type": "4", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "c707e29cc2b7e4c47832053a51312798b89bab262556ce615fad2ddf9817d62d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89ab858a-7a78-403b-a251-18ebf50ed77d", "node_type": "1", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "5b09a8a13cde594287c6c002b864bfb7212afdfec49dcf8f0a463543baeba8b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c88b7851-7cf4-44eb-9963-5953e476fd83", "node_type": "1", "metadata": {}, "hash": "d95649612af87f1ae54b26a9320d8b54ca95a608d0d6f5089c03a5883c166efc", "class_name": "RelatedNodeInfo"}}, "text": "\"\"\"\n# Commented out IPython magic to ensure Python compatibility.\n# %pip install llama-index-vector-stores-faiss\n# !pip install llama-index\n# pip install torch transformers python-pptx Pillow\n# pip install docx2txt\n\nimport faiss\nfrom IPython.display import Markdown, display\nfrom llama_index.vector_stores.faiss import FaissVectorStore\nfrom llama_index.core import (\n    SimpleDirectoryReader,\n    load_index_from_storage,\n    VectorStoreIndex,\n    StorageContext,\n)\nimport sys\nimport logging\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\nfrom llama_index.vector_stores.faiss import FaissVectorStore\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.llms.ollama import Ollama\nimport faiss\nimport os\n\n# Creating a Faiss Index\n\n# Load documents, build the VectorStoreIndex\n\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n\nmpnet_embeddings = HuggingFaceEmbedding(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": False},\n)\nd = 768\nfaiss_index = faiss.IndexFlatL2(d)\n\n\n# Download Data", "start_char_idx": 2614, "end_char_idx": 3910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c88b7851-7cf4-44eb-9963-5953e476fd83": {"__data__": {"id_": "c88b7851-7cf4-44eb-9963-5953e476fd83", "embedding": null, "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b", "node_type": "4", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "c707e29cc2b7e4c47832053a51312798b89bab262556ce615fad2ddf9817d62d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fe92b9f-affa-4523-bf28-4ed02ed7bea2", "node_type": "1", "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}, "hash": "3da68f2b7668084cc3f2cfc29bbe05b9dd1dccec236291613585cf383ae2dfec", "class_name": "RelatedNodeInfo"}}, "text": "# Download Data\n\n\n# load documents\ndocuments = SimpleDirectoryReader(\"differents_textes/moodle/\").load_data()\n\nvector_store = FaissVectorStore(faiss_index=faiss_index)\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(\n    documents, storage_context=storage_context\n)\n\n# save index to disk\nindex.storage_context.persist()\n\n# load index from disk\nvector_store = FaissVectorStore.from_persist_dir(\"./storage\")\nstorage_context = StorageContext.from_defaults(\n    vector_store=vector_store, persist_dir=\"./storage\"\n)\nindex = load_index_from_storage(storage_context=storage_context)\n\n# Query Index\n\n# set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"What did the author do growing up?\")\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\n# set Logging to DEBUG for more detailed outputs\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\n    \"What did the author do after his time at Y Combinator?\"\n)\n\ndisplay(Markdown(f\"<b>{response}</b>\"))\n\"\"\"", "start_char_idx": 3895, "end_char_idx": 4977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "702c4176-4be1-4f05-b802-87ca415965ed": {"__data__": {"id_": "702c4176-4be1-4f05-b802-87ca415965ed", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf", "node_type": "1", "metadata": {}, "hash": "b6b00b840c250487af01797abb2f14da33e52f88943a5b6f1a3e7f3ad546d351", "class_name": "RelatedNodeInfo"}}, "text": "import os\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom PyPDF2 import PdfReader\n\nfrom embedding_models import *\n\n\ndef read_text_from_file(file_path: str) -> str:\n    if file_path.lower().endswith(\".pdf\"):\n        with open(file_path, \"rb\") as f:\n            reader = PdfReader(f)\n            metadata = reader.metadata\n            text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n            return text, metadata\n    elif file_path.lower().endswith(\".txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            return f.read(), {}\n    elif file_path.lower().endswith(\".json\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return json.dumps(json.load(f)), {}\n    else:\n        raise ValueError(\"Unsupported file type. Please upload a .txt or .pdf file.\")\n\n\n# Function to load documents individually\ndef load_documents_from_directory(directory):\n    documents = []\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith((\".txt\", \".pdf\", \".json\")):\n                print(\"Traitement de \", filename)\n                file_path = os.path.join(root, filename)\n                try:\n                    text, metadata = read_text_from_file(file_path)\n                    documents.append(\n                        {\"content\": text, \"source\": file_path, \"metadata\": metadata}\n                    )\n                except ValueError as e:\n                    print(f\"Error processing {file_path}: {e}\")\n    return documents", "start_char_idx": 0, "end_char_idx": 1781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf": {"__data__": {"id_": "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "702c4176-4be1-4f05-b802-87ca415965ed", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "22ad31a809849c19482940f83f82a4243df7d0d7365350f041cb78b6c6e947a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08253c87-cdad-45c9-8fab-c5a2218d3b99", "node_type": "1", "metadata": {}, "hash": "2fa51a36269957ab45141fcc66e741386950502fe4e1dc3a2610b0b600f90d95", "class_name": "RelatedNodeInfo"}}, "text": "# Initialize embeddings\nhf_embeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n)\nollama_embeddings = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=\"nomic-embed-text\",\n    show_progress=\"true\",\n    temperature=0,\n)\ndistilbert_embeddings = HuggingFaceEmbeddings(\n    model_name=\"distilbert-base-nli-stsb-mean-tokens\"\n)\ninstructor_embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-large\")\ncamembert_embeddings = HuggingFaceEmbeddings(\n    model_name=\"dangvantuan/sentence-camembert-base\"\n)\nmpnet_embeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n    model_kwargs={\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": False},\n)\n\n\ndef charge_vectorstore(embedding, index_path):\n    # pour se mettre au niveau du r\u00e9pertoire au dessus contenant les vectorstores\n    index_path = \"../\" + str(index_path)\n    if os.path.exists(index_path):\n        vectorstore = FAISS.load_local(\n            index_path,\n            embeddings=embedding,\n            # allow_dangerous_deserialization=True\n        )\n        print(f\"Index charg\u00e9 \u00e0 partir du chemin existant: {index_path}\")\n    else:\n        documents = load_documents_from_directory(\"../differents_textes\")\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=400, chunk_overlap=150\n        )\n        chunks = []\n        for doc in documents:\n            splits = text_splitter.create_documents(\n                text_splitter.split_text(doc[\"content\"])\n            )\n            for split in splits:\n                split.metadata = {\"source\": doc[\"source\"], **doc.get(\"metadata\", {})}\n                chunks.append(split)\n        vectorstore = FAISS.from_documents(documents=chunks, embedding=embedding)\n        vectorstore.save_local(index_path)\n        print(f\"Nouvel index cr\u00e9\u00e9 et sauvegard\u00e9: {index_path}\")\n    return vectorstore", "start_char_idx": 1784, "end_char_idx": 3712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08253c87-cdad-45c9-8fab-c5a2218d3b99": {"__data__": {"id_": "08253c87-cdad-45c9-8fab-c5a2218d3b99", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "aa1761dffbfab8951817bf1e00d8134525b2d1334489b77c738a581dff3e3c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1159f79-6e7b-4d32-83cd-33cb636925aa", "node_type": "1", "metadata": {}, "hash": "e3809d670f22865d7ec52ece89882f9db1189d40b2dbd94d2e2a42d9cdd4a673", "class_name": "RelatedNodeInfo"}}, "text": "# Function to test retrieval and track usage\ndef test_retrieval(question, path_vectorstore, embedding, model_name):\n    vectorstore = charge_vectorstore(embedding, path_vectorstore)\n    retriever = vectorstore.as_retriever()\n    search_results = retriever.vectorstore.similarity_search(question, k=15)\n\n    # Count usage of each source\n    source_count = {}\n    for result in search_results:\n        source = result.metadata[\"source\"]\n        if source not in source_count:\n            source_count[source] = 0\n        source_count[source] += 1\n\n    context = \"\\n--\\n\".join(\n        [\n            f\"Source: {result.metadata['source']}\\n{result.page_content}\"\n            for result in search_results\n        ]\n    )\n\n    # print(        f\"---------\\nContext for question '{question}' using {model_name}:\\n{context}\"    )\n    print(f\"File usage count for {model_name}:\")\n    for source, count in source_count.items():\n        print(f\"{os.path.basename(source)}: {count} times\")\n\n    return source_count", "start_char_idx": 3715, "end_char_idx": 4716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1159f79-6e7b-4d32-83cd-33cb636925aa": {"__data__": {"id_": "d1159f79-6e7b-4d32-83cd-33cb636925aa", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08253c87-cdad-45c9-8fab-c5a2218d3b99", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "885154db763a1554e4ab1fc772150458fb6b037f9fcf2bd86f1726b4f2525537", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68259f3b-f030-42d1-a0ae-4a738dbbf2a0", "node_type": "1", "metadata": {}, "hash": "5b9c561536fa7f0dd9373924efd4c2f15ca62257b2f602b3227e99da0abea3f2", "class_name": "RelatedNodeInfo"}}, "text": "# Function to plot usage\ndef plot_usage(data, question):\n    plt.figure(figsize=(12, 6))\n    sns.set(style=\"whitegrid\")\n\n    models = list(data.keys())\n    all_sources = set()\n    for model_data in data.values():\n        all_sources.update(model_data.keys())\n\n    all_sources = sorted(all_sources)\n    usage_matrix = []\n    for model in models:\n        model_data = data[model]\n        usage_counts = [model_data.get(source, 0) for source in all_sources]\n        usage_matrix.append(usage_counts)\n\n    usage_matrix = list(zip(*usage_matrix))  # Transpose the matrix\n\n    fig, ax = plt.subplots(figsize=(14, 8))\n    x = range(len(models))\n    for i, source in enumerate(all_sources):\n        ax.bar(x, [usage_matrix[i][j] for j in x], label=os.path.basename(source))\n\n    ax.set_xlabel(\"Models\")\n    ax.set_ylabel(\"Number of times source used\")\n    ax.set_title(f\"Source Usage Comparison for Question: {question}\")\n    ax.set_xticks(x)\n    ax.set_xticklabels(models)\n    ax.legend(title=\"Sources\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\n        os.path.join(\n            \"../screenshot_compare_embedding\", f\"{question.replace(' ', '_')}.png\"\n        )\n    )\n    plt.show()\n\n\n# Example question\n# question = \"Parle moi de ce que Aleister Crowley est all\u00e9 faire en Egypte\"\nquestion = input(\"Posez une question:\")\n# Collect data for all models\nusage_data = {}\n\nprint(\"Using HuggingFace Embeddings (all-MiniLM-L6-v2):\")\nusage_data[\"all-MiniLM-L6-v2\"] = test_retrieval(\n    question,\n    \"vectorstores/HF_vectorstore\",\n    hf_embeddings,\n    \"HuggingFace Embeddings (all-MiniLM-L6-v2)\",\n)\n\nprint(\"\\nUsing Ollama Embeddings:\")\nusage_data[\"nomic-embed-text\"] = test_retrieval(\n    question, index_en_path_OL, ollama_embeddings, \"Ollama Embeddings\"\n)", "start_char_idx": 4719, "end_char_idx": 6489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68259f3b-f030-42d1-a0ae-4a738dbbf2a0": {"__data__": {"id_": "68259f3b-f030-42d1-a0ae-4a738dbbf2a0", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1159f79-6e7b-4d32-83cd-33cb636925aa", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "1cafc5646e286fb09be20ec863f43845ffe15abcccfc8dedb9e83fb7cfd5e150", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c165d12b-54fe-459c-a3ce-0a0a8d62cb64", "node_type": "1", "metadata": {}, "hash": "c0440195569b95f36ac3ff993cf274e926d4520438425cc809f23b5c929ddf1f", "class_name": "RelatedNodeInfo"}}, "text": "print(\"\\nUsing DistilBERT Embeddings (distilbert-base-nli-stsb-mean-tokens):\")\nusage_data[\"distilbert-base-nli-stsb-mean-tokens\"] = test_retrieval(\n    question,\n    \"vectorstores/DistilBERT_vectorstore\",\n    distilbert_embeddings,\n    \"DistilBERT Embeddings (distilbert-base-nli-stsb-mean-tokens)\",\n)\n\nprint(\"\\nUsing Instructor Base Embeddings (hkunlp/instructor-base):\")\nusage_data[\"hkunlp/instructor-base\"] = test_retrieval(\n    question,\n    index_en_path_instructor_base,\n    HuggingFaceEmbeddings(model_name=embedding_model_hf_en_instructor_base),\n    \"Instructor Embeddings (hkunlp/instructor-base)\",\n)\n\"\"\"\nprint(\"\\nUsing Instructor xl Embeddings (hkunlp/instructor-xl):\")\nusage_data[\"hkunlp/instructor-xl\"] = test_retrieval(\n    question,\n    index_en_path_instructor_xl,\n    HuggingFaceEmbeddings(model_name=embedding_model_hf_en_instructor_xl),\n    \"Instructor Embeddings (hkunlp/instructor-xl)\",\n)\n\"\"\"\nprint(\"\\nUsing Instructor Embeddings (hkunlp/instructor-large):\")\nusage_data[\"hkunlp/instructor-large\"] = test_retrieval(\n    question,\n    index_en_path_instructor_large,\n    instructor_embeddings,\n    \"Instructor Embeddings (hkunlp/instructor-large)\",\n)\n\nprint(\"\\nUsing MPNet Embeddings (sentence-transformers/all-mpnet-base-v2):\")\nusage_data[\"sentence-transformers/all-mpnet-base-v2\"] = test_retrieval(\n    question,\n    index_en_path_mpnet,\n    mpnet_embeddings,\n    \"MPNet Embeddings (sentence-transformers/all-mpnet-base-v2)\",\n)\n\nprint(\"\\nUsing Camembert Embeddings (dangvantuan/sentence-camembert-base):\")\nusage_data[\"dangvantuan/sentence-camembert-base\"] = test_retrieval(\n    question,\n    index_fr_path_camembert,\n    mpnet_embeddings,", "start_char_idx": 6492, "end_char_idx": 8150, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c165d12b-54fe-459c-a3ce-0a0a8d62cb64": {"__data__": {"id_": "c165d12b-54fe-459c-a3ce-0a0a8d62cb64", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9269ff6db3d6c8b2382edccfa366259b3e577b7", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "c23dbfd2d966a5771cf61c6b30e2e837130221c7ea491b2d938dbcf28fcc723e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68259f3b-f030-42d1-a0ae-4a738dbbf2a0", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}, "hash": "ff9a4d510e58b0b5560e302c34da00b293915ec03e9834753d8f5bcbc119ca24", "class_name": "RelatedNodeInfo"}}, "text": "index_fr_path_camembert,\n    mpnet_embeddings,\n    \"Camembert Embeddings (dangvantuan/sentence-camembert-base)\",\n)\n\n# Plot usage data\nplot_usage(usage_data, question)", "start_char_idx": 8104, "end_char_idx": 8270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a89dd7c2-f147-4122-9799-0bc050ef67e9": {"__data__": {"id_": "a89dd7c2-f147-4122-9799-0bc050ef67e9", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/embedding_models.py", "file_name": "embedding_models.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/embedding_models.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da03217c2e9e57bb553c35fa72450cf6f482e29b", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/embedding_models.py", "file_name": "embedding_models.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/embedding_models.py"}, "hash": "e88f454355aa09ddf7a0262a839d847c7f74f440db34659ad72318926a23cee9", "class_name": "RelatedNodeInfo"}}, "text": "# Configuration\nembedding_model_hf_en_mpnet = \"sentence-transformers/all-mpnet-base-v2\"\nindex_en_path_mpnet = \"vectorstores/MPNet_vectorstore\"\n\nembedding_model_hf_fr = \"dangvantuan/sentence-camembert-base\"\nindex_fr_path_camembert = \"vectorstores/Camembert_vectorstore\"\n\nembedding_model_hf_en_instructor_large = \"hkunlp/instructor-large\"\nindex_en_path_instructor_large = \"vectorstores/Instructor_large_vectorstore\"\n\nembedding_model_hf_en_instructor_base = \"hkunlp/instructor-base\"\nindex_en_path_instructor_base = \"vectorstores/Instructor_base_vectorstore\"\n\nembedding_model_hf_en_instructor_xl = \"hkunlp/instructor-xl\"\nindex_en_path_instructor_xl = \"vectorstores/Instructor_xl_vectorstore\"\n\nembedding_model_ol_en_nomic = \"nomic-embed-text\"\nindex_en_path_OL = \"vectorstores/OL_vectorstore\"  # nomic-embed", "start_char_idx": 0, "end_char_idx": 801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69d0505d-88d9-4bdd-af2a-83241b7a199a": {"__data__": {"id_": "69d0505d-88d9-4bdd-af2a-83241b7a199a", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "hash": "c5bd11955953c9e7a57f47ca208446210bc90b51860314e7843a293fab41380b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d455a7a-e2f5-4a7f-b258-e66708d8552f", "node_type": "1", "metadata": {}, "hash": "c43dcd0f5b078a12854948ae3c779d01b6e7970c44f3bee7173526818aaa222c", "class_name": "RelatedNodeInfo"}}, "text": "from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom typing import List, Dict\nimport os\nimport json\nimport requests\nimport tempfile\nimport chainlit as cl\nfrom langchain.text_splitter import (\n    RecursiveCharacterTextSplitter,\n)\nfrom langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\nfrom langchain_community.llms import Ollama\nfrom langchain_community.vectorstores import FAISS\n\nfrom PyPDF2 import PdfReader\nfrom typing import List\n\nfrom urllib import request\nfrom utils.embedding_models import *\n\n\nmodel = Ollama(base_url=\"http://localhost:11434\", model=\"llama3:instruct\")\n\n\nembeddings_OL = OllamaEmbeddings(\n    base_url=\"http://localhost:11434\",\n    model=embedding_model_ol_en_nomic,\n    show_progress=\"true\",\n    temperature=0,\n)\n\n# on d\u00e9cide ici quel index et quel mod\u00e8le utiliser\nembeddings = HuggingFaceEmbeddings(\n    model_name=embedding_model_hf_en_instructor_large)\nindex_path = index_en_path_instructor_large\nfaiss_index = None\n\n\ndef add_files_to_index(index_path, embeddings, file_path):\n    \"\"\"\n    Pour ajouter un fichier, et non pas cr\u00e9er l'index\n    \"\"\"\n    if not os.path.exists(index_path):\n        raise ValueError(\n            f\"L'index {index_path} n'existe pas. Veuillez le cr\u00e9er avant d'ajouter de nouveaux documents.\")\n\n    # Charger l'index existant\n    vectorstore = FAISS.load_local(\n        index_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n\n    # Initialiser le text splitter\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=300, chunk_overlap=150)\n\n    text, metadata = read_text_from_file(file_path)\n    document = Document(page_content=text, metadata=metadata)\n    chunks = text_splitter.split_documents([document])\n    vectorstore.add_documents(chunks)\n\n    vectorstore.save_local(index_path)\n    print(\"Les nouveaux documents ont \u00e9t\u00e9 ajout\u00e9s \u00e0 l'index et l'index a \u00e9t\u00e9 sauvegard\u00e9.\")", "start_char_idx": 0, "end_char_idx": 1961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d455a7a-e2f5-4a7f-b258-e66708d8552f": {"__data__": {"id_": "2d455a7a-e2f5-4a7f-b258-e66708d8552f", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "hash": "c5bd11955953c9e7a57f47ca208446210bc90b51860314e7843a293fab41380b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69d0505d-88d9-4bdd-af2a-83241b7a199a", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "hash": "99396863aa8ee795e8d443bdfe7690409dfeaca79dba8454d425ea0769c933d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6546d46-2cf5-4207-8761-523a1edf02ce", "node_type": "1", "metadata": {}, "hash": "135cd070ae6cc9aaada6607e420519968afd8389bcedadf70325d4c071e100d3", "class_name": "RelatedNodeInfo"}}, "text": "# utilis\u00e9\ndef load_new_documents(directory: str) -> List[Document]:\n\n    # Initialize CharacterTextSplitter\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=300, chunk_overlap=150\n    )\n\n    # Split each document into chunks\n    chunks = []\n\n    documents = load_documents_from_directory(directory=directory)\n\n    for doc in documents:\n        splits = text_splitter.create_documents(\n            text_splitter.split_text(doc[\"content\"])\n        )\n        for split in splits:\n            split.metadata = {\n                \"source\": doc[\"source\"], **doc.get(\"metadata\", {})}\n            chunks.append(split)\n    return chunks\n\n\ndef read_text_from_file(file_path: str) -> str:\n    \"\"\"Function to read PDF and return text\"\"\"\n\n    if file_path.lower().endswith(\".pdf\"):\n        with open(file_path, \"rb\") as f:\n            reader = PdfReader(f)\n            metadata = reader.metadata\n            text = \"\\n\".join(page.extract_text()\n                             or \"\" for page in reader.pages)\n            return text, metadata\n    elif file_path.lower().endswith(\".txt\"):\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n            return f.read(), {}\n    elif file_path.lower().endswith(\".json\"):\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            return json.dumps(json.load(f)), {}\n    else:\n        raise ValueError(\n            \"Unsupported file type. Please upload a .txt or .pdf file.\")\n\n\n# Function to load documents individually\n\n\ndef load_documents_from_directory(directory):\n    documents = []\n    for root, _, files in os.walk(directory):\n        for filename in files:\n            if filename.lower().endswith((\".txt\", \".pdf\", \".json\")):\n                print(\"Traitement de \", filename)\n                file_path = os.path.join(root, filename)\n                try:\n                    text, metadata = read_text_from_file(file_path)\n                    documents.append(\n                        {\"content\": text, \"source\": file_path, \"metadata\": metadata}\n                    )\n                except ValueError as e:\n                    print(f\"Error processing {file_path}: {e}\")\n    return documents", "start_char_idx": 1964, "end_char_idx": 4148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6546d46-2cf5-4207-8761-523a1edf02ce": {"__data__": {"id_": "a6546d46-2cf5-4207-8761-523a1edf02ce", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5c31d65acf25a33c1d33105d9eb721f941d1c16", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "hash": "c5bd11955953c9e7a57f47ca208446210bc90b51860314e7843a293fab41380b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d455a7a-e2f5-4a7f-b258-e66708d8552f", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}, "hash": "b497e330e500810c60c3b63ccba93ba3ba92ad7b01f36e5f76195f46e4b54008", "class_name": "RelatedNodeInfo"}}, "text": "def change_model(new_model):\n    # on change de mod\u00e8le d'embedding pour en prendre un plus adapt\u00e9\n\n    if new_model == \"instructor-large\":\n        embeddings_HF = HuggingFaceEmbeddings(\n            model_name=embedding_model_hf_en_instructor_large\n        )\n        index_path = index_en_path_instructor_large\n\n    elif new_model == \"instructor-xl\":\n        embeddings_HF = HuggingFaceEmbeddings(\n            model_name=embedding_model_hf_en_instructor_xl\n        )\n        index_path = index_en_path_instructor_xl\n\n    elif new_model == \"instructor-base\":\n        embeddings_HF = HuggingFaceEmbeddings(\n            model_name=embedding_model_hf_en_instructor_base\n        )\n        index_path = index_en_path_instructor_base\n\n    elif new_model == \"mpnet-v2\":\n        embeddings_HF = HuggingFaceEmbeddings(\n            model_name=embedding_model_hf_en_mpnet)\n        index_path = index_en_path_mpnet\n\n    elif new_model == \"camembert-base\":\n        embeddings_HF = HuggingFaceEmbeddings(model_name=embedding_model_hf_fr)\n        index_path = index_fr_path_camembert\n\n    return embeddings_HF, index_path\n\n\n\"\"\"\n\ndef add_documents_to_index(vectorstore, new_documents):\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=400, chunk_overlap=150)\n    new_docs = []\n\n    for doc in new_documents:\n        chunks = text_splitter.create_documents(doc)\n        new_docs.extend(chunks)\n\n    vectorstore.add_documents(new_docs)\n    vectorstore.save_local(index_path)\n\ndef add_documents(directory: str):\n    retriever = cl.user_session.get(\"retriever\")\n    vectorstore = retriever.vectorstore\n\n    add_documents_to_index(vectorstore, directory)\n\n    vectorstore.save_local(index_path)\n    print(\"Nouveaux documents ajout\u00e9s et index mis \u00e0 jour.\")\n\"\"\"", "start_char_idx": 4151, "end_char_idx": 5909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d82dab27-aec3-41d3-af4d-806388e449de": {"__data__": {"id_": "d82dab27-aec3-41d3-af4d-806388e449de", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/modify_pdf_metadata.py", "file_name": "modify_pdf_metadata.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/modify_pdf_metadata.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2b76af6b9f0f9ff5150fb180e4d5bd4b467c6c3", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/modify_pdf_metadata.py", "file_name": "modify_pdf_metadata.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/modify_pdf_metadata.py"}, "hash": "ea854861d57e674d207974580f04c8ae6c9a849d769d26fca662136fcadb3875", "class_name": "RelatedNodeInfo"}}, "text": "from PyPDF2 import PdfReader\nfrom spire.pdf import *\nfrom spire.pdf.common import *\n\n# pip install Spire.pdf\n\n\ndef extract_metadata(pdf_path):\n    with open(pdf_path, \"rb\") as f:\n        reader = PdfReader(f)\n        metadata = reader.metadata\n        print(str(metadata) + \"\\n\")\n\n\ndef modify_metadata_Peter_Pan(pdf_path):\n    doc = PdfDocument()\n    doc.LoadFromFile(pdf_path)\n\n    information = doc.DocumentInformation\n\n    information.Author = \"J. M. Barrie\"\n    information.Keywords = \"Peter-Pan, book, story, children, adventure, tale\"\n    information.Subject = \"Story of Peter-Pan\"\n    information.Title = \"Peter-Pan\"\n    # information.SetCustomProperty(\"Telephone\", \"845-3-111\")\n\n    doc.SaveToFile(pdf_path)\n    doc.Close()\n\n    extract_metadata(pdf_path)\n\n\ndef modify_metadata_Walk_Egyptian(pdf_path):\n    doc = PdfDocument()\n    doc.LoadFromFile(pdf_path)\n\n    information = doc.DocumentInformation\n\n    information.Author = \"Caroline Tully\"\n    information.Keywords = \"Egypt, occult, Aleister Crowley, Book of The Law, Article, Florence Farr, Rose Kelly, Samuel Mathers\"\n    information.Subject = \"article investigates the story of Aleister Crowley\u2019s reception of  The  Book  of  the  Law  in \\\n    Cairo, Egypt, in 1904, focusing on the question of why it occurred in Egypt\"\n    information.Title = \"Walk like an Egyptian:\\\n    Egypt as authority in Aleister Crowley\u2019s reception of The Book of the Law \"\n    # information.SetCustomProperty(\"Telephone\", \"845-3-111\")\n\n    doc.SaveToFile(pdf_path)\n    doc.Close()\n\n    extract_metadata(pdf_path)\n\n\n\"\"\"\nmodify_metadata_Peter_Pan(\"differents_textes/peter-pan.pdf\")\nextract_metadata(\"differents_textes/Dumas_Les_trois_mousquetaires_1.pdf\")\nmodify_metadata_Walk_Egyptian(\n    \"differents_textes/Tully Walk like an Egyptian .pdf\")\n\"\"\"", "start_char_idx": 0, "end_char_idx": 1789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5640608b-e5be-4a2f-955f-08c83de5fa4e": {"__data__": {"id_": "5640608b-e5be-4a2f-955f-08c83de5fa4e", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94492d9bcbabe018649c059764d995c6dca54ac3", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "a53d27e4e4733b2a7ad25119f35db91a8a750cf972dffcf5a3c8f0475c74265b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34a79c52-1c4b-41b4-93aa-8ea0323966aa", "node_type": "1", "metadata": {}, "hash": "33a473c29ec395f0ed8c2221c6541338e0d4e689289621bc15cf085e076ae2db", "class_name": "RelatedNodeInfo"}}, "text": "from selenium import webdriver\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom bs4 import BeautifulSoup\nfrom langchain.docstore.document import Document\n\nimport os\nimport time\nfrom dotenv import load_dotenv\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef authenticate_firefox(driver, login_url):\n    \"\"\"\n    Authenticate with the website using Firefox.\n    \"\"\"\n    parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n    env_path = os.path.join(parent_dir, '.env')\n    load_dotenv(dotenv_path=env_path)\n\n    username = os.getenv(\"USERNAME_CAS\")\n    password = os.getenv(\"PASSWORD_CAS\")\n\n    driver.get(login_url)\n    username_input = driver.find_element(\n        By.ID, \"username\")\n    password_input = driver.find_element(\n        By.ID, \"password\")\n    submit_button = driver.find_element(\n        By.NAME, \"submit\")\n\n    username_input.send_keys(username)\n    password_input.send_keys(password)\n    submit_button.click()\n\n\ndef extract_moodle_links(driver):\n    \"\"\"\n    Extracts the links with the \"aalink stretched-link\" class from a Moodle page.\n    \"\"\"\n    links = []\n    pdf_files = []\n\n    for link in driver.find_elements(By.CSS_SELECTOR, \"a.aalink.stretched-link\"):\n        href = link.get_attribute(\"href\")\n        if href.endswith(\".pdf\"):\n            print(f\"{href} a \u00e9t\u00e9 ajout\u00e9\")\n        else:\n            links.append(href)\n\n    return links\n\n\ndef recupere_liste_cours(driver):\n    liste_cours = []\n    wait = WebDriverWait(driver, 6)  # Attendre jusqu'\u00e0 10 secondes\n    elements = wait.until(EC.presence_of_all_elements_located(\n        (By.CSS_SELECTOR, \"a.aalink.coursename.mr-2.mb-1\")))\n    for link in elements:\n        href = link.get_attribute(\"href\")\n        print(f\"{href} ajout\u00e9\")\n        liste_cours.append(href)\n\n    print(liste_cours)\n    return liste_cours\n\n\ndef prepare_options():\n    options = FirefoxOptions()\n    options.headless = True\n\n    options.", "start_char_idx": 0, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34a79c52-1c4b-41b4-93aa-8ea0323966aa": {"__data__": {"id_": "34a79c52-1c4b-41b4-93aa-8ea0323966aa", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94492d9bcbabe018649c059764d995c6dca54ac3", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "a53d27e4e4733b2a7ad25119f35db91a8a750cf972dffcf5a3c8f0475c74265b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5640608b-e5be-4a2f-955f-08c83de5fa4e", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "37b339c875ff8c7ddd76748ffcba22f3e3230fa53b58c089c8cfd752786ba4e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d", "node_type": "1", "metadata": {}, "hash": "c46d3341030bca165c72ce28d1fc90bdc108efb408cfb0e3b23c58fbc599a05d", "class_name": "RelatedNodeInfo"}}, "text": "headless = True\n\n    options.set_preference(\"browser.download.folderList\", 2)\n    options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n    options.set_preference(\"browser.download.manager.focusWhenStarting\", False)\n    options.set_preference(\"browser.download.alwaysOpenPanel\", False)\n    options.set_preference(\"browser.download.animateNotifications\", False)\n    options.set_preference(\"browser.download.panel.shown\", False)\n    options.set_preference(\"browser.download.manager.useWindow\", False)\n    options.set_preference(\"dom.webnotifications.enabled\", False)\n    options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\",\n                           \"application/msword, application/csv, application/ris, text/csv, image/png, application/pdf, text/html, text/plain, application/zip, application/x-zip, application/x-zip-compressed, application/download, application/octet-stream\")\n    options.set_preference(\"browser.download.manager.alertOnEXEOpen\", False)\n    options.set_preference(\"browser.download.useDownloadDir\", True)\n    options.set_preference(\"browser.helperApps.alwaysAsk.force\", False)\n    options.set_preference(\"browser.download.manager.closeWhenDone\", True)\n    options.set_preference(\"browser.download.animateNotifications\", False)\n\n    options.set_preference(\n        \"browser.download.manager.showAlertOnComplete\", False)\n    options.set_preference(\n        \"services.sync.prefs.sync.browser.download.manager.showWhenStarting\", False)\n    options.set_preference(\n        \"browser.download.dir\", \"/home/UHA/e2303253/ProjetLLM/sandboxRAG/differents_textes/moodle\")\n    # Example:options.set_preference(\"browser.download.dir\", \"C:\\Tutorial\\down\")\n    mime_types = [\n        'text/plain',\n        'application/vnd.ms-excel',\n        'text/csv',\n        'application/csv',\n        'text/comma-separated-values',\n        'application/download',\n        'application/octet-stream',\n        'binary/octet-stream',", "start_char_idx": 2078, "end_char_idx": 4033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d": {"__data__": {"id_": "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94492d9bcbabe018649c059764d995c6dca54ac3", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "a53d27e4e4733b2a7ad25119f35db91a8a750cf972dffcf5a3c8f0475c74265b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34a79c52-1c4b-41b4-93aa-8ea0323966aa", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "6689a241f9f4de7ab9b5d4b11166d7964c2a156d51f1ab85b74335cafd297cc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "573dd2de-c2aa-4dae-83ec-4f26cb164093", "node_type": "1", "metadata": {}, "hash": "2073bfe80b1c8b3cd4194efee3737933d0d3b3e20b535730a3f88706e048654e", "class_name": "RelatedNodeInfo"}}, "text": "'application/download',\n        'application/octet-stream',\n        'binary/octet-stream',\n        'application/binary',\n        'application/x-unknown',\n        'application/octet-stream',\n        'application/pdf',\n        'application/x-pdf'\n    ]\n    options.set_preference(\n        \"browser.helperApps.neverAsk.saveToDisk\", \",\".join(mime_types))\n    # Disable built-in PDF viewer\n    options.set_preference(\"pdfjs.disabled\", True)\n\n    return options\n\n\ndef load_web_documents_firefox(urls, login_url=None):\n    \"\"\"\n    Fetch and parse the HTML content from a list of URLs using Firefox.\n    \"\"\"\n    documents = {\n        \"web_result\": [],\n        \"pdf_to_read\": []\n    }\n\n    with webdriver.Firefox(options=prepare_options()) as driver:\n        if login_url is not None:\n            authenticate_firefox(driver, login_url)\n\n            # page post-connexion\n            driver.get(\n                \"https://e-services.uha.fr/_authenticate?requestedURL=/index.html\")\n            driver.execute_script(\n                \"window.addEventListener('load', function() { Notification.requestPermission(permission => { if (permission === 'granted') { Notification.permission = 'default'; } }); }, false);\")\n\n        for url in urls:\n            try:\n                # print(f\"Lien ouvert : {url['url']}\")\n                if url[\"url\"].endswith(\".pdf\") or \"/mod/resource/view.php\" in url[\"url\"]:\n                    documents[\"pdf_to_read\"].append(url[\"url\"])\n                    continue\n                if (url[\"type\"] == \"webpage_from_moodle\"):\n                    # pour les liens menant vers du web depuis moodle\n                    driver.get(url[\"url\"]+\"&redirect=1\")\n                else:\n                    driver.get(url[\"url\"])  # pour le reste\n\n                if (url[\"type\"] == \"accueil_moodle\"):\n                    print(\"accueil_moodle fonctionne bien\")\n                    liste_cours = recupere_liste_cours(driver)\n                    for cours in liste_cours:\n                        urls.append(\n                            {\"url\": cours,", "start_char_idx": 3943, "end_char_idx": 5998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "573dd2de-c2aa-4dae-83ec-4f26cb164093": {"__data__": {"id_": "573dd2de-c2aa-4dae-83ec-4f26cb164093", "embedding": null, "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94492d9bcbabe018649c059764d995c6dca54ac3", "node_type": "4", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "a53d27e4e4733b2a7ad25119f35db91a8a750cf972dffcf5a3c8f0475c74265b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d", "node_type": "1", "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}, "hash": "b7c1ee9e90b47a16bdfbb0264801f1a9d7b04fb4994f072c95ecce0f32cfc4da", "class_name": "RelatedNodeInfo"}}, "text": "append(\n                            {\"url\": cours, \"type\": \"cours_moodle\"})\n\n                if (url[\"type\"] == \"cours_moodle\"):\n                    # r\u00e9cup\u00e9rer les liens et pdf de la page moodle ici\n                    links = extract_moodle_links(driver)\n\n                    for link in links:\n                        urls.append(\n                            {\"url\": link, \"type\": \"webpage_from_moodle\"})\n                        # on met les liens r\u00e9cup\u00e9r\u00e9s dans la liste\n                        # pour les r\u00e9cup\u00e9rer\n                time.sleep(1)\n                html_content = driver.page_source\n\n                soup = BeautifulSoup(html_content, 'html.parser')\n                text = soup.get_text()\n                doc = Document(page_content=text, metadata={\n                               \"source\": url[\"url\"]})\n                documents[\"web_result\"].append(doc)\n            except Exception as e:\n                print(f\"Error fetching {url['url']}: {e}\")\n\n        for pdf_url in documents[\"pdf_to_read\"]:\n            driver.get(pdf_url)\n\n            time.sleep(1)\n            # driver.switch_to.window(driver.window_handles[0])\n\n            driver.execute_script(\n                \"window.setTimeout(function(){ window.location.reload(); }, 1000);\")\n\n    print(\"R\u00e9cup\u00e9ration web termin\u00e9e\")\n    return documents", "start_char_idx": 5948, "end_char_idx": 7269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "132cfefd-6bd0-41d2-95e7-9abb3536e098": {"__data__": {"id_": "132cfefd-6bd0-41d2-95e7-9abb3536e098", "embedding": null, "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d", "node_type": "4", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "d15abc23027909534ccfc2169c5606a8e9350a83c590a976c1ed4436891efe07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06c15a9b-114e-42e2-8aff-cd010d2ed4e3", "node_type": "1", "metadata": {}, "hash": "c2cda2059660c7efa784c9ed59c282297471ed8400176a8770dfda1a57995bb6", "class_name": "RelatedNodeInfo"}}, "text": "from chainlit.input_widget import TextInput, Select, Switch, Slider\nfrom typing import Optional\nfrom operator import itemgetter\nfrom langchain_community.llms import Ollama\n\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.schema.runnable import Runnable, RunnablePassthrough, RunnableLambda\nfrom langchain.schema.runnable.config import RunnableConfig\nfrom langchain.memory import ConversationBufferMemory\n\nfrom chainlit.types import ThreadDict\nimport chainlit as cl\n\n\ndef setup_runnable():\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n    model = Ollama(model=\"llama3:8b\")\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", \"You are a helpful chatbot\"),\n            MessagesPlaceholder(variable_name=\"history\"),\n            (\"human\", \"{question}\"),\n        ]\n    )\n\n    runnable = (\n        RunnablePassthrough.assign(\n            history=RunnableLambda(\n                memory.load_memory_variables) | itemgetter(\"history\")\n        )\n        | prompt\n        | model\n        | StrOutputParser()\n    )\n    cl.user_session.set(\"runnable\", runnable)\n\n\n\"\"\"\n@cl.password_auth_callback\ndef auth():\n    return cl.User(identifier=\"test\")\n\"\"\"\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    cl.user_session.set(\n        \"memory\", ConversationBufferMemory(return_messages=True))\n    setup_runnable()\n\n\n@cl.on_chat_resume\nasync def on_chat_resume(thread: ThreadDict):\n    memory = ConversationBufferMemory(return_messages=True)\n    root_messages = [m for m in thread[\"steps\"] if m[\"parentId\"] == None]\n    for message in root_messages:\n        if message[\"type\"] == \"user_message\":\n            memory.chat_memory.add_user_message(message[\"output\"])\n        else:\n            memory.chat_memory.add_ai_message(message[\"output\"])\n\n    cl.user_session.set(\"memory\", memory)\n\n    setup_runnable()", "start_char_idx": 0, "end_char_idx": 1996, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06c15a9b-114e-42e2-8aff-cd010d2ed4e3": {"__data__": {"id_": "06c15a9b-114e-42e2-8aff-cd010d2ed4e3", "embedding": null, "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d", "node_type": "4", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "d15abc23027909534ccfc2169c5606a8e9350a83c590a976c1ed4436891efe07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "132cfefd-6bd0-41d2-95e7-9abb3536e098", "node_type": "1", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "82d055b2fda16f28542789bc12349996ddcce5ac302b119d2f96e535c68da48a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66b9dbd2-6d08-4776-8002-5a9dae4f54e3", "node_type": "1", "metadata": {}, "hash": "f344043efda61271ca0b33831c7de361a354ec612ed5365dc7f076fec61b3135", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_message\nasync def on_message(message: cl.Message):\n    memory = cl.user_session.get(\"memory\")  # type: ConversationBufferMemory\n\n    runnable = cl.user_session.get(\"runnable\")  # type: Runnable\n\n    res = cl.Message(content=\"\")\n\n    async for chunk in runnable.astream(\n        {\"question\": message.content},\n        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n    ):\n        await res.stream_token(chunk)\n\n    await res.send()\n\n    memory.chat_memory.add_user_message(message.content)\n    memory.chat_memory.add_ai_message(res.content)\n\n\n@cl.set_chat_profiles\nasync def chat_profile():\n    return [\n        cl.ChatProfile(\n            name=\"GPT-3.5\",\n            markdown_description=\"The underlying LLM model is **GPT-3.5**.\",\n            icon=\"https://picsum.photos/200\",\n        ),\n        cl.ChatProfile(\n            name=\"GPT-4\",\n            markdown_description=\"The underlying LLM model is **GPT-4**.\",\n            icon=\"https://picsum.photos/250\",\n        ),\n    ]\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    chat_profile = cl.user_session.get(\"chat_profile\")\n    await cl.Message(\n        content=f\"starting chat using the {chat_profile} chat profile\"\n    ).send()\n\n\n@cl.on_chat_start\nasync def on_chat_start():\n    cl.user_session.set(\"chat_history\", [])\n    cl.user_session.set(\"chat_history\", [{\"role\": \"system\",\n                        \"content\": \"behave as if you are news reporter.\"}])", "start_char_idx": 1999, "end_char_idx": 3436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66b9dbd2-6d08-4776-8002-5a9dae4f54e3": {"__data__": {"id_": "66b9dbd2-6d08-4776-8002-5a9dae4f54e3", "embedding": null, "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d", "node_type": "4", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "d15abc23027909534ccfc2169c5606a8e9350a83c590a976c1ed4436891efe07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06c15a9b-114e-42e2-8aff-cd010d2ed4e3", "node_type": "1", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "f5b6a55014dec942fce93c6af501149d16e293516657709dc0ea167e33324c67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8716fb21-57fa-4e2b-afd1-eb867ae9a550", "node_type": "1", "metadata": {}, "hash": "442453d867679485fbe2a7c7ae2e1f7b87029e880eafd5c92f6320ebf223ca29", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_message\nasync def generate_response(query: cl.Message):\n    chat_history = cl.user_session.get(\"chat_history\")\n    chat_history.append({\"role\": \"user\", \"content\": query.content})\n\n    response = cl.Message(content=\"\")\n    answer = 'ollama.chat(model=\"llama3:8b\", messages=chat_history, stream=True)'\n\n    complete_answer = \"\"\n    for token_dict in answer:\n        token = token_dict[\"message\"][\"content\"]\n        complete_answer += token\n        await response.stream_token(token)\n\n    chat_history.append({\"role\": \"assistant\", \"content\": complete_answer})\n    cl.user_session.set(\"chat_history\", chat_history)\n\n    await response.send()", "start_char_idx": 3439, "end_char_idx": 4083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8716fb21-57fa-4e2b-afd1-eb867ae9a550": {"__data__": {"id_": "8716fb21-57fa-4e2b-afd1-eb867ae9a550", "embedding": null, "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8587ec0bf9ca4a54ac448433f70b323f08a400d", "node_type": "4", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "d15abc23027909534ccfc2169c5606a8e9350a83c590a976c1ed4436891efe07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66b9dbd2-6d08-4776-8002-5a9dae4f54e3", "node_type": "1", "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}, "hash": "72ee1770cdfdfced228c71782480b7ce3e5bedc3ca028aa5e2d4a4b8ba1911c8", "class_name": "RelatedNodeInfo"}}, "text": "@cl.on_chat_start\nasync def start():\n    settings = await cl.ChatSettings(\n        [\n            TextInput(id=\"AgentName\", label=\"Agent Name\", initial=\"AI\"),\n            Select(\n                id=\"Model\",\n                label=\"OpenAI - Model\",\n                values=[\"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\",\n                        \"gpt-4\", \"gpt-4-32k\"],\n                initial_index=0,\n            ),\n            Switch(id=\"Streaming\", label=\"OpenAI - Stream Tokens\", initial=True),\n            Slider(\n                id=\"SAI_Height\",\n                label=\"Stability AI - Image Height\",\n                initial=512,\n                min=256,\n                max=2048,\n                step=64,\n                tooltip=\"Measured in pixels\",\n            ),\n        ]\n    ).send()\n\n    res = await cl.AskActionMessage(\n        content=\"Pick an action!\",\n        actions=[\n            cl.Action(name=\"continue\", value=\"continue\", label=\"\u2705 Continue\"),\n            cl.Action(name=\"cancel\", value=\"cancel\", label=\"\u274c Cancel\"),\n        ],\n    ).send()\n\n    if res and res.get(\"value\") == \"continue\":\n        await cl.Message(\n            content=\"Continue!\",\n        ).send()\n\n    res = await cl.AskUserMessage(content=\"What is your name?\", timeout=10).send()\n    if res:\n        await cl.Message(\n            content=f\"Your name is: {res['output']}\",\n        ).send()\n    text_content = \"Hello, this is a text element.\"\n    elements = [\n        cl.Text(name=\"simple_text\", content=text_content, display=\"inline\")\n    ]\n\n    await cl.Message(\n        content=\"Check out this text element!\",\n        elements=elements,\n    ).send()\n\n\n@cl.on_settings_update\nasync def setup_agent(settings):\n    print(\"on_settings_update\", settings)", "start_char_idx": 4086, "end_char_idx": 5809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"449df74210f7139bab3a667da805cbe995689b70": {"node_ids": ["e5cfb31c-5236-468e-bfcc-e8f61a7d70d7", "e170b2de-aa38-48b5-a15a-0544cb14ed5a"], "metadata": {"file_path": "HF-llama3-Instruct.py", "file_name": "HF-llama3-Instruct.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/HF-llama3-Instruct.py"}}, "bd4e88c9310cf38842a3791cf351890b7028c06e": {"node_ids": ["95d8c729-f596-470d-a1ff-f4e1ff3c3ff3"], "metadata": {"file_path": "OLLAMA_CHAT.MD", "file_name": "OLLAMA_CHAT.MD", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/OLLAMA_CHAT.MD"}}, "682b5f9b2cc0b4225e20e8312ccb41c21a07343e": {"node_ids": ["6044ec8a-e179-4313-b5b4-6cec70c8671b"], "metadata": {"file_path": "Ollama_chat.py", "file_name": "Ollama_chat.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat.py"}}, "c1cc21c73e8df2591264b7e1fe4b0f19ad41618f": {"node_ids": ["12336feb-25eb-4296-a182-060ed3eda05b"], "metadata": {"file_path": "Ollama_chat_history.py", "file_name": "Ollama_chat_history.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/Ollama_chat_history.py"}}, "fa7ad47f2921567bc51f1826c13278586e8960a7": {"node_ids": ["5144edd0-f473-49ec-8901-a134b9c9b93b", "d92716bf-d0d9-491c-a9eb-0ba2a0412d37", "eb17f6d0-566e-41d7-a415-95ecbe17d24c"], "metadata": {"file_path": "PR_Reviewer/PR_reviewer_github.py", "file_name": "PR_reviewer_github.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/PR_reviewer_github.py"}}, "be741491bb050d86e011369fa21d499f748cb82c": {"node_ids": ["1d5fdc24-4ab2-42c3-a4f3-87cc51359744", "9675bbca-c254-4679-9e54-7cee7471e43d"], "metadata": {"file_path": "PR_Reviewer/github_recup.py", "file_name": "github_recup.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/github_recup.py"}}, "fef500d8481bfcad975021a7b91ff340b84b34e0": {"node_ids": ["52bda799-0661-40a4-8d8e-2e00db6cd267"], "metadata": {"file_path": "PR_Reviewer/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/PR_Reviewer/requirements.txt"}}, "4507ac4676a6387c4b52a0d1111e94753a102b32": {"node_ids": ["3ad1b863-e59a-4d2d-b9d7-e64865f5fd5c", "e41d42f9-f77c-45b1-95e4-cf84f9bed0cb", "1d659236-9c98-493f-a887-1bab441715d4", "6d53c00f-6846-488d-92d6-609e35f935e4"], "metadata": {"file_path": "sandboxRAG/chainlit.md", "file_name": "chainlit.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/chainlit.md"}}, "e59c940584cc87ebdfe475beed038fabc7490f6f": {"node_ids": ["0845d479-118d-4f16-8175-5775b61a433f"], "metadata": {"file_path": "mailAssistantByPrompts/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/README.md"}}, "96255605229db00f87066d54854a2946f36be5c2": {"node_ids": ["5b11d6ce-59cb-4428-b0b3-6fefb138fd21", "eb10b6c8-ea71-427c-a6ab-5e090212ec10", "173af6aa-4d89-45c9-9c1f-4169ea8ba8d4"], "metadata": {"file_path": "mailAssistantByPrompts/main.py", "file_name": "main.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/main.py"}}, "f50a824f030689bc98d3c9afb1374a9facc0d041": {"node_ids": ["7387b719-1c3b-4d15-b0be-c8042b07e189"], "metadata": {"file_path": "mailAssistantByPrompts/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/mailAssistantByPrompts/requirements.txt"}}, "89f4ccb8b518fb2addd6968f34e02f7e33086f55": {"node_ids": ["5ced1dae-b900-4fa0-a219-a0266e2fcde1", "2f6806bd-0369-4dc6-889f-87537ab4e57e", "009b1bc3-d665-4a69-94ba-b3deda024b8d"], "metadata": {"file_path": "modele_personnalise/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/README.md"}}, "0a68efd81dbbace8e4671e4e048c6b81ebfeb67b": {"node_ids": ["20b1145a-267e-4b48-ada5-bebc2e453add", "4fbc4ab5-4e02-47fc-a691-4ff5ae5faad2", "434447fb-16a4-413b-a73b-6979b5f28d3d"], "metadata": {"file_path": "modele_personnalise/exo_math_2prompts.py", "file_name": "exo_math_2prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_2prompts.py"}}, "0e7e54b1c48f3f958e3f98205f90fc1ceb8a26b0": {"node_ids": ["f2e2b4be-05bd-4a3f-84db-6cbcb963d58a", "668702ac-9686-41e6-a060-d49ecdb511a6", "8a20f4ab-e30a-4bef-bd32-ee56f9899299"], "metadata": {"file_path": "modele_personnalise/exo_math_3prompts.py", "file_name": "exo_math_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/exo_math_3prompts.py"}}, "9efa71524a0ed4c2d73cbd0de801185f55016f48": {"node_ids": ["cf6b6221-b673-4f11-a2a4-4ce633f7e050", "f4403db4-6635-400e-bfe9-65a29304b667", "99e19d07-c7e1-45c0-a54f-1e1797ca4faa", "a2fc86b9-9bae-4884-8a68-92756ccbdbb1", "2e649ee5-5f73-4c32-a500-0db71926e9b1", "bba38736-3c6e-4460-846e-9105ddf2cf6c"], "metadata": {"file_path": "modele_personnalise/prep_message_chainlit_3prompts.py", "file_name": "prep_message_chainlit_3prompts.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/prep_message_chainlit_3prompts.py"}}, "7cee7ecffbc822f47db8d5a6352b1a81af1eed5b": {"node_ids": ["f2f45d7c-1b2f-4af0-a8f7-f6d0290f66dc", "b81375ed-6b04-47a5-8802-ce2d700c0c8b", "03911321-4ada-498f-89c6-153120171682"], "metadata": {"file_path": "modele_personnalise/v1_test_interface_cl.py", "file_name": "v1_test_interface_cl.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/modele_personnalise/v1_test_interface_cl.py"}}, "a7011225db0e8e53ca1171dc4d4730630f8a8057": {"node_ids": ["973a9bc9-daad-4963-8f96-efcaeab6d251", "87418484-729c-42a9-8613-2f374fa4f156"], "metadata": {"file_path": "readme.md", "file_name": "readme.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/readme.md"}}, "5bce626893bf47c40ca1454df6fc4c990875f9cc": {"node_ids": ["710d2581-73a5-41fb-95eb-166a93d68698"], "metadata": {"file_path": "requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/requirements.txt"}}, "5990a1d6e725b94c0410a8d7fc47a3839633c07e": {"node_ids": ["41847277-2046-4a94-b383-972bcf06bf94", "6c66285c-40dd-4cf2-b03d-a5a59cb1d3a0", "3c3d245e-cd50-4994-b82a-bd18d025ef4b"], "metadata": {"file_path": "sandboxRAG/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/README.md"}}, "2a4be4759c3a6c6efb18ee73fcd5b692148b776c": {"node_ids": ["9cea47e3-604f-43bd-8769-7dae043922e1", "c48a19b6-8a54-4580-bddd-fbb1fc556a67", "983fdefd-5cc9-445e-8a2e-afcec2d87bbb"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/RAGPdfOrTxt.py", "file_name": "RAGPdfOrTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/RAGPdfOrTxt.py"}}, "bbf569e1d5e0adc358dc1d5e9d4ff9da50dc561c": {"node_ids": ["0c886c5b-2a49-40f0-bb36-051f24784ee0", "046ac453-e203-489b-8f09-b2655a558628", "6bb487e8-ac28-4eb5-b29a-b0bb1ac8c453", "f1ff6f39-5ba2-4c12-82f2-6e23974487b2"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/chHuggingFaceRAG.py", "file_name": "chHuggingFaceRAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/chHuggingFaceRAG.py"}}, "d4ff68a0c067e1c3e94fadd875ba82ede9ac6992": {"node_ids": ["9ba12866-4df7-4c5d-a9eb-36f3cd54b72e", "ffbe0438-54fa-4dd5-bc87-5c739ec3fd27"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/elasticSearch-test.py", "file_name": "elasticSearch-test.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/elasticSearch-test.py"}}, "58cbe867b4a48bc62e1f9e42986fb0dde8d61134": {"node_ids": ["9849c6cc-de3c-4a79-8d1c-7ba669f201bb", "b62de8c1-f71e-4c6e-b83f-6301ec4d7117", "9bcc0635-95e9-4304-b192-53a2d5060d16", "a5026196-1dbc-4ff2-bfc9-3e904ed809cd", "b7dbc69b-b410-437f-b4fd-f106ad67bf49", "9ad17cff-aafd-493d-b98b-5d3c3adfc192", "4a283b59-f15f-47fc-b3ed-6a81577e2e05"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/README.md", "file_name": "README.md", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/README.md"}}, "47c9d1f461f64d806f8cfab80a59b47bcc0684d0": {"node_ids": ["e86ad346-6d4c-49df-9782-44034ea980ee"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/app.py", "file_name": "app.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/app.py"}}, "6da36ca7f2c12fecf479ba97087c4854781277ae": {"node_ids": ["0515024f-c184-43de-9830-ba7e592b41ea", "b1c45f48-654e-4c30-bdbe-77842f60e439"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/local-rag-example-main/rag.py", "file_name": "rag.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/local-rag-example-main/rag.py"}}, "437ca10e7fcb650ad9d6cc7e00e85cf426195652": {"node_ids": ["d147bf70-ffe7-4154-aa6e-c8aa0ae8fad7", "09e17bcc-8168-4050-b24f-0a7a2b8c1591", "355e3db2-0528-4f8e-b515-cb80d8ccb13d", "b81565f2-a954-49d3-a895-c26d3c9244c9"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/localRAGBasicForTxt.py", "file_name": "localRAGBasicForTxt.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/localRAGBasicForTxt.py"}}, "4c84b615c79ae3c69552051e5b4c2f05c55718eb": {"node_ids": ["06a78d16-75e4-4a9f-b0ee-02523dd160a1"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/requirements.txt", "file_name": "requirements.txt", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/requirements.txt"}}, "0db6ce47ff95cc8c2405086b0b9721e8f2d3ca64": {"node_ids": ["d9e4b838-2f23-4e87-8ceb-619184d9112b", "4e5ee4e3-8f19-4060-b0a7-e9f6fa0784a7", "8c8dd152-fc27-4e62-95f4-718e8b5b439f", "c8d484d7-6307-46a3-9ebc-1bc83682cab3"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/similarity_search.py", "file_name": "similarity_search.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/similarity_search.py"}}, "e994a9b32cc2cff65d7d736018149f076cf4a9ab": {"node_ids": ["91d40976-02c4-4352-b7af-0b5c5c750770"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/testElsCloudv2.py", "file_name": "testElsCloudv2.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/testElsCloudv2.py"}}, "b9a16ceebef3baf18c14472addad583f57beebb1": {"node_ids": ["00758c9f-dbeb-4881-8ac4-5ab4808ba4a9", "9b356805-9161-4f68-908f-bb21afcdc0ff", "b3f28b63-c0e1-483d-96ea-5f23a0b23be8", "d23a1de1-a1cc-45d6-9402-c2dece59952b"], "metadata": {"file_path": "sandboxRAG/brouillonRAG/upload.py", "file_name": "upload.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/brouillonRAG/upload.py"}}, "b930774918cc5aeecbaceff681af53f7a9117ec4": {"node_ids": ["bc67192d-5a82-4291-bb8b-0ab50a594df0", "9974d7a5-8002-433d-bba4-d6468dd3fc51", "2e828478-aeea-4834-9dae-713ba982e820", "d73007a1-d8ba-4529-98ca-fa7c4645c93d"], "metadata": {"file_path": "sandboxRAG/dossier_entier_RAG.py", "file_name": "dossier_entier_RAG.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/dossier_entier_RAG.py"}}, "3bdfee13d4d04ddd930eab471b6c82dc1bb6a73b": {"node_ids": ["f5c852aa-0697-4490-8078-b3cb0759af1d", "89ab858a-7a78-403b-a251-18ebf50ed77d", "1fe92b9f-affa-4523-bf28-4ed02ed7bea2", "c88b7851-7cf4-44eb-9963-5953e476fd83"], "metadata": {"file_path": "sandboxRAG/llama_index_HF.py", "file_name": "llama_index_HF.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/llama_index_HF.py"}}, "e9269ff6db3d6c8b2382edccfa366259b3e577b7": {"node_ids": ["702c4176-4be1-4f05-b802-87ca415965ed", "abb4d95e-45c1-4278-bdb6-fa0e1519fcaf", "08253c87-cdad-45c9-8fab-c5a2218d3b99", "d1159f79-6e7b-4d32-83cd-33cb636925aa", "68259f3b-f030-42d1-a0ae-4a738dbbf2a0", "c165d12b-54fe-459c-a3ce-0a0a8d62cb64"], "metadata": {"file_path": "sandboxRAG/utils/compare_embedding_model.py", "file_name": "compare_embedding_model.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/compare_embedding_model.py"}}, "da03217c2e9e57bb553c35fa72450cf6f482e29b": {"node_ids": ["a89dd7c2-f147-4122-9799-0bc050ef67e9"], "metadata": {"file_path": "sandboxRAG/utils/embedding_models.py", "file_name": "embedding_models.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/embedding_models.py"}}, "c5c31d65acf25a33c1d33105d9eb721f941d1c16": {"node_ids": ["69d0505d-88d9-4bdd-af2a-83241b7a199a", "2d455a7a-e2f5-4a7f-b258-e66708d8552f", "a6546d46-2cf5-4207-8761-523a1edf02ce"], "metadata": {"file_path": "sandboxRAG/utils/manip_documents.py", "file_name": "manip_documents.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/manip_documents.py"}}, "a2b76af6b9f0f9ff5150fb180e4d5bd4b467c6c3": {"node_ids": ["d82dab27-aec3-41d3-af4d-806388e449de"], "metadata": {"file_path": "sandboxRAG/utils/modify_pdf_metadata.py", "file_name": "modify_pdf_metadata.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/modify_pdf_metadata.py"}}, "94492d9bcbabe018649c059764d995c6dca54ac3": {"node_ids": ["5640608b-e5be-4a2f-955f-08c83de5fa4e", "34a79c52-1c4b-41b4-93aa-8ea0323966aa", "3a70c39f-00b9-4aca-bb43-a1c2b5caa17d", "573dd2de-c2aa-4dae-83ec-4f26cb164093"], "metadata": {"file_path": "sandboxRAG/utils/web_scraper.py", "file_name": "web_scraper.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/sandboxRAG/utils/web_scraper.py"}}, "e8587ec0bf9ca4a54ac448433f70b323f08a400d": {"node_ids": ["132cfefd-6bd0-41d2-95e7-9abb3536e098", "06c15a9b-114e-42e2-8aff-cd010d2ed4e3", "66b9dbd2-6d08-4776-8002-5a9dae4f54e3", "8716fb21-57fa-4e2b-afd1-eb867ae9a550"], "metadata": {"file_path": "widget_chainlit.py", "file_name": "widget_chainlit.py", "url": "https://github.com/eisenhowair/ProjetLLM/blob/main/widget_chainlit.py"}}}}